<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LLM自动评价方法</title>
    <link href="/LLM%E8%87%AA%E5%8A%A8%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95.html"/>
    <url>/LLM%E8%87%AA%E5%8A%A8%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95.html</url>
    
    <content type="html"><![CDATA[<h1 id="大语言模型自动评价方法">大语言模型自动评价方法</h1><p>LLM的自动评估是一种常见的，可能也是最受欢迎的评估方法，通常使用标准度量或指标以及评估工具来评估模型的性能，如准确性、BLEU（Papineniet al.，2002）、ROUGE（Lin，2004）、BERTScore（Zhang etal.，2019）等。由于其主观性、自动计算性和简单性，大多数现有的评估工作都采用了这种评估协议。因此，大多数确定性任务，如自然语言理解和数学问题，通常采用这种评估协议。</p><p>与人工评估相比，自动评估不需要密集的人工参与，节省了成本和时间。最近，随着LLM的发展，一些先进的自动评估技术也被设计来帮助评估。林和陈（2023）提出了LLM-EVAL，这是一种使用LLM进行开放域对话的统一多维度自动评估方法。PandaLM（Wangetal.，2023h）可以通过训练LLM来评估不同的模型，从而实现可复现和自动化的语言模型评估。Jain（2023）等人提出了一个自监督的评估框架，通过消除对新数据的费力标注，实现了在现实世界部署环境中评估模型的更有效形式。</p><h2 id="automatic-evaluation">Automatic Evaluation</h2><h3 id="acuracy">1. Acuracy</h3><p><span class="math display">\[准确率（Accuracy） = \frac{Number of correct predictions}{Total numberpf predictions} = \frac{正确预测数}{预测总数}\]</span></p><h3 id="bleu">2. BLEU</h3><blockquote><p>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: amethod for automatic evaluation of machine translation. In Proceedingsof the 40th annual meeting of the Association for ComputationalLinguistics, pages 311–318.</p></blockquote><p><span class="math display">\[B L E U=B P\times\exp\left(\sum_{n=1}^{N}W_{n}\times\log P_{n}\right)\]</span></p><p><span class="math display">\[B P=\left\{ {\begin{array}{l l}{1}&amp;{}&amp;{ {}{\mathrm{~l}}c&gt;1r}\\ {\exp(1-l r/l c)}&amp;{}&amp;{ {}{\mathrm{~l} }c\leq lr}\end{array} }\right.\]</span></p><p>神经网络生成的句子是candidate，给定的标准译文是reference。</p><ul><li><p>lc=candidate的长度，lr=最短的reference的长度</p></li><li><p>BP 是惩罚因子，如果译文的长度小于最短的参考译文，则 BP 小于1。避免当机器翻译的长度比较短时，BLEU得分也会比较高，但是这个翻译是会损失很多信息的。</p></li><li><p>BLEU 需要计算译文 1-gram，2-gram，...，N-gram 的精确率，一般 N设置为 4 即可，公式中的 Pn 指 n-gram的精确率。n-gram是连续n个词的组合。</p></li><li><p>BLEU 的 1-gram 精确率表示译文忠于原文的程度，而其他 n-gram表示翻译的流畅程度。</p></li><li><p>Wn 指 n-gram 的权重，一般设为均匀权重，即对于任意 n 都有 Wn =1/N。</p></li></ul><p><span class="math display">\[p_n= \frac{\sum_{c\in candidates}\sum_{n-gram\in c}Count_{clip}(n-gram)}{\sum_{ c^\prime \in candidates} \sum_{n-gram^\prime\inc^\prime}Count(n-gram^\prime)}\]</span></p><p>Count(n-gram')表示n−gram′在candidate中的个数， <spanclass="math display">\[C o u n t_{c l i p}=\mathrm{min}(C o u n t,M a x_{-}R e f_{-}C o u n t)\]</span></p><h3 id="rouge">3. ROUGE</h3><blockquote><p>Lin, C.-Y. (2004). Rouge: A package for automatic evaluation ofsummaries. In Text summarization branches out, pages 74–81.</p></blockquote><ul><li>ROUGE-N: 在 N-gram 上计算召回率</li><li>ROUGE-L: 考虑了机器译文和参考译文之间的最长公共子序列</li><li>ROUGE-W: 改进了ROUGE-L，用加权的方法计算最长公共子序列</li></ul><p><strong>ROUGE-N</strong> 主要统计 N-gram 上的召回率，对于N-gram，可以计算得到 ROUGE-N 分数，计算公式如下：</p><figure><img src="https://image.yinan.fun/202309251305125.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>公式的分母是统计在reference中N-gram的个数，而分子是统计reference与candidate共有的 N-gram 个数。</p><p>给定多个参考译文 <strong>Si</strong> <span class="math display">\[R O D G E-N_{multi}=\operatorname*{max}_{i}R O O G E-N(S_{i},C)\]</span> <strong>ROUGE-L</strong> 中的 L 指最长公共子序列 (longestcommon subsequence, LCS)，ROUGE-L计算的时候使用了机器译文<strong>C</strong>和参考译文<strong>S</strong>的最长公共子序列，计算公式如下：<span class="math display">\[R_{L C S}=\frac{L C S(C,S)}{l e n(S)}\]</span></p><p><span class="math display">\[P_{L C S}=\frac{L C S(C,S)}{l e n(C)}\]</span></p><p><span class="math display">\[F_{L C S}=\frac{(1+\beta^{2})R_{L C S}P_{L C S} }{R_{L CS}+\beta^{2}P_{L C S} }\]</span></p><p>公式中的 RLCS 表示召回率，而 PLCS 表示精确率，FLCS 就是 ROUGE-L。一般beta 会设置为很大的数，因此 FLCS 几乎只考虑了 RLCS(即召回率)。<strong>注意这里 beta 大，则 F 会更加关注 R，而不是P，可以看下面的公式。</strong>如果 beta 很大，则 PLCS那一项可以忽略不计。</p><p>ROUGE-W 是 ROUGE-L 的改进版，使用一种加权最长公共子序列方法(WLCS)，给连续翻译正确的更高的分数。</p><p>ROUGE-S 也是对 N-gram 进行统计，但是其采用的 N-gram 允许"跳词(Skip)"，即单词不需要连续出现。</p><h3 id="self-supervised-evaluation-framework">4. self-supervisedevaluation framework</h3><blockquote><p>Jain, N., Saifullah, K., Wen, Y., Kirchenbauer, J., Shu, M., Saha,A., Goldblum, M., Geiping, J., and Goldstein, T. (2023). Bring your owndata! self-supervised evaluation for large language models. arXivpreprint arXiv:2306.13651.</p></blockquote><p>目前的评估使用带有人工策划标签的小型领域特定数据集，这些评估集通常从狭窄而简化的分布中抽样，数据源可能会在不知不觉中泄露到培训集，这可能会导致误导性评估。</p><p>We demonstrate self-supervised evaluation strategies for measuringclosed-book knowledge, toxicity, and long-range context dependence, inaddition to sensitivity to grammatical structure and tokenizationerrors.</p><figure><img src="https://image.yinan.fun/202309251306278.png"alt="image-20230922214003156" /><figcaption aria-hidden="true">image-20230922214003156</figcaption></figure><p>Based on this intervention, we measure the change in thelog-perplexity (log(ppl(x))), between the original and negated sentence.Formally, we define the sensitivity score as the following: <spanclass="math display">\[{\mathrm{SENSITIVIT} }\operatorname{SCORE}={\frac{1}{n}}\sum_{i}^{n}{\mathrm{log} }{\mathrm{(ppl}}(x_{i}^{\prime}))-{\mathrm{log} }{\mathrm{(ppl} }(x_{i})).\]</span></p><p>接下来是第二类方法，称为“间接或分解的启发式方法（indirect ordecomposedheuristics）”。在这种方法中，我们<strong>利用较小的模型（smallermodels）来评估主模型（the mainmodel）生成的答案</strong>，这些较小的模型可以是微调过的模型或原始的分解模型（rawdecompositions）。其核心思想是选取在这些大模型擅长的任务上表现更好的小模型来进行评估。这些较小模型的输出被看作是弱得分（weakscores），然后将它们结合起来为生成的输出提供一个最终的标签或评价。这种间接评估方法能够<strong>更加细致地评估模型的性能</strong>，尤其在判断对散文的喜爱程度等这些任务。</p><h3 id="bertscore">5. BERTScore</h3><blockquote><p>Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y.(2019). Bertscore: Evaluating text generation with bert. arXiv preprintarXiv:1904.09675.</p></blockquote><p>基于N-gram重叠的度量标准只对词汇变化敏感，不能识别句子语义或语法的变化。因此，它们被反复证明与人工评估差距较大。</p><p>于是有人提出使用句子上下文表示(bert全家桶)和人工设计的计算逻辑对句子相似度进行计算。这样的评价指标鲁棒性较好，在缺乏训练数据的情况下也具有较好表现。</p><p>思路是非常简单的：即对两个生成句和参考句（wordpiece进行tokenize）分别用bert提取特征，然后对2个句子的每一个词分别计算内积，可以得到一个相似性矩阵。</p><figure><img src="https://image.yinan.fun/202309251304050.png"alt="image-20230922111952603" /><figcaption aria-hidden="true">image-20230922111952603</figcaption></figure><p>基于这个矩阵，我们可以分别对参考句和生成句做一个最大相似性得分的累加然后归一化，得到bertscore的precision，recall和F1：</p><figure><img src="https://image.yinan.fun/202309251305151.png"alt="image-20230922111853984" /><figcaption aria-hidden="true">image-20230922111853984</figcaption></figure><p>还可以考虑给不同的词以权重。作者使用idf函数，即给定M个参考句，词w的idf为：<span class="math display">\[\operatorname{idf}(w)=-\log \frac{1}{M} \sum_{i=1}^{M} \mathbb{I}\left[w\in x^{(i)}\right]\]</span> 其中<spanclass="math inline">\(\mathbb{I}()\)</span>是一个指示函数。用此式更新上述评分，例如recall：<span class="math display">\[R_{\mathrm{BERT} }=\frac{\sum_{x_{i} \in x} \text { idf}\left(x_{i}\right) \max _{\hat{x}_{j} \in \hat{x} }\mathbf{x}_{i}^{\top} \hat{\mathbf{x} }_{j} }{\sum_{x_{i} \in x} \text {idf }\left(x_{i}\right)}\]</span></p><h3 id="llm-eval">6. LLM-EVAL</h3><blockquote><p>Lin, Y.-T. and Chen, Y.-N. (2023). Llm-eval: Unified multidimensionalautomatic evaluation for open-domain conversations with large languagemodels. arXiv preprint arXiv:2305.13711.</p></blockquote><p>The single prompt is created by concatenating the dialogue context,reference (if available), and generated response, and then fed to alarge language model, which outputs scores for each dimension based onthe defined schema.xs</p><p><img src="https://image.yinan.fun/202309251304291.png" alt="image-20230922202719792" style="zoom: 50%;" /></p><ul><li><p>Unified Evaluation Schema</p><p>The evaluation schema is a natural language instruction that definesthe task and the desired evaluation criteria. The schema is provided asa format instruction, which <strong>specifies the structure and therange of the scores for each dimension</strong>. For example, theevaluation schema can be:</p><figure><img src="https://image.yinan.fun/202309251304462.png"alt="image-20230922202303631" /><figcaption aria-hidden="true">image-20230922202303631</figcaption></figure></li><li><p>Single Prompt for Evaluation</p><p>The prompt is concatenated with the dialogue context, the reference(if available), and the generated response, and then fed to the largelanguage model to output a score for each evaluation dimension, based onthe defined schema.</p><figure><img src="https://image.yinan.fun/202309251304679.png"alt="image-20230922202344802" /><figcaption aria-hidden="true">image-20230922202344802</figcaption></figure></li><li><p>Efficient Evaluation</p><p>By using a single prompt with a unified evaluation schema, LLM-EVALcan efficiently obtain multi-dimensional scores for the responseswithout the need for multiple prompts.</p><figure><img src="https://image.yinan.fun/202309251304926.png"alt="image-20230922202430502" /><figcaption aria-hidden="true">image-20230922202430502</figcaption></figure></li><li><p>Different LLMs</p><p>In conclusion, our analysis demonstrates that<strong>dialogue-optimized LLMs</strong>, such as Claude and ChatGPT,yield <strong>better performance</strong> in the LLMEVAL method foropen-domain conversation evaluation. Although smaller models likeAnthropic Claude-instant may not achieve the best performance, they canstill be considered for resourcelimited scenarios.</p><figure><img src="https://image.yinan.fun/202309251304520.png"alt="image-20230922204729102" /><figcaption aria-hidden="true">image-20230922204729102</figcaption></figure></li><li><p>Baseline Evaluation Metrics</p><figure><img src="https://image.yinan.fun/202309251304232.png"alt="image-20230924100711864" /><figcaption aria-hidden="true">image-20230924100711864</figcaption></figure></li></ul><h3 id="pandalm">7. PandaLM</h3><blockquote><p>Wang, Y., Yu, Z., Zeng, Z., Yang, L., Wang, C., Chen, H., Jiang, C.,Xie, R., Wang, J., Xie, X., et al. (2023h). Pandalm: An automaticevaluation benchmark for llm instruction tuning optimization. arXivpreprint arXiv:2306.05087.</p></blockquote><ul><li><p>开源代码/模型</p><p><ahref="https://github.com/WeOpenML/PandaLM">https://github.com/WeOpenML/PandaLM</a></p><p><ahref="https://huggingface.co/WeOpenML/PandaLM-7B-v1">https://huggingface.co/WeOpenML/PandaLM-7B-v1</a></p></li></ul><p>当前，大家评估大模型的方法主要有两个：</p><ul><li>调用OpenAI的API接口，发送数据给OpenAI可能会像三星员工泄露代码一样造成数据泄露问题。</li><li>雇佣专家进行人工标注，雇佣专家标注大量数据又十分费时费力且昂贵。</li></ul><p>PandaLM只需要在「本地部署」，且「不需要人类参与」，因此PandaLM的评估是可以保护隐私且相当廉价的。</p><p>当两个不同的大模型对同一个指令和上下文产生不同响应时，PandaLM旨在比较这两个大模型的响应质量，并输出比较结果，比较理由以及可供参考的响应。比较结果有三种：响应1更好，响应2更好，响应1与响应2质量相似。</p><figure><img src="https://image.yinan.fun/202309251305749.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>结果表明，PandaLM-7B达到了93.75%的GPT-3.5评价能力和88.28%的GPT-4评价能力。</p><figure><img src="https://image.yinan.fun/202309251305475.png"alt="image-20230924101010218" /><figcaption aria-hidden="true">image-20230924101010218</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神经网络训练</title>
    <link href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83.html"/>
    <url>/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83.html</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络训练">神经网络训练</h1><h2 id="general-guidance">General Guidance</h2><p><img src="https://image.yinan.fun/202309182239371.png" /></p><h3 id="model-bias">Model Bias</h3><ul><li><p>The model is too simple.</p><p>find a needle in a haystack, but there is no needle</p></li><li><p>Solution: redesign your model to make it more flexible</p><ol type="1"><li>More features</li><li>Deep Learning (more neurons, layers)</li></ol></li></ul><p><img src="https://image.yinan.fun/202309191041005.png" /></p><h3 id="optimization-issue">Optimization Issue</h3><p>A needle is in a haystack, just cannot find it.</p><h3 id="model-bias-v.s.-optimization-issue">Model Bias v.s. OptimizationIssue</h3><p>怎么区分？</p><ul><li><p>Start from shallower networks (or other models), which are easierto optimize.</p></li><li><p>If deeper networks do not obtain smaller loss on training data,then there is optimization issue.</p></li><li><p>Solution: More powerful optimization technology</p></li></ul><h3 id="overfitting">Overfitting</h3><ul><li><p>Small loss on training data, large loss on testing data.</p></li><li><p>Solution</p><ol type="1"><li><p>More training data</p></li><li><p>Data augmentation</p><p>图片翻转、放大（不能颠倒）</p></li><li><p>constrained model</p><p>Less parameters, sharing parameters(CNN), Less features, Earlystopping, Regularization, Dropout</p></li></ol></li></ul><h3 id="bias-complexity-trade-off">Bias-Complexity Trade-off</h3><p><img src="https://image.yinan.fun/202309191041689.png" /></p><h3 id="mismatch">Mismatch</h3><ul><li>Your training and testing data have different distributions. Beaware of how data is generated.</li></ul><h2 id="gradient-is-small">Gradient is small</h2><h3 id="optimization-fails">Optimization Fails</h3><ul><li>gradient is close to zero (critical point)<ol type="1"><li>local minima</li><li>saddle point</li></ol></li></ul><p><img src="https://image.yinan.fun/202309191041294.png" /></p><h3 id="区分local-minima和saddle-point">区分local minima和saddlepoint</h3><ul><li>Tayler Series Approximation</li></ul><p><img src="https://image.yinan.fun/202309191041581.png" /></p><p>在critical point，gradient为0，不考虑第二项</p><p><img src="https://image.yinan.fun/202309191041837.png" /></p><p><img src="https://image.yinan.fun/202309191041923.png" /></p><ul><li>Example</li></ul><p><img src="https://image.yinan.fun/202309191041773.png" /></p><h3 id="saddle-point更新参数">Saddle point更新参数</h3><p>𝐻 may tell us parameter update direction</p><p><img src="https://image.yinan.fun/202309191041177.png" /></p><p>找到H负的eigen value <spanclass="math inline">\(\lambda\)</span>对应的eigen vector <spanclass="math inline">\(\mu\)</span>，用这个eigen vector更新 <spanclass="math inline">\(\theta =\theta&#39;+\mu\)</span>，Loss就会下降</p><p>Update the parameter along the direction of <spanclass="math inline">\(\mu\)</span> You can escape the saddle point anddecrease the loss.(this method is seldom used in practice，计算量大)</p><p>大部分情况，loss无法下降是卡在saddle point（一部分eigenvalue为正，一部分为负）</p><p><img src="https://image.yinan.fun/202309191042158.png" /></p><h2 id="batch">Batch</h2><h3 id="optimization-with-batch">Optimization with Batch</h3><p>每个batch更新一次参数</p><p><img src="https://image.yinan.fun/202309191042170.png" /></p><h3 id="small-batch-v.s.-large-batch">Small Batch v.s. Large Batch</h3><ul><li>Smaller batch requires longer time for one epoch (longer time forseeing all data once) (Parallel computing)</li></ul><p><img src="https://image.yinan.fun/202309191042028.png" /></p><ul><li>Batch size 越大，准确率越低</li></ul><p><img src="https://image.yinan.fun/202309191042660.png" /></p><p>Smaller batch size has better performance. “Noisy” update is betterfor training</p><p>每一次batch更新参数，loss函数都是略有差异的</p><p><img src="https://image.yinan.fun/202309191042742.png" /></p><ul><li><p>Small batch is better on testing data? (bettergeneralization)</p><p>On Large-Batch Training for Deep Learning: Generalization Gap andSharp Minima https://arxiv.org/abs/1609.04836</p></li></ul><p><img src="https://image.yinan.fun/202309191042772.png" /></p><p>Batch size is a hyperparameter you have to decide.</p><ul><li><p>全都要？</p><ul><li><p>Large Batch Optimization for Deep Learning: Training BERT in 76minutes (https://arxiv.org/abs/1904.00962)</p></li><li><p>Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in15 Minutes (https://arxiv.org/abs/1711.04325)</p></li><li><p>Stochastic Weight Averaging in Parallel: Large-Batch TrainingThat Generalizes Well (https://arxiv.org/abs/2001.02312)</p></li><li><p>Large Batch Training of Convolutional Networks(https://arxiv.org/abs/1708.03888)</p></li><li><p>Accurate, large minibatch sgd: Training imagenet in 1 hour(https://arxiv.org/abs/1706.02677)</p></li></ul></li></ul><h2 id="momentum">Momentum</h2><ul><li>球从高处滚下来</li></ul><p><img src="https://image.yinan.fun/202309191042460.png" /></p><h3 id="gradient-descent-momentum">Gradient Descent + Momentum</h3><p>每一步的移动 = 上一次的移动 - gradient</p><p><img src="https://image.yinan.fun/202309191042888.png" /></p><p><span class="math inline">\(m^{i}\)</span> is the weighted sum of allthe previous gradient: <spanclass="math inline">\(g^{0},\,g^{1},\,\ldots,\,g^{i-1}\)</span> <spanclass="math display">\[{ {m^{0}=0} }\\ { {m^{1}=-\eta g^{0} }}\\ m^{2}=-\lambda\eta g^{0}-\etag^{1}\]</span> Movement not just based on gradient, but previousmovement.</p><h2 id="adaptive-learning-rate">Adaptive Learning Rate</h2><ul><li><p>Training stuck ≠ Small Gradient</p><p>可能在error surface的山谷来回震荡</p></li></ul><p><img src="https://image.yinan.fun/202309191042207.png" /></p><h3 id="different-parameters-needs-different-learning-rate">Differentparameters needs different learning rate</h3><p><img src="https://image.yinan.fun/202309191042982.png" /></p><ul><li><p>Root Mean Square</p><p><span class="math inline">\(\sigma\)</span>是所以gradient的平方的平均开根号</p></li></ul><p><span class="math display">\[\theta_{i}^{t+1} \rightarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}\\{{\sigma_{i}^{t}=\sqrt{\frac{1}{t+1}\displaystyle\sum_{i=0}^{t}(g_{i}^{t})^{2}}} }\]</span></p><p>error surface越平滑，<span class="math inline">\(\sigma\)</span>越小，<span class="math inline">\(\theta\)</span> 更新越大。rrorsurface越陡，<span class="math inline">\(\sigma\)</span> 越大，<spanclass="math inline">\(\theta\)</span> 更新越小。</p><p><img src="https://image.yinan.fun/202309191042331.png" /></p><h3 id="learning-rate-adapts-dynamically">Learning rate adaptsdynamically</h3><p>Root Mean Square 不能根据当前 error surface 的情况快速地及时变化</p><ul><li><p>RMSProp</p><p><span class="math inline">\(\sigma\)</span> 是之前的 <spanclass="math inline">\(\sigma\)</span> 和这次的gradient加权</p><p>The recent gradient has larger influence, and the past gradients haveless influence.</p></li></ul><p><span class="math display">\[\theta_{i}^{t+1} \rightarrow\theta_{i}^{t}-\frac{\eta}{\sigma_{i}^{t}}g_{i}^{t}\\\sigma_{i}^{t}={\sqrt{\alpha{\left(\sigma_{i}^{t-1}\right)}^{2}+{(1-\alpha)\left({g_{i}^{t}}\right)}^{2} }}\]</span></p><ul><li>Adam: RMSProp + Momentum</li></ul><p><img src="https://image.yinan.fun/202309191042884.png" /></p><p><img src="https://image.yinan.fun/202309191042493.png" /></p><p>红圈里因为<spanclass="math inline">\(\sigma\)</span>很小，会突然跳很远</p><h3 id="learning-rate-scheduling">Learning Rate Scheduling</h3><p><img src="https://image.yinan.fun/202309191042270.png" /></p><ol type="1"><li><p>Learning Rate Decay</p><p>As the training goes, we are closer to the destination, so we reducethe learning rate.</p></li></ol><p><img src="https://image.yinan.fun/202309191042887.png" /></p><ol start="2" type="1"><li><p>Warm Up （黑科技）</p><p>Increase and then decrease?</p><p>了解更多：RAdam: <ahref="https://arxiv.org/abs/1908.03265">https://arxiv.org/abs/1908.03265</a></p></li></ol><p><img src="https://image.yinan.fun/202309191043259.png" /></p><ul><li>Residual Network: <ahref="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></li><li>Transformer: <ahref="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li></ul><p><img src="https://image.yinan.fun/202309191043594.png" /></p><h3 id="summary-of-optimization">Summary of Optimization</h3><ul><li>(Vanilla) Gradient Descent</li></ul><p><span class="math display">\[\theta_{i}^{t+1} \leftarrow\theta_{i}^{t}-\eta g_{i}^{t}\]</span></p><ul><li>Various Improvements</li></ul><p><img src="https://image.yinan.fun/202309191043638.png" /></p><h2 id="batch-normalization">Batch Normalization</h2><p>把error surface的山铲平</p><p>假设一个简单的网络 <span class="math display">\[y = w_1x_1+w_2x_2+b\]</span> 当<span class="math inline">\(x_1\)</span>很小，<spanclass="math inline">\(x_2\)</span>很大时，<spanclass="math inline">\(w_1\)</span>的变化对Loss的影响很小，<spanclass="math inline">\(w_2\)</span>的变化对Loss的影响很大，就会出现图中左边的情况。</p><p><img src="https://image.yinan.fun/202309191043406.png" /></p><p>让输入feature不同的dimension有相同的数值范围，得到更好的errorsurface</p><h3 id="feature-normalization">Feature Normalization</h3><p><img src="https://image.yinan.fun/202309191043068.png" /></p><p>In general, feature normalization makes gradient descent convergefaster.</p><ul><li>Considering Deep Learning</li></ul><p>This is a large network! 每一个batch是一个更大的网络</p><p><img src="https://image.yinan.fun/202309191043237.png" /></p><p>Normalization后在乘以<spanclass="math inline">\(\gamma\)</span>再加<spanclass="math inline">\(\beta\)</span>，<spanclass="math inline">\(\gamma\)</span>和<spanclass="math inline">\(\beta\)</span>是网络内可学习的参数。</p><p>因为Normalization后<spanclass="math inline">\(\widetilde{z}^{i}\)</span>的均值为0，这个限制可能会对网络产生不好的影响，<spanclass="math inline">\(\gamma\)</span>再加<spanclass="math inline">\(\beta\)</span>让网络输出的均值不一定是0</p><p><span class="math inline">\(\gamma\)</span>初始化为1 vector，<spanclass="math inline">\(\beta\)</span>初始化为0 vector <spanclass="math display">\[\widetilde{z}^{i}=\frac{z^i-\mu}{\sigma} \\{\hat{z} }^{i}=\gamma\Theta{\widetilde{z} }^{i}+\beta\]</span> <img src="https://image.yinan.fun/202309191102296.png" /></p><h3 id="testing">Testing</h3><p>We do not always have batch at testing stage.</p><p>Computing the moving average of <spanclass="math inline">\(\mu\)</span> and <spanclass="math inline">\(\sigma\)</span> of the batches duringtraining.</p><p><img src="https://image.yinan.fun/202309191102297.png" /></p><h3 id="bn-on-cnn">BN on CNN</h3><p>在卷积层后面的BN与1d情况的BN层是很类似的，同样是沿着batch的维度求均值和方差。并且，它的参数大小也仅仅与featuredim（channels）有关，也为2*d。但是，有一点需要注意的是，在求均值和方差的时候，实际上其不仅是沿着batch的维度求取，在每个channel上的宽度和高度方向也求取均值。</p><p><img src="https://image.yinan.fun/202309191110637.png" /></p><p>均值和方差的shape是[1, 2, 1,1]。也就是说，求取均值和方差时，是沿着batch, h,w三个维度进行的，只保证每个channel的统计值是独立的，所以求得均值和方差：</p><p><img src="https://image.yinan.fun/202309191111003.png" /></p><h3 id="to-learn-more">To learn more</h3><ul><li><p>Batch Renormalization: https://arxiv.org/abs/1702.03275</p></li><li><p>Layer Normalization: https://arxiv.org/abs/1607.06450</p></li><li><p>Instance Normalization: https://arxiv.org/abs/1607.08022</p></li><li><p>Group Normalization: https://arxiv.org/abs/1803.08494</p></li><li><p>Weight Normalization: https://arxiv.org/abs/1602.07868</p></li><li><p>Spectrum Normalization: https://arxiv.org/abs/1705.10941</p></li></ul><h2 id="参考">参考</h2><ol type="1"><li><ahref="https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php">https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php</a></li><li><ahref="https://www.bilibili.com/video/BV1Wv411h7kN">https://www.bilibili.com/video/BV1Wv411h7kN</a></li><li><ahref="https://zhuanlan.zhihu.com/p/403073810">https://zhuanlan.zhihu.com/p/403073810</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>李宏毅</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion Model</title>
    <link href="/Diffusion-Model.html"/>
    <url>/Diffusion-Model.html</url>
    
    <content type="html"><![CDATA[<h1 id="diffusion-model">Diffusion Model</h1><h2 id="diffusion-model-是如何运作的">Diffusion Model是如何运作的？</h2><p>将噪声输入Denoise Model T次生成干净的图像，步骤数t（噪声的程度）一同输入Denoise Model</p><p><img src="https://image.yinan.fun/202309161633319.png" /></p><h3 id="denoise-模组内部实际做的事情">Denoise 模组内部实际做的事情</h3><p>产生输入图片的杂序，再将图片减去杂序</p><p>为什么不直接训练一个生成图像的模型？难度更大</p><p><img src="https://image.yinan.fun/202309161633712.png" /></p><h2 id="如何训练-noise-predictor">如何训练 Noise Predictor</h2><p><strong>Forward Process (Diffusion Process)</strong></p><p><img src="https://image.yinan.fun/202309161633108.png" /></p><p><img src="https://image.yinan.fun/202309161633844.png" /></p><h3 id="text-to-image">Text-to-Image</h3><p><img src="https://image.yinan.fun/202309161634005.png" /></p><p>把文字一同作为Denoise模组的输入</p><p><imgsrc="https://image.yinan.fun/202309161634588.png" />Denoise模组内部</p><p><img src="https://image.yinan.fun/202309161634450.png" /></p><p>训练</p><p><img src="https://image.yinan.fun/202309161634572.png" /></p><h3 id="denoising-diffusion-probabilistic-models">Denoising DiffusionProbabilistic Models</h3><p><img src="https://image.yinan.fun/202309161634961.png" /></p><h1 id="diffusion-model-背后的数学原理">Diffusion Model背后的数学原理</h1><h3 id="training">Training</h3><p><img src="https://image.yinan.fun/202309141116049.png" /></p><p>采样干净的图像<span class="math inline">\(x_0\)</span>，噪声<spanclass="math inline">\(\epsilon\)</span>和<spanclass="math inline">\(t\)</span></p><p>将<span class="math inline">\(x_0\)</span>和<spanclass="math inline">\(\epsilon\)</span>按照与t有关的权重相加，并和<spanclass="math inline">\(t\)</span>作为Noisy Predictor的输入</p><p><img src="https://image.yinan.fun/202309141116051.png" /></p><h3 id="inference">Inference</h3><p><img src="https://image.yinan.fun/202309141116052.png" /></p><p><img src="https://image.yinan.fun/202309141116053.png" /></p><h2 id="影像生成模型本质上的共同目標">影像生成模型本质上的共同目標</h2><p>图像生成模型将简单的分布映射到复杂的分布，并且让生成图像的分布尽可能接近真实图像的分布。</p><p><img src="https://image.yinan.fun/202309141116054.png" /></p><h3 id="maximum-likelihood-estimation">Maximum LikelihoodEstimation</h3><p><img src="https://image.yinan.fun/202309152149527.png" /></p><p>Sample <spanclass="math inline">\(\{x^{1},x^{2},\ldots,x^{m}\}\)</span> from <spanclass="math inline">\(P_{d a t a}(x)\)</span> ，让模型生成采样的样本<span class="math inline">\(\{x^{1},x^{2},\ldots,x^{m}\}\)</span>的概率最大 <span class="math display">\[\begin{align*}\theta^{*}&amp;=a rg\operatorname*{max}_{\theta}\prod_{i=1}^{m}P_{\theta}\left(x^{i}\right)\=a r g\operatorname*{max}_{\theta}l og\prod_{i=1}^{m}P_{\theta}\left(x^{i}\right)\\&amp;=a r g\,\operatorname*{max}_{\theta}\sum_{i=1}^{m}l o gP_{\theta}(x^{i})\,\approx a r g\,\operatorname*{max}_{\theta}E_{x-P_{da t a} }[l o g P_{\theta}(x)]\end{align*}\]</span> 减去与 <span class="math inline">\(\theta\)</span>无关的第二项 <span class="math inline">\(\int_{x}P_{d a t a}(x)l o gP_{d a t a}(x)d x\)</span> <span class="math display">\[\begin{align*}&amp;=a r g\operatorname*{max}_{\theta}\int_{x}P_{d a t a}(x)l o gP_{\theta}(x)d x -\int_{x}P_{d a t a}(x)l o g P_{d a t a}(x)d x \\&amp;=a r g\operatorname*{max}_{\theta}\int_{x}P_{d a t a}(x)l og{\frac{P_{\theta}(x)}{P_{d a t a}(x)} }d x\ =a rg\operatorname*{min}_{\theta}K L(P_{d a t a}||P_{\theta})\end{align*}\]</span> Maximum Likelihood = Minimize KL Divergence</p><p>Maximum Likelihood Estimation可以看成让生成图像的分布与真实图像的分布尽可能小</p><h3 id="vae-compute-p_thetax">VAE: Compute <spanclass="math inline">\(P_{\theta}(x)\)</span></h3><p><img src="https://image.yinan.fun/202309152149516.png" /></p><p>先sample <span class="math inline">\(z\)</span> 再得到 <spanclass="math inline">\(x\)</span>，<spanclass="math inline">\(P(z)\)</span>很容易计算 <spanclass="math display">\[P_{\theta}(x)=\int_{z}P(z)P_{\theta}\left(x|z\right)d z\]</span></p><ul><li>可以定义<span class="math inline">\(P_{\theta}(x|z)\)</span></li></ul><p><span class="math display">\[P_{\theta}(x|z)=\begin{cases}1,\qquad G(z)=x \\0,\qquad G(z)\neq x\end{cases}\]</span></p><p>问题是<spanclass="math inline">\(P_{\theta}(x|z)\)</span>可能会几乎都是 0</p><p><img src="https://image.yinan.fun/202309152150349.png" /></p><p>假设网络的输出<spanclass="math inline">\(G(z)\)</span>是Gaussian分布的mean，可得？</p><p>G(z)和x的距离越小，<spanclass="math inline">\(P_{\theta}(x|z)\)</span>就越大</p><p><span class="math display">\[{ {P_{\theta}(x|z)} } \propto\ e x p(-||G(z)-x||_{2})\]</span></p><h3 id="vae-lower-bound-of-logp_thetax">VAE: Lower bound of <spanclass="math inline">\(logP_{\theta}(x)\)</span></h3><p><img src="https://image.yinan.fun/202309161534378.png" /></p><p>下面的公式中的<span class="math inline">\(q(z|x)\)</span>可以是任意的分布，在VAE中是Encoder $$ <spanclass="math display">\[\begin{align}l o g P_{\theta}(x)&amp;=\int_{z}\,q(z|x)l o g P(x)d z \\&amp;=\int_{z}q(z|x)l o g\left({\frac{P(z,x)}{P(z|x)} }\right)d z\=\int_{z}\,q(z|x)l o g\left({\frac{P(z,x)}{q(z|x)}}{\frac{q(z|x)}{P(z|x)} }\right)d z  \\&amp;=\int_{z}q(z|x)l o g\left({\frac{P(z,x)}{q(z|x)} }\right)dz+\int_{z}q(z|x)l o g\left({\frac{q(z|x)}{P(z|x)} }\right)d z\end{align}\]</span> <span class="math display">\[第二项是$KL\left(q(z|x)||P(z|x)\right)$，始终大于0，因此\]</span> _{z},q(z|x)l o g,(),d z =[l o g,()] $$ 可以通过最大化Lowerbound来使<span class="math inline">\(logP(x)_{\theta}\)</span>最大</p><h3 id="ddpm-compute-p_thetax">DDPM: Compute <spanclass="math inline">\(P_{\theta}(x)\)</span></h3><p><img src="https://image.yinan.fun/202309152150350.png" /> <spanclass="math display">\[P_{\theta}(x_{0})=\int_{x_{1}:x_{T}}P\left(x_{T}\right)P_{\theta}(x_{T-1}|x_{T})\ldotsP_{\theta}(x_{t-1}|x_{t})\ldots P_{\theta}(x_{0}|x_{1})d x_{1}:x_{T}\]</span> 表示对所有可能的<spanclass="math inline">\(x_{1}:x_{T}\)</span>求积分</p><p>通过与VAE相同的推导，可以计算得到DDPM <spanclass="math inline">\(logP_{\theta}(x)\)</span> 的lower bound</p><p><img src="https://image.yinan.fun/202309152150351.png" /> <spanclass="math display">\[q(x_{1};x_{T}|x_{0})=q(x_{1}|x_{0})q(x_{2}|x_{1})\ldots q(x_{T}|x_{T-1})\]</span></p><h3 id="计算-qx_tx_0">计算 <spanclass="math inline">\(q(x_t|x_0)\)</span></h3><p><img src="https://image.yinan.fun/202309152150352.png" /></p><p>其中</p><p><img src="https://image.yinan.fun/202309152150353.png" /></p><p>推理可得 <span class="math display">\[x_t = {\sqrt{1-\beta_{1} }}\ldots{\sqrt{1-\beta_{t} }} \times x_0+\sqrt{1-(1-\beta_{1})\dots(1-\beta_{t})} \times \epsilon\]</span> 令<span class="math inline">\(\sqrt{\overline{\alpha}_{t} } =(1-\beta_{1})\dots(1-\beta_{t})\)</span>， <span class="math display">\[x_t = \sqrt{\overline{\alpha}_{t} } \times x_0+ \sqrt{1 -\overline{\alpha}_{t} } \times \epsilon\]</span></p><h3 id="ddpm-lower-bound-of-logp_thetax">DDPM: Lower bound of <spanclass="math inline">\(logP_{\theta}(x)\)</span></h3><p>经过数学推理。。。</p><p><ahref="https://arxiv.org/pdf/2208.11970.pdf">https://arxiv.org/pdf/2208.11970.pdf</a></p><p><img src="https://image.yinan.fun/202309152150354.png" /></p><p>得到<span class="math inline">\(logP_{\theta}(x)\)</span>的lowerbound： <span class="math display">\[\operatorname{E}_{q}(x_{1}|x_{0})\left[l o g P(x_{0}|x_{1})\right]\,-KL{\bigl(}q(x_{T}|x_{0}){\bigr)}||P(x_{T}){\bigr)}-\sum_{t=2}^{T}\operatorname{E}_{q}(x_{t}|x_{0})\left[KL{\bigl(}q(x_{t-1}|x_{t},x_{0})||P(x_{t-1}|x_{t})\bigr)\right]\]</span> 第二项<span class="math inline">\(KL{\bigl(}q(x_{T}|x_{0}){\bigr)}||P(x_{T}){\bigr)}\)</span>与网络没有关系，不需要考虑</p><p>先考虑第三项<spanclass="math inline">\(\sum_{t=2}^{T}\operatorname{E}_{q}(x_{t}|x_{0})\left[KL{\bigl(}q(x_{t-1}|x_{t},x_{0})||P(x_{t-1}|x_{t})\bigr)\right]\)</span></p><p><img src="https://image.yinan.fun/202309152150355.png" /> <spanclass="math display">\[\begin{align}q(x_{t-1}|x_{t},x_{0})&amp;={\frac{q(x_{t-1},x_{t},x_{0})}{q(x_{t},x_{0})}}\\&amp;={\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})q(x_{0})}{q(x_{t}|x_{0})q(x_{0})}} \\&amp;={\frac{q(x_{t}|x_{t-1})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})} }\end{align}\]</span> 再经过数学推理。。。</p><p><img src="https://image.yinan.fun/202309152150356.png" /></p><p>得到<spanclass="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>依旧是Gaussian分布，均值和方差分别是</p><p><img src="https://image.yinan.fun/202309152150357.png" /></p><p>所以，第三项就是最小化两个Gaussian分布的KL散度</p><p>计算两个Gaussian分布的KL散度的公式 <span class="math display">\[D_{\mathrm{KL} }({\cal N}(x;\mu_{x},\Sigma_{x})\parallel{\calN}(y;\mu_{y},\Sigma_{y}))=\frac{1}{2}\left[\log\frac{\left|\Sigma_{y}\right|}{\left|\Sigma_{x}\right|}-d+\mathrm{tr}(\Sigma_{y}^{-1}\Sigma_{x})+(\mu_{y}-\mu_{x})^{T}\Sigma_{y}^{-1}(\mu_{y}-\mu_{x})\right]\]</span></p><ul><li>更简单的方法</li></ul><p><spanclass="math inline">\(q(x_{t-1}|x_{t},x_{0})\)</span>的均值和方差是固定的，与网络无关</p><p>对于<spanclass="math inline">\(P(x_{t-1}|x_{t})\bigr)\)</span>，只考虑均值</p><p><img src="https://image.yinan.fun/202309152150358.png" /></p><p>要最小化两个Gaussian的分布的KL散度，就要让他们的均值更接近</p><p><img src="https://image.yinan.fun/202309152150359.png" /> <spanclass="math display">\[x_{t}=\sqrt{\bar{\alpha}_{t} }x_{0}+\sqrt{1-\bar{\alpha}_{t}}\varepsilon\]</span></p><p><span class="math display">\[x_{t}-\sqrt{1-\bar{\alpha}_{t} }\varepsilon=\sqrt{\bar{\alpha}_{t}}x_{0}\]</span></p><p><span class="math display">\[{\frac{x_{t}-{\sqrt{1-{\bar{\alpha} }_{t} }\varepsilon} }{ {\sqrt{{\bar{\alpha} }_{t} }} }}=x_{0}\]</span></p><p>将<spanclass="math inline">\(x_0\)</span>代入均值的公式中，再经过数学推理。。。<span class="math display">\[\frac{\sqrt{\alpha_{t-1}}\beta_{t}\left(\frac{\alpha_{t}-\sqrt{1-\overline{\alpha_{t}}}\varepsilon}{\sqrt{\overline{\alpha_{t} }} }\right)+\sqrt{\alpha_{t}}(1-\overline{\alpha}_{t-1})x_{t} }{1-\overline{\alpha}_{t} } \\=\frac{1}{\sqrt{\alpha_{t} }}\left(x_{t}-\frac{1-\alpha_{t}}{\sqrt{1-\overline{\alpha}_{t} }}\,\varepsilon\right)\]</span></p><p>上式中只有<spanclass="math inline">\(\epsilon\)</span>是未知的，需要网络预测（<spanclass="math inline">\(\epsilon\)</span>是加上<spanclass="math inline">\(x_0\)</span>生成<spanclass="math inline">\(x_t\)</span>的噪声）</p><p><img src="https://image.yinan.fun/202309152150360.png" /></p><p><img src="https://image.yinan.fun/202309152150361.png" /></p><h2 id="sampling为什么要加noise">Sampling为什么要加Noise</h2><p><img src="https://image.yinan.fun/202309152150362.png" /></p><p>为什么不直接取 Mean？</p><p><img src="https://image.yinan.fun/202309152150363.png" /></p><p>以下是李宏毅老师的猜测：</p><p>同样的问题：为什么生成文句時需要 Sample？</p><p><img src="https://image.yinan.fun/202309152150364.png" /></p><ul><li><p>The Curious Case of Neural Text Degeneration</p><p><ahref="https://arxiv.org/abs/1904.09751">https://arxiv.org/abs/1904.09751</a></p><p>生成文句使用Sample效果更好，输出最大概率会一直输出重复的句子。</p><p><img src="https://image.yinan.fun/202309152150365.png" /></p></li><li><p>語音合成也需要 Sampling !</p><p>在推理的时候加入dropout可以获得更高的结果</p><p><ahref="https://arxiv.org/abs/1712.05884">https://arxiv.org/abs/1712.05884</a></p><p><img src="https://image.yinan.fun/202309152150366.png" /></p></li><li><p>Diffusion Model 是一種 <strong>Autoregressive</strong></p></li></ul><p><img src="https://image.yinan.fun/202309152150367.png" /></p><blockquote><p>Auto-regression is a time series model that <strong>uses observationsfrom previous time steps as input</strong> to a regression equation topredict the value at the next time step</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>李宏毅</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux命令大全</title>
    <link href="/Linux%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8.html"/>
    <url>/Linux%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8.html</url>
    
    <content type="html"><![CDATA[<h1 id="linux-常用操作命令大全">Linux 常用操作命令大全</h1><p>https://blog.csdn.net/m0_46422300/article/details/104645072</p><h1 id="一基础知识">一、基础知识</h1><h2 id="linux系统的文件结构">1.1 Linux系统的文件结构</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">/bin        二进制文件，系统常规命令<br>/boot       系统启动分区，系统启动时读取的文件<br>/dev        设备文件<br>/etc        大多数配置文件<br>/home       普通用户的家目录<br>/lib        32位函数库<br>/lib64      64位库<br>/media      手动临时挂载点<br>/mnt        手动临时挂载点<br>/opt        第三方软件安装位置<br>/proc       进程信息及硬件信息<br>/root       临时设备的默认挂载点<br>/sbin       系统管理命令<br>/srv        数据<br>/var        数据<br>/sys        内核相关信息<br>/tmp        临时文件<br>/usr        用户相关设定<br></code></pre></td></tr></table></figure><h2 id="linux系统命令行的含义">1.2 Linux系统命令行的含义</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs awk">示例：root@app00:~<span class="hljs-comment"># </span><br>root    <span class="hljs-regexp">//</span>用户名，root为超级用户<br>@       <span class="hljs-regexp">//</span>分隔符<br>app00   <span class="hljs-regexp">//</span>主机名称<br>~       <span class="hljs-regexp">//</span>当前所在目录，默认用户目录为~，会随着目录切换而变化，例如：（root@app00:/bin<span class="hljs-comment"># ，当前位置在bin目录下）</span><br><span class="hljs-comment">#       //表示当前用户是超级用户，普通用户为$，例如：（&quot;yao@app00:/root$&quot; ，表示使用用户&quot;yao&quot;访问/root文件夹）</span><br></code></pre></td></tr></table></figure><h2 id="命令的组成">1.3 命令的组成</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">示例：命令 参数名 参数值<br></code></pre></td></tr></table></figure><h1 id="二基础操作">二、基础操作</h1><h2 id="重启系统">2.1 重启系统</h2><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs arduino">(<span class="hljs-number">1</span>)立刻关机<br>  shutdown -h now 或者 <span class="hljs-built_in">poweroff</span><br>(<span class="hljs-number">2</span>)两分钟后关机<br>  shutdown -h <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><h2 id="关闭系统">2.2 关闭系统</h2><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs arduino">(<span class="hljs-number">1</span>)立刻重启<br>  shutdown -r now 或者 <span class="hljs-built_in">reboot</span><br>(<span class="hljs-number">2</span>)两分钟后重启<br>  shutdown -r <span class="hljs-number">2</span> <br></code></pre></td></tr></table></figure><h2 id="帮助命令help">2.3 帮助命令（help）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">ifconfig  --help     <span class="hljs-regexp">//</span>查看 ifconfig 命令的用法<br></code></pre></td></tr></table></figure><h2 id="命令说明书man">2.4 命令说明书（man）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">man shutdown         <span class="hljs-regexp">//</span>打开命令说明后，可按<span class="hljs-string">&quot;q&quot;</span>键退出<br></code></pre></td></tr></table></figure><h2 id="切换用户su">2.5 切换用户（su）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">su yao               <span class="hljs-regexp">//</span>切换为用户<span class="hljs-string">&quot;yao&quot;</span>,输入后回车需要输入该用户的密码<br><span class="hljs-keyword">exit</span>                 <span class="hljs-regexp">//</span>退出当前用户<br></code></pre></td></tr></table></figure><h1 id="三目录操作">三、目录操作</h1><h2 id="切换目录cd">3.1 切换目录（cd）</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> /                 <span class="hljs-string">//</span>切换到根目录<br><span class="hljs-keyword">cd</span> <span class="hljs-string">/bin</span>              <span class="hljs-string">//</span>切换到根目录下的bin目录<br><span class="hljs-keyword">cd</span> <span class="hljs-string">../</span>               <span class="hljs-string">//</span>切换到上一级目录 或者使用命令：<span class="hljs-keyword">cd</span> <span class="hljs-string">..</span><br><span class="hljs-keyword">cd</span> ~                 <span class="hljs-string">//</span>切换到home目录<br><span class="hljs-keyword">cd</span> -                 <span class="hljs-string">//</span>切换到上次访问的目录<br><span class="hljs-keyword">cd</span> xx<span class="hljs-params">(文件夹名)</span>       <span class="hljs-string">//</span>切换到本目录下的名为xx的文件目录，如果目录不存在报错<br><span class="hljs-keyword">cd</span> <span class="hljs-string">/xxx/xx/x</span>         <span class="hljs-string">//</span>可以输入完整的路径，直接切换到目标目录，输入过程中可以使用tab键快速补全<br></code></pre></td></tr></table></figure><h2 id="查看目录ls">3.2 查看目录（ls）</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">ls</span>                   <span class="hljs-string">//</span>查看当前目录下的所有目录和文件<br><span class="hljs-keyword">ls</span> -a                <span class="hljs-string">//</span>查看当前目录下的所有目录和文件（包括隐藏的文件）<br><span class="hljs-keyword">ls</span> -l                <span class="hljs-string">//</span>列表查看当前目录下的所有目录和文件（列表查看，显示更多信息），与命令<span class="hljs-string">&quot;ll&quot;</span>效果一样<br><span class="hljs-keyword">ls</span> <span class="hljs-string">/bin</span>              <span class="hljs-string">//</span>查看指定目录下的所有目录和文件 <br></code></pre></td></tr></table></figure><h2 id="创建目录mkdir">3.3 创建目录（mkdir）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">mkdir tools          <span class="hljs-regexp">//</span>在当前目录下创建一个名为tools的目录<br>mkdir <span class="hljs-regexp">/bin/</span>tools     <span class="hljs-regexp">//</span>在指定目录下创建一个名为tools的目录<br></code></pre></td></tr></table></figure><h2 id="删除目录与文件rm">3.3 删除目录与文件（rm）</h2><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">rm</span> 文件名              <span class="hljs-comment">//删除当前目录下的文件</span><br><span class="hljs-keyword">rm</span> -f 文件名           <span class="hljs-comment">//删除当前目录的的文件（不询问）</span><br><span class="hljs-keyword">rm</span> -r 文件夹名         <span class="hljs-comment">//递归删除当前目录下此名的目录</span><br><span class="hljs-keyword">rm</span> -rf 文件夹名        <span class="hljs-comment">//递归删除当前目录下此名的目录（不询问）</span><br><span class="hljs-keyword">rm</span> -rf *              <span class="hljs-comment">//将当前目录下的所有目录和文件全部删除</span><br><span class="hljs-keyword">rm</span> -rf <span class="hljs-comment">/*             //将根目录下的所有文件全部删除【慎用！相当于格式化系统】</span><br></code></pre></td></tr></table></figure><h2 id="修改目录mv">3.4 修改目录（mv）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">mv 当前目录名 新目录名        <span class="hljs-regexp">//</span>修改目录名，同样适用与文件操作<br> <span class="hljs-regexp">/usr/</span>tmp<span class="hljs-regexp">/tool /</span>opt       <span class="hljs-regexp">//</span>将<span class="hljs-regexp">/usr/</span>tmp目录下的tool目录剪切到 /opt目录下面<br>mv -r <span class="hljs-regexp">/usr/</span>tmp<span class="hljs-regexp">/tool /</span>opt    <span class="hljs-regexp">//</span>递归剪切目录中所有文件和文件夹<br></code></pre></td></tr></table></figure><h2 id="拷贝目录cp">3.5 拷贝目录（cp）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">cp <span class="hljs-regexp">/usr/</span>tmp<span class="hljs-regexp">/tool /</span>opt       <span class="hljs-regexp">//</span>将<span class="hljs-regexp">/usr/</span>tmp目录下的tool目录复制到 /opt目录下面<br>cp -r <span class="hljs-regexp">/usr/</span>tmp<span class="hljs-regexp">/tool /</span>opt    <span class="hljs-regexp">//</span>递归剪复制目录中所有文件和文件夹<br></code></pre></td></tr></table></figure><h2 id="搜索目录find">3.6 搜索目录（find）</h2><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gradle"><span class="hljs-keyword">find</span> <span class="hljs-regexp">/bin -name &#x27;a*&#x27;        /</span><span class="hljs-regexp">/查找/</span>bin目录下的所有以a开头的文件或者目录<br></code></pre></td></tr></table></figure><h2 id="查看当前目录pwd">3.7 查看当前目录（pwd）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">pwd                         <span class="hljs-regexp">//</span>显示当前位置路径<br></code></pre></td></tr></table></figure><h1 id="四文件操作">四、文件操作</h1><h2 id="新增文件touch">4.1 新增文件（touch）</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">touch  <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>         <span class="hljs-comment">//在当前目录下创建名为a的txt文件（文件不存在），如果文件存在，将文件时间属性修改为当前系统时间</span><br></code></pre></td></tr></table></figure><h2 id="删除文件rm">4.2 删除文件（rm）</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">rm 文件名              <span class="hljs-regexp">//</span>删除当前目录下的文件<br>rm -f 文件名           <span class="hljs-regexp">//</span>删除当前目录的的文件（不询问）<br></code></pre></td></tr></table></figure><h2 id="编辑文件vivim">4.3 编辑文件（vi、vim）</h2><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">vi</span> 文件名              //打开需要编辑的文件<br>--进入后，操作界面有三种模式：命令模式（<span class="hljs-keyword">command</span> <span class="hljs-keyword">mode</span>）、插入模式（Insert <span class="hljs-keyword">mode</span>）和底行模式（<span class="hljs-keyword">last</span> <span class="hljs-built_in">line</span> <span class="hljs-keyword">mode</span>）<br>命令模式<br>-刚进入文件就是命令模式，通过方向键控制光标位置，<br>-使用命令<span class="hljs-string">&quot;dd&quot;</span>删除当前整行<br>-使用命令<span class="hljs-string">&quot;/字段&quot;</span>进行查找<br>-按<span class="hljs-string">&quot;i&quot;</span>在光标所在字符前开始插入<br>-按<span class="hljs-string">&quot;a&quot;</span>在光标所在字符后开始插入<br>-按<span class="hljs-string">&quot;o&quot;</span>在光标所在行的下面另起一新行插入<br>-按<span class="hljs-string">&quot;：&quot;</span>进入底行模式<br>插入模式<br>-此时可以对文件内容进行编辑，左下角会显示 <span class="hljs-string">&quot;-- 插入 --&quot;</span><span class="hljs-comment">&quot;</span><br>-按<span class="hljs-string">&quot;ESC&quot;</span>进入底行模式<br>底行模式<br>-退出编辑：      :q<br>-强制退出：      :q!<br>-保存并退出：    :<span class="hljs-keyword">wq</span><br>## 操作步骤示例 ##<br><span class="hljs-number">1</span>.保存文件：按<span class="hljs-string">&quot;ESC&quot;</span> -&gt; 输入<span class="hljs-string">&quot;:&quot;</span> -&gt; 输入<span class="hljs-string">&quot;wq&quot;</span>,回车     //保存并退出编辑<br><span class="hljs-number">2</span>.取消操作：按<span class="hljs-string">&quot;ESC&quot;</span> -&gt; 输入<span class="hljs-string">&quot;:&quot;</span> -&gt; 输入<span class="hljs-string">&quot;q!&quot;</span>,回车     //撤销本次修改并退出编辑<br>## 补充 ##<br><span class="hljs-keyword">vim</span> +<span class="hljs-number">10</span> filename.txt                   //打开文件并跳到第<span class="hljs-number">10</span>行<br><span class="hljs-keyword">vim</span> -R /etc/passwd                     //以只读模式打开文件<br></code></pre></td></tr></table></figure><h2 id="查看文件">4.4 查看文件</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">cat <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>          <span class="hljs-comment">//查看文件最后一屏内容</span><br>less <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>         <span class="hljs-comment">//PgUp向上翻页，PgDn向下翻页，&quot;q&quot;退出查看</span><br>more <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>         <span class="hljs-comment">//显示百分比，回车查看下一行，空格查看下一页，&quot;q&quot;退出查看</span><br>tail -<span class="hljs-number">100</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>    <span class="hljs-comment">//查看文件的后100行，&quot;Ctrl+C&quot;退出查看</span><br></code></pre></td></tr></table></figure><h1 id="五文件权限">五、文件权限</h1><h2 id="权限说明">5.1 权限说明</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs clean">文件权限简介：<span class="hljs-string">&#x27;r&#x27;</span> 代表可读（<span class="hljs-number">4</span>），<span class="hljs-string">&#x27;w&#x27;</span> 代表可写（<span class="hljs-number">2</span>），<span class="hljs-string">&#x27;x&#x27;</span> 代表执行权限（<span class="hljs-number">1</span>），括号内代表<span class="hljs-string">&quot;8421法&quot;</span><br>##文件权限信息示例：-rwxrw-r--<br>-第一位：<span class="hljs-string">&#x27;-&#x27;</span>就代表是文件，<span class="hljs-string">&#x27;d&#x27;</span>代表是文件夹<br>-第一组三位：拥有者的权限<br>-第二组三位：拥有者所在的组，组员的权限<br>-第三组三位：代表的是其他用户的权限<br></code></pre></td></tr></table></figure><h2 id="文件权限">5.2 文件权限</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">普通授权    chmod +x <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>    <br><span class="hljs-number">8421</span>法     chmod <span class="hljs-number">777</span> <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.txt</span>     <span class="hljs-comment">//1+2+4=7，&quot;7&quot;说明授予所有权限</span><br></code></pre></td></tr></table></figure><h1 id="六打包与解压">六、打包与解压</h1><h2 id="说明">6.1 说明</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">.zip</span>、<span class="hljs-string">.rar</span>        <span class="hljs-string">//windows</span>系统中压缩文件的扩展名<br><span class="hljs-string">.tar</span>              <span class="hljs-string">//Linux</span>中打包文件的扩展名<br><span class="hljs-string">.gz</span>               <span class="hljs-string">//Linux</span>中压缩文件的扩展名<br><span class="hljs-string">.tar.gz</span>           <span class="hljs-string">//Linux</span>中打包并压缩文件的扩展名<br></code></pre></td></tr></table></figure><h2 id="打包文件">6.2 打包文件</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">tar -zcvf 打包压缩后的文件名 要打包的文件<br>参数说明：z：调用gzip压缩命令进行压缩; c：打包文件; v：显示运行过程; f：指定文件名;<br>示例：<br>tar -zcvf a.tar file1 file2,...      <span class="hljs-regexp">//</span>多个文件压缩打包<br></code></pre></td></tr></table></figure><h2 id="解压文件">6.3 解压文件</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">tar -zxvf <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.tar</span>                      <span class="hljs-comment">//解包至当前目录</span><br>tar -zxvf <span class="hljs-selector-tag">a</span><span class="hljs-selector-class">.tar</span> -C /usr------        <span class="hljs-comment">//指定解压的位置</span><br>unzip test<span class="hljs-selector-class">.zip</span>             <span class="hljs-comment">//解压*.zip文件 </span><br>unzip -l test<span class="hljs-selector-class">.zip</span>          <span class="hljs-comment">//查看*.zip文件的内容 </span><br></code></pre></td></tr></table></figure><h1 id="七其他常用命令">七、其他常用命令</h1><h2 id="find">7.1 find</h2><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sqf"><span class="hljs-built_in">find</span> . -<span class="hljs-built_in">name</span> <span class="hljs-string">&quot;*.c&quot;</span>     <span class="hljs-comment">//将目前目录及其子目录下所有延伸档名是 c 的文件列出来</span><br><span class="hljs-built_in">find</span> . -<span class="hljs-built_in">type</span> f         <span class="hljs-comment">//将目前目录其其下子目录中所有一般文件列出</span><br><span class="hljs-built_in">find</span> . -ctime -<span class="hljs-number">20</span>      <span class="hljs-comment">//将目前目录及其子目录下所有最近 20 天内更新过的文件列出</span><br><span class="hljs-built_in">find</span> /var/<span class="hljs-built_in">log</span> -<span class="hljs-built_in">type</span> f -mtime +<span class="hljs-number">7</span> -ok rm &#123;&#125; \;     <span class="hljs-comment">//查找/var/log目录中更改时间在7日以前的普通文件，并在删除之前询问它们</span><br><span class="hljs-built_in">find</span> . -<span class="hljs-built_in">type</span> f -perm <span class="hljs-number">644</span> -<span class="hljs-built_in">exec</span> ls -l &#123;&#125; \;       <span class="hljs-comment">//查找前目录中文件属主具有读、写权限，并且文件所属组的用户和其他用户具有读权限的文件</span><br><span class="hljs-built_in">find</span> / -<span class="hljs-built_in">type</span> f -<span class="hljs-built_in">size</span> <span class="hljs-number">0</span> -<span class="hljs-built_in">exec</span> ls -l &#123;&#125; \;         <span class="hljs-comment">//为了查找系统中所有文件长度为0的普通文件，并列出它们的完整路径</span><br></code></pre></td></tr></table></figure><h2 id="whereis">7.2 whereis</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">whereis <span class="hljs-keyword">ls</span>             <span class="hljs-string">//</span>将和<span class="hljs-keyword">ls</span>文件相关的文件都查找出来<br></code></pre></td></tr></table></figure><h2 id="which">7.3 which</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">说明：which指令会在环境变量<span class="hljs-variable">$PATH</span>设置的目录里查找符合条件的文件。<br>which bash             <span class="hljs-regexp">//</span>查看指令<span class="hljs-string">&quot;bash&quot;</span>的绝对路径<br></code></pre></td></tr></table></figure><h2 id="sudo">7.4 sudo</h2><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs axapta">说明：sudo命令以系统管理者的身份执行指令，也就是说，经由 sudo 所执行的指令就好像是 root 亲自执行。需要输入自己账户密码。<br>使用权限：在 /etc/sudoers 中有出现的使用者<br>sudo -l                              <span class="hljs-comment">//列出目前的权限</span><br>$ sudo -u yao vi ~www/<span class="hljs-keyword">index</span>.html    <span class="hljs-comment">//以 yao 用户身份编辑  home 目录下www目录中的 index.html 文件</span><br></code></pre></td></tr></table></figure><h2 id="grep">7.5 grep</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">grep -i <span class="hljs-string">&quot;the&quot;</span> demo_file              <span class="hljs-regexp">//</span>在文件中查找字符串(不区分大小写)<br>grep -A <span class="hljs-number">3</span> -i <span class="hljs-string">&quot;example&quot;</span> demo_text     <span class="hljs-regexp">//</span>输出成功匹配的行，以及该行之后的三行<br>grep -r <span class="hljs-string">&quot;ramesh&quot;</span> *                   <span class="hljs-regexp">//</span>在一个文件夹中递归查询包含指定字符串的文件<br></code></pre></td></tr></table></figure><h2 id="service">7.6 service</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">说明：service命令用于运行System V init脚本，这些脚本一般位于<span class="hljs-regexp">/etc/i</span>nit.d文件下，这个命令可以直接运行这个文件夹里面的脚本，而不用加上路径<br>service ssh status      <span class="hljs-regexp">//</span>查看服务状态 <br>service --status-all    <span class="hljs-regexp">//</span>查看所有服务状态 <br>service ssh restart     <span class="hljs-regexp">//</span>重启服务 <br></code></pre></td></tr></table></figure><h2 id="free">7.7 free</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">说明：这个命令用于显示系统当前内存的使用情况，包括已用内存、可用内存和交换内存的情况 <br>free -g            <span class="hljs-regexp">//</span>以G为单位输出内存的使用量，-g为GB，-m为MB，-k为KB，-b为字节 <br>free -t            <span class="hljs-regexp">//</span>查看所有内存的汇总<br></code></pre></td></tr></table></figure><h2 id="top">7.8 top</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">top               <span class="hljs-regexp">//</span>显示当前系统中占用资源最多的一些进程, shift+m 按照内存大小查看<br></code></pre></td></tr></table></figure><h2 id="df">7.9 df</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">说明：显示文件系统的磁盘使用情况<br>df -h            <span class="hljs-regexp">//</span>一种易看的显示<br></code></pre></td></tr></table></figure><h2 id="mount">7.10 mount</h2><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">mount <span class="hljs-string">/dev/sdb1</span> <span class="hljs-string">/u01</span>              <span class="hljs-string">//</span>挂载一个文件系统，需要先创建一个目录，然后将这个文件系统挂载到这个目录上<br>dev/sdb1 <span class="hljs-string">/u01</span> ext2 defaults 0 2   <span class="hljs-string">//</span>添加到fstab中进行自动挂载，这样任何时候系统重启的时候，文件系统都会被加载 <br></code></pre></td></tr></table></figure><h2 id="uname">7.11 uname</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">说明：<span class="hljs-built_in">uname</span>可以显示一些重要的系统信息，例如内核名称、主机名、内核版本号、处理器类型之类的信息 <br><span class="hljs-built_in">uname</span> -a<br></code></pre></td></tr></table></figure><h2 id="yum">7.12 yum</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">说明：安装插件命令<br>yum install httpd      <span class="hljs-regexp">//</span>使用yum安装apache <br>yum update httpd       <span class="hljs-regexp">//</span>更新apache <br>yum remove httpd       <span class="hljs-regexp">//</span>卸载/删除apache <br></code></pre></td></tr></table></figure><h2 id="rpm">7.13 rpm</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs stylus">说明：插件安装命令<br>rpm -ivh httpd-<span class="hljs-number">2.2</span>.<span class="hljs-number">3</span>-<span class="hljs-number">22.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.el5</span><span class="hljs-selector-class">.i386</span><span class="hljs-selector-class">.rpm</span>      <span class="hljs-comment">//使用rpm文件安装apache </span><br>rpm -uvh httpd-<span class="hljs-number">2.2</span>.<span class="hljs-number">3</span>-<span class="hljs-number">22.0</span>.<span class="hljs-number">1</span><span class="hljs-selector-class">.el5</span><span class="hljs-selector-class">.i386</span><span class="hljs-selector-class">.rpm</span>      <span class="hljs-comment">//使用rpm更新apache </span><br>rpm -ev httpd                                 <span class="hljs-comment">//卸载/删除apache </span><br></code></pre></td></tr></table></figure><h2 id="date">7.14 date</h2><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade"><span class="hljs-built_in">date</span> -s <span class="hljs-string">&quot;01/31/2010 23:59:53&quot;</span>   <span class="hljs-comment">///设置系统时间</span><br></code></pre></td></tr></table></figure><h2 id="wget">7.15 wget</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs awk">说明：使用wget从网上下载软件、音乐、视频 <br>示例：wget http:<span class="hljs-regexp">//</span>prdownloads.sourceforge.net<span class="hljs-regexp">/sourceforge/</span>nagios/nagios-<span class="hljs-number">3.2</span>.<span class="hljs-number">1</span>.tar.gz<br><span class="hljs-regexp">//</span>下载文件并以指定的文件名保存文件<br>wget -O nagios.tar.gz http:<span class="hljs-regexp">//</span>prdownloads.sourceforge.net<span class="hljs-regexp">/sourceforge/</span>nagios/nagios-<span class="hljs-number">3.2</span>.<span class="hljs-number">1</span>.tar.gz<br></code></pre></td></tr></table></figure><h2 id="ftp">7.16 ftp</h2><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">ftp</span> <span class="hljs-built_in">IP</span>/hostname    <span class="hljs-comment">//访问ftp服务器</span><br><span class="hljs-keyword">mls</span> *.html -       <span class="hljs-comment">//显示远程主机上文件列表</span><br></code></pre></td></tr></table></figure><h2 id="scp">7.17 scp</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">scp <span class="hljs-regexp">/opt/</span>data.txt  <span class="hljs-number">192.168</span>.<span class="hljs-number">1.101</span>:<span class="hljs-regexp">/opt/</span>    <span class="hljs-regexp">//</span>将本地opt目录下的data文件发送到<span class="hljs-number">192.168</span>.<span class="hljs-number">1.101</span>服务器的opt目录下<br></code></pre></td></tr></table></figure><h1 id="八系统管理">八、系统管理</h1><h2 id="防火墙操作">8.1 防火墙操作</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs awk">service iptables status      <span class="hljs-regexp">//</span>查看iptables服务的状态<br>service iptables start       <span class="hljs-regexp">//</span>开启iptables服务<br>service iptables stop        <span class="hljs-regexp">//</span>停止iptables服务<br>service iptables restart     <span class="hljs-regexp">//</span>重启iptables服务<br>chkconfig iptables off       <span class="hljs-regexp">//</span>关闭iptables服务的开机自启动<br>chkconfig iptables on        <span class="hljs-regexp">//</span>开启iptables服务的开机自启动<br><span class="hljs-comment">##centos7 防火墙操作</span><br>systemctl status firewalld.service     <span class="hljs-regexp">//</span>查看防火墙状态<br>systemctl stop firewalld.service       <span class="hljs-regexp">//</span>关闭运行的防火墙<br>systemctl disable firewalld.service    <span class="hljs-regexp">//</span>永久禁止防火墙服务<br></code></pre></td></tr></table></figure><h2 id="修改主机名centos-7">8.2 修改主机名（CentOS 7）</h2><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dsconfig"><span class="hljs-string">hostnamectl</span> <span class="hljs-built_in">set-hostname</span> 主机名<br></code></pre></td></tr></table></figure><h2 id="查看网络">8.3 查看网络</h2><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ifconfig</span><br></code></pre></td></tr></table></figure><h2 id="修改ip">8.4 修改IP</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs awk">修改网络配置文件，文件地址：<span class="hljs-regexp">/etc/</span>sysconfig<span class="hljs-regexp">/network-scripts/i</span>fcfg-eth0<br>------------------------------------------------<br>主要修改以下配置：  <br>TYPE=Ethernet               <span class="hljs-regexp">//</span>网络类型<br>BOOTPROTO=static            <span class="hljs-regexp">//</span>静态IP<br>DEVICE=ens00                <span class="hljs-regexp">//</span>网卡名<br>IPADDR=<span class="hljs-number">192.168</span>.<span class="hljs-number">1.100</span>        <span class="hljs-regexp">//</span>设置的IP<br>NETMASK=<span class="hljs-number">255.255</span>.<span class="hljs-number">255.0</span>       <span class="hljs-regexp">//</span>子网掩码<br>GATEWAY=<span class="hljs-number">192.168</span>.<span class="hljs-number">1.1</span>         <span class="hljs-regexp">//</span>网关<br>DNS1=<span class="hljs-number">192.168</span>.<span class="hljs-number">1.1</span>            <span class="hljs-regexp">//</span>DNS<br>DNS2=<span class="hljs-number">8.8</span>.<span class="hljs-number">8.8</span>                <span class="hljs-regexp">//</span>备用DNS<br>ONBOOT=yes                  <span class="hljs-regexp">//</span>系统启动时启动此设置<br>-------------------------------------------------<br>修改保存以后使用命令重启网卡：service network restart<br></code></pre></td></tr></table></figure><h2 id="配置映射">8.5 配置映射</h2><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dns">修改文件： vi /etc/hosts<br>在文件最后添加映射地址，示例如下：<br> <span class="hljs-number">192.168.1.101</span>  node1<br> <span class="hljs-number">192.168.1.102</span>  node2<br> <span class="hljs-number">192.168.1.103</span>  node3<br>配置好以后保存退出，输入命令：ping node1 ，可见实际 ping 的是 <span class="hljs-number">192.168.1.101</span>。<br></code></pre></td></tr></table></figure><h2 id="查看进程">8.6 查看进程</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">ps -ef         <span class="hljs-regexp">//</span>查看所有正在运行的进程<br></code></pre></td></tr></table></figure><h2 id="结束进程">8.7 结束进程</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">kill pid       <span class="hljs-regexp">//</span>杀死该pid的进程<br>kill -<span class="hljs-number">9</span> pid    <span class="hljs-regexp">//</span>强制杀死该进程   <br></code></pre></td></tr></table></figure><h2 id="查看链接">8.8 查看链接</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs awk">ping IP        <span class="hljs-regexp">//</span>查看与此IP地址的连接情况<br>netstat -an    <span class="hljs-regexp">//</span>查看当前系统端口<br>netstat -an | grep <span class="hljs-number">8080</span>     <span class="hljs-regexp">//</span>查看指定端口<br></code></pre></td></tr></table></figure><h2 id="快速清屏">8.9 快速清屏</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">ctrl+l        <span class="hljs-regexp">//</span>清屏，往上翻可以查看历史操作<br></code></pre></td></tr></table></figure><h2 id="远程主机">8.10 远程主机</h2><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">ssh</span> <span class="hljs-built_in">IP</span>       <span class="hljs-comment">//远程主机，需要输入用户名和密码</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>备忘大全</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
      <tag>命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>训练奖励模型</title>
    <link href="/%E8%AE%AD%E7%BB%83%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B.html"/>
    <url>/%E8%AE%AD%E7%BB%83%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B.html</url>
    
    <content type="html"><![CDATA[<h1 id="训练奖励模型">训练奖励模型</h1><p>因为之前找的训练好的奖励模型不能在<ahref="https://github.com/hpcaitech/ColossalAI">ColossalAI</a>中的强化学习阶段使用，并且是用纯英文数据集训练的，因此我们用<ahref="https://github.com/hpcaitech/ColossalAI">ColossalAI</a>的代码和中文数据集训练了奖励模型。</p><h2 id="数据集">数据集</h2><p>基于anthropic的Training a Helpful and Harmless Assistant withReinforcement Learning from Human Feedback论文开源的关于有助和无害的人类偏好数据hh-rlhf的翻译成中文的<ahref="https://huggingface.co/datasets/dikw/hh_rlhf_cn">hh_rlhf_cn</a>。</p><p>中文+英文数据集，使得同时能够识别中英文偏好。</p><figure><img src="https://image.yinan.fun/202309140020191.png"alt="hh_rlhf_cn数据集" /><figcaption aria-hidden="true">hh_rlhf_cn数据集</figcaption></figure><p>将数据集的context与chosen、regected合并，作为奖励模型的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, load_from_disk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">merge_prefix</span>(<span class="hljs-params">example</span>):<br>    context_all = <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> context <span class="hljs-keyword">in</span> example[<span class="hljs-string">&quot;context&quot;</span>]:<br>        context_all += <span class="hljs-string">&quot;\n\n&quot;</span> + context[<span class="hljs-string">&quot;role&quot;</span>] + <span class="hljs-string">&quot;: &quot;</span> + context[<span class="hljs-string">&quot;text&quot;</span>];<br>    example[<span class="hljs-string">&quot;chosen&quot;</span>] = context_all + <span class="hljs-string">&quot;\n\n&quot;</span> + example[<span class="hljs-string">&quot;chosen&quot;</span>][<span class="hljs-string">&quot;role&quot;</span>] + <span class="hljs-string">&quot;: &quot;</span> + example[<span class="hljs-string">&quot;chosen&quot;</span>][<span class="hljs-string">&quot;text&quot;</span>];<br>    example[<span class="hljs-string">&quot;rejected&quot;</span>] = context_all + <span class="hljs-string">&quot;\n\n&quot;</span> + example[<span class="hljs-string">&quot;rejected&quot;</span>][<span class="hljs-string">&quot;role&quot;</span>] + <span class="hljs-string">&quot;: &quot;</span> + example[<span class="hljs-string">&quot;rejected&quot;</span>][<span class="hljs-string">&quot;text&quot;</span>];<br>    <span class="hljs-keyword">return</span> example<br><br>data = load_dataset(<span class="hljs-string">&quot;/Volumes/T7/数据集/hh_rlhf_cn&quot;</span>)<br>new_data = data.<span class="hljs-built_in">map</span>(merge_prefix, remove_columns=[<span class="hljs-string">&quot;context&quot;</span>])<br>new_data.save_to_disk(<span class="hljs-string">&quot;/Volumes/T7/数据集/hh_rlhf_cn_processed&quot;</span>)<br><br><span class="hljs-comment"># data = load_from_disk(&quot;/Volumes/T7/数据集/hh_rlhf_cn_processed&quot;)</span><br></code></pre></td></tr></table></figure><h2 id="代码">代码</h2><p>使用<ahref="https://github.com/hpcaitech/ColossalAI">ColossalAI</a>中的代码</p><h3 id="奖励模型代码">奖励模型代码</h3><p><ahref="https://huggingface.co/bigscience/bloom-560m">bloom-560m</a>语言模型+输出大小为1的线性层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BLOOMRM</span>(<span class="hljs-title class_ inherited__">RewardModel</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    BLOOM Reward model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pretrained (str): Pretrained model name or path.</span><br><span class="hljs-string">        config (BloomConfig): Model config.</span><br><span class="hljs-string">        lora_rank (int): LoRA rank.</span><br><span class="hljs-string">        lora_train_bias (str): LoRA bias training mode.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 pretrained: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 config: <span class="hljs-type">Optional</span>[BloomConfig] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 lora_rank: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">                 lora_train_bias: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;none&#x27;</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> pretrained <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            model = BloomModel.from_pretrained(pretrained)<br>        <span class="hljs-keyword">elif</span> config <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            model = BloomModel(config)<br>        <span class="hljs-keyword">else</span>:<br>            model = BloomModel(BloomConfig())<br><br>        value_head = nn.Linear(model.config.hidden_size, <span class="hljs-number">1</span>)<br>        value_head.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1</span> / (model.config.hidden_size + <span class="hljs-number">1</span>))<br>        <span class="hljs-built_in">super</span>().__init__(model, value_head, lora_rank, lora_train_bias)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RewardModel</span>(<span class="hljs-title class_ inherited__">LoRAModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Reward model base class.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        model (nn.Module): Reward model.</span><br><span class="hljs-string">        value_head (nn.Module): Value head to get reward score.</span><br><span class="hljs-string">        lora_rank (int): LoRA rank.</span><br><span class="hljs-string">        lora_train_bias (str): LoRA bias training mode.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                 model: nn.Module,</span><br><span class="hljs-params">                 value_head: <span class="hljs-type">Optional</span>[nn.Module] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 lora_rank: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">                 lora_train_bias: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;none&#x27;</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(lora_rank=lora_rank, lora_train_bias=lora_train_bias)<br>        self.model = model<br>        self.convert_to_lora()<br><br>        <span class="hljs-keyword">if</span> value_head <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> value_head.out_features != <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The value head of reward model&#x27;s output dim should be 1!&quot;</span>)<br>            self.value_head = value_head<br>        <span class="hljs-keyword">else</span>:<br>            self.value_head = nn.Linear(model.config.n_embd, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sequences: torch.LongTensor, attention_mask: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span></span>) -&gt; torch.Tensor:<br>        outputs = self.model(sequences, attention_mask=attention_mask)<br>        last_hidden_states = outputs[<span class="hljs-string">&#x27;last_hidden_state&#x27;</span>]<br>        values = self.value_head(last_hidden_states)[:, :-<span class="hljs-number">1</span>]<br>        value = values.mean(dim=<span class="hljs-number">1</span>).squeeze(<span class="hljs-number">1</span>)    <span class="hljs-comment"># ensure shape is (B)</span><br>        <span class="hljs-keyword">return</span> value<br></code></pre></td></tr></table></figure><h3 id="损失函数代码">损失函数代码</h3><p><span class="math display">\[loss(\theta)=-\frac{1}{K \choose 2}E_{(x,y_w,y_l)\simD}[log(\sigma(r_\theta(x,y_w)-r_\theta(x,y_l)))]\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LogSigLoss</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Pairwise Loss for Reward Model</span><br><span class="hljs-string">    Details: https://arxiv.org/abs/2203.02155</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor</span>) -&gt; torch.Tensor:<br>        probs = torch.sigmoid(chosen_reward - reject_reward)<br>        log_probs = torch.log(probs)<br>        loss = -log_probs.mean()<br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><h3 id="代码运行参数">代码运行参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">torchrun --standalone --nproc_per_node=1 train_reward_model.py \<br>    --model &#x27;bloom&#x27; \<br>    --strategy colossalai_zero2 \<br>    --loss_fn &#x27;log_sig&#x27; \<br>    --dataset &#x27;Anthropic/hh-rlhf&#x27; \<br>    --pretrain ~/model/bloom-560m \<br>    --tokenizer ~/model/bloom-560m<br></code></pre></td></tr></table></figure><h3 id="运行结果">运行结果</h3><table><thead><tr class="header"><th>Step</th><th>Loss</th><th>Dist</th><th>Acc</th></tr></thead><tbody><tr class="odd"><td>344317</td><td>0.09899902</td><td>0.38238678</td><td>0.61886833</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Proximal Policy Optimization (PPO)</title>
    <link href="/ppo.html"/>
    <url>/ppo.html</url>
    
    <content type="html"><![CDATA[<h1 id="proximal-policy-optimization-ppo">Proximal Policy Optimization(PPO)</h1><h2 id="policy-gradient">Policy Gradient</h2><p><span class="math display">\[\tau=\left\{s_1, a_1, s_2, a_2, \cdots, s_T, a_T\right\}\]</span></p><p>当参数是<span class="math inline">\(\theta\)</span>的时候某一个trajectory 回合里面发生<spanclass="math inline">\(\tau\)</span>的几率：</p><p><span class="math display">\[\begin{aligned}p_\theta(\tau)&amp; =p\left(s_1\right) p_\theta\left(a_1 \mid s_1\right)p\left(s_2 \mid s_1, a_1\right) p_\theta\left(a_2 \mid s_2\right)p\left(s_3 \mid s_2, a_2\right) \cdots \\&amp; =p\left(s_1\right) \prod_{t=1}^T p_\theta\left(a_t \mid s_t\right)p\left(s_{t+1} \mid s_t, a_t\right) \\&amp;\end{aligned}\]</span> ExpectReward，穷举所有的Trajectory不同回合的Reward乘以不同回合的几率再求和就是期望Reward</p><p><span class="math display">\[\bar{R}_\theta=\sum_\tau R(\tau) p_\theta(\tau)=E_{\tau \simp_\theta(\tau)}[R(\tau)]\]</span> 要求期望Reward最大值采用梯度上升 <span class="math display">\[\begin{aligned}\nabla \bar{R}_\theta&amp;=\sum_\tau R(\tau) \nablap_\theta(\tau)=\sum_\tau R(\tau) p_\theta(\tau) \frac{\nablap_\theta(\tau)}{p_\theta(\tau)}\\&amp;=\sum_\tau R(\tau) p_\theta(\tau) \nabla \log p_\theta(\tau)\\&amp;=E_{\tau \sim p_\theta(\tau)}\left[R(\tau) \nabla \logp_\theta(\tau)\right]\approx \frac{1}{N} \sum_{n=1}^N R\left(\tau_n\right) \nabla \logp_\theta\left(\tau_n\right)\end{aligned}\]</span> 由于env的那部分<span class="math inline">\(p\left(s_{t+1} \mids_t, a_t\right)\)</span>和无关<spanclass="math inline">\(\theta\)</span>，所以无需对其做gradient，只需要对actor的那部分<spanclass="math inline">\(p_\theta\left(a_t \mids_t\right)\)</span>做gradient，所以：</p><p><span class="math display">\[\nabla \bar{R}_\theta=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n}R\left(\tau_n\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\]</span>这里为什么要用一个log呢？（也就是为什么要多一个再除以p的操作）直接对p取梯度不好吗？因为如果动作a很好但是很罕见，而动作b很多但是不太好，但是在求和操作的时候，就会把b的Reward求和变得很大，为了使得目标函数最大，模型就会偏好出现几率较高actionb，而这不是我们想要的。除以p的操作就是一个标准化的操作，出现次数越多也就是概率越大，除以自己就标准化了</p><figure><img src="https://image.yinan.fun/202309072110938" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="on-policy---off-policy">On-policy -&gt; Off-policy</h2><ul><li><p>On-policy: The agent learned and the agent interacting with theenvironment is the same.</p></li><li><p>Off-policy: The agent learned and the agent interacting with theenvironment is different.</p></li></ul><p>前面的policygradient的做法就是on-policy，即要learn的policy和与环境互动的policy是同一个。当update参数以后,从<spanclass="math inline">\(\theta\)</span>变成<spanclass="math inline">\(\theta&#39;\)</span>,那么之前sample出来的data就不能用了。Update一次参数，只能做一次gradientacent，而后再去 collect data，非常耗时，所以需要 offpolicy。off-policy可以拿一批data用好几次。</p><h3 id="importance-sampling">Importance Sampling</h3><p>从分布p中采样x，当p无法积分，那就约等于右边的均值</p><p><span class="math display">\[E_{x \sim p}[f(x)] \approx N \sum_{i=1}^N f\left(x^i\right)\]</span>假设：无法从p中采样data，只能从q中采样data，那么这个等式无法近似 <spanclass="math display">\[E_{x \sim p}[f(x)]=\int f(x) p(x) d x=\int f(x) \frac{p(x)}{q(x)} q(x) dx=E_{x \sim q}\left[f(x)\left[\frac{p(x)}{q(x)}\right]\right.\]</span> 变成从q 分布中采样data，<spanclass="math inline">\(\displaystyle{\frac{p(x)}{q(x)}}\)</span>相当于修正权重</p><h3 id="issue-of-importance-sampling">Issue of Importance Sampling</h3><p>期望相同，但是方差不同</p><p><span class="math display">\[V A R[X] { =E[X^{2}]-(E[X])^{2}}\]</span> <span class="math inline">\(f(x)\)</span>的方差 <spanclass="math display">\[\operatorname{Var}_{x\sim p}[f(x)]=E_{x\sim p}[f(x)^{2}]-\left(E_{x\simp}[f(x)]\right)^{2}\]</span> <spanclass="math inline">\(f(x)\displaystyle{\frac{p(x)}{q(x)}}\)</span>的方差<span class="math display">\[\begin{aligned}Var_{x\sim q}[f(x)\displaystyle{\frac{p(x)}{q(x)}}]&amp;=E_{x\simq}\left[\left(f(x)\displaystyle{\frac{p(x)}{q(x)}}\right)^{2}\right]-\left(E_{x\simq}\left[f(x)\displaystyle{\frac{p(x)}{q(x)}}\right]\right)^{2}\\&amp;={E_{x\simp}\left[f(x)^{2}\displaystyle{\left[\displaystyle{\frac{p(x)}{q(x)}}\right]}-(E_{x\sim p}[f(x)]\right)^{2}}\end{aligned}\]</span> 方差主要差别在第一项，也就是<spanclass="math inline">\(\displaystyle{\frac{p(x)}{q(x)}}\)</span>的值。所以如果p和q差很多，而且采样不够多，区别就差很大。当然，如果sample次数足够多，那么期望相同，也没有太大区别。</p><h3 id="off-policy">Off-policy</h3><p><span class="math display">\[\nabla \bar{R}_\theta=E_{\tau \simp_{\theta^{\prime}}(\tau)}\left[\frac{p_\theta(\tau)}{p_{\theta^{\prime}}(\tau)}R(\tau) \nabla \log p_\theta(\tau)\right]\]</span></p><p><span class="math inline">\(\theta&#39;\)</span>sample一次data,可以给 <span class="math inline">\(\theta\)</span>uptake很多次,完了以后再去 Sample data</p><h2 id="add-constraint">Add Constraint</h2><h2 id="actor-critic">Actor-Critic</h2><h3id="减小actor的方差causality方法">减小Actor的方差：Causality方法</h3><p>策略梯度的公式变为 <span class="math display">\[\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\thetalog\pi_\theta(a_{i,t}|s_{i,t})(\sum_{t&#39;=t}^{T}r(s_{i,t&#39;},a_{i,t&#39;}))\]</span> 即后一项关于的奖励的累加只累加当前时间步<spanclass="math inline">\(t\)</span>之后的奖励，直观的理解是<spanclass="math inline">\(t&#39;\)</span>时刻策略的改变不会影响到<spanclass="math inline">\(t&#39;\)</span>之前时间步的奖励</p><p>在上式中，对<span class="math inline">\(\sum_{t=1}^{T}\nabla_\thetalog\pi_\theta(a_{i,t}|s_{i,t})\)</span>使用的是MonteCarlo估计方法，这种方法方差大。将该项可以写成<spanclass="math inline">\(\hat{Q}_{i}(s_t,a_t)\)</span>，Q值代表未来的累计奖励的期望，可以使用值函数近似的方法来估计<spanclass="math inline">\(\hat{Q}_{i}(s_t,a_t)\)</span>，从而进一步减少方差。 <span class="math display">\[\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\thetalog\pi_\theta(a_{i,t}|s_{i,t})\hat{Q}_{i}(s_t,a_t)\]</span></p><h3 id="baseline方法">Baseline方法</h3><p>如果希望在上式的基础上，进一步减少方差，那么可以为<spanclass="math inline">\(\hat{Q}_{i}(s_t,a_t)\)</span> 添加baseline，将baseline记为<spanclass="math inline">\(b\)</span>，则策略梯度的公式为 <spanclass="math display">\[\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\thetalog\pi_\theta(a_{i,t}|s_{i,t})(\hat{Q}(s_t,a_t)-b)\]</span> 可以证明，只有在<spanclass="math inline">\(b\)</span>与动作<spanclass="math inline">\(a_t\)</span>无关的情况下，上述改进才与之前的策略梯度公式等价。<spanclass="math inline">\(b\)</span>一般选择为状态<spanclass="math inline">\(s_t\)</span>的值函数（还与<spanclass="math inline">\(\theta\)</span>有关？），即<spanclass="math inline">\(b=\hat{v}_i(s)\)</span> 。</p><p>当baseline是状态值函数时，策略梯度可以写成 <spanclass="math display">\[\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\thetalog\pi_\theta(a_{i,t}|s_{i,t})(\hat{Q}(s_t,a_t)-\hat{V}(s_t))\]</span> 其中， <spanclass="math inline">\(\hat{Q}(s_t,a_t)-\hat{V}(s_t)=\hat{A}(s_t,a_t)\)</span>称为AdvantageFunction，其中 <span class="math display">\[Q(s_t,a_t)=r(s_t,a_t)+\sum_{s_{t+1}}P(s_{t+1}|s_t,a_t)[V(s_{t+1})]\]</span>如果不考虑状态转移概率，用采样的方式来估计状态转移，则在当前策略参数下，<span class="math display">\[Q(s_t,a_t)\approx r(s_t,a_t)+V(s_{t+1})\]</span> 因此策略梯度公式可以进一步写成 <span class="math display">\[\nabla J(\theta)=\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}\nabla_\thetalog\pi_\theta(a_{i,t}|s_{i,t})(r(s_t,a_t)+\hat{V}(s_{t+1})-\hat{V}(s_t))\]</span> 在上式中，我们需要估计状态值函数<spanclass="math inline">\(\hat{V}(s_t)\)</span>的值。用于估计<spanclass="math inline">\(\hat{V}(s_t)\)</span>的部分被称为<strong>Critic</strong>。</p><p><strong>Actor-Critic的基本流程为：</strong> <spanclass="math display">\[采样\rightarrow 更新Critic参数\rightarrow 根据Critic计算AdvantageFunction \rightarrow 更新Actor参数\]</span></p><h2 id="参考">参考</h2><p><ahref="https://blog.51cto.com/u_15721703/5575736">https://blog.51cto.com/u_15721703/5575736</a></p><p><ahref="https://www.zhihu.com/question/56692640/answer/289913574">https://www.zhihu.com/question/56692640/answer/289913574</a></p><p>[https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/](</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fastboot安装驱动</title>
    <link href="/fastboot%E5%AE%89%E8%A3%85%E9%A9%B1%E5%8A%A8.html"/>
    <url>/fastboot%E5%AE%89%E8%A3%85%E9%A9%B1%E5%8A%A8.html</url>
    
    <content type="html"><![CDATA[<h1id="使用fastboot命令出现waiting-for-any-device">使用Fastboot命令出现&lt;waitingfor any device&gt;</h1><h2 id="可能是fastboot驱动未安装">可能是fastboot驱动未安装</h2><ol type="1"><li>打开设备管理器</li><li>找到<code>Android</code>（带黄色感叹号）</li><li>右键--&gt;<code>更新驱动程序</code></li><li>选择<code>浏览我的电脑以查找驱动程序</code></li><li>选择<code>让我从电脑上的可用驱动列表上选择</code></li><li>选择<code>通用串行总线设备</code></li><li>选择<code>WinUSB设备</code>--&gt;<code>ADB设备</code>--&gt;<code>下一步</code>--&gt;<code>是</code></li><li>点击<code>关闭</code></li></ol><p>设备管理器中的Android设备的黄色感叹号消失，驱动安装成功。</p>]]></content>
    
    
    <categories>
      
      <category>安卓刷机</category>
      
    </categories>
    
    
    <tags>
      
      <tag>安装刷机</tag>
      
      <tag>fastboot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分类问题中概率与分数</title>
    <link href="/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E6%A6%82%E7%8E%87%E4%B8%8E%E5%88%86%E6%95%B0.html"/>
    <url>/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E6%A6%82%E7%8E%87%E4%B8%8E%E5%88%86%E6%95%B0.html</url>
    
    <content type="html"><![CDATA[<h1 id="分类问题中概率与分数">分类问题中概率与分数</h1><h2 id="什么是分数">什么是分数</h2><p>对于二分类器来说，将其中一个类别称为正类，<strong>更高的分数意味着模型更有信心输入（observation）属于正类。</strong></p><h2 id="什么是概率">什么是概率</h2><p>概率可能很难定义。例如，如果我们做一个降雨预测器，它告诉我们今天下雨的几率为20%，我们如何判断它是否正确？在检查预测时，要么会下雨（要么不会下雨）。标准的方法是，如果我们采取看起来与今天相似的日子，那么其中20%的日子应该会下雨，但随后我们就会争论不同的日子必须有多相似才能被组合在一起。在更极端的情况下，例如预测选举结果，很难看出我们如何“重复”实验来赋予结果以意义。</p><p>在《信号与噪音》中，内特·西尔弗在第4章的天气预报中谈到了这个问题。与其担心是什么让日子彼此相似，不如采取不同的方法。从历史上看，在那些模型预测20%的时间会下雨日子里，实际下雨的时间百分比是多少？如果答案与20%大相径庭，那么模型的概率在实际估计概率方面做得不是很好。</p><p>在实践中，我们假设概率，因为可能只有一天正好有0.2000的降雨概率。如果我们把所有预测在17.5%到22.5%之间，并把它们组合在一个桶里，我们预计实际降雨量在17.5%到22.5%之间。</p><p>这种将预测概率与实际发生次数进行比较的过程称为校准（calibration）。校准曲线根据实际发生率绘制预测概率。</p><h2 id="概率校准曲线">概率校准曲线</h2><p>下雨预测的例子，使用<ahref="https://www.kaggle.com/datasets/rtatman/did-it-rain-in-seattle-19482017">Kaggle数据集</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">rain = pd.read_csv(<span class="hljs-string">&#x27;seattleWeather_1948-2017.csv&#x27;</span>).dropna()<br>rain.head()<br></code></pre></td></tr></table></figure><table><thead><tr class="header"><th style="text-align: left;"></th><th style="text-align: right;">DATE</th><th style="text-align: right;">PRCP</th><th style="text-align: right;">TMAX</th><th style="text-align: right;">TMIN</th><th>RAIN</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">0</td><td style="text-align: right;">1948-01-01</td><td style="text-align: right;">0.47</td><td style="text-align: right;">51</td><td style="text-align: right;">42</td><td>True</td></tr><tr class="even"><td style="text-align: left;">1</td><td style="text-align: right;">1948-01-02</td><td style="text-align: right;">0.59</td><td style="text-align: right;">45</td><td style="text-align: right;">36</td><td>True</td></tr><tr class="odd"><td style="text-align: left;">2</td><td style="text-align: right;">1948-01-03</td><td style="text-align: right;">0.42</td><td style="text-align: right;">45</td><td style="text-align: right;">35</td><td>True</td></tr><tr class="even"><td style="text-align: left;">3</td><td style="text-align: right;">1948-01-04</td><td style="text-align: right;">0.31</td><td style="text-align: right;">45</td><td style="text-align: right;">34</td><td>True</td></tr><tr class="odd"><td style="text-align: left;">4</td><td style="text-align: right;">1948-01-05</td><td style="text-align: right;">0.17</td><td style="text-align: right;">45</td><td style="text-align: right;">32</td><td>True</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.calibration <span class="hljs-keyword">import</span> calibration_curve, CalibratedClassifierCV<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNB<br><br><span class="hljs-comment"># load the data into a dataframe</span><br>rain = pd.read_csv(<span class="hljs-string">&#x27;seattleWeather_1948-2017.csv&#x27;</span>).dropna()<br>rain.head()<br><br><span class="hljs-comment"># use the temperatures to predict whether or not there was rain that day</span><br>features = rain[[<span class="hljs-string">&#x27;TMIN&#x27;</span>, <span class="hljs-string">&#x27;TMAX&#x27;</span>]]<br>target = rain.RAIN.astype(<span class="hljs-built_in">bool</span>)<br>X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># use a Naive Bayes Classifier and make hard predictions</span><br>nb_rain = GaussianNB().fit(X_train, y_train)<br>accuracy = nb_rain.score(X_train, y_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;The hard predictions were right <span class="hljs-subst">&#123;<span class="hljs-number">100</span>*accuracy:<span class="hljs-number">5.2</span>f&#125;</span>% of the time&#x27;</span>)<br><br><span class="hljs-comment"># use the predict_proba method to get the probability of the positive case</span><br>predictions = nb_rain.predict_proba(X_train)[:, <span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># call calibration_curve to &quot;bin&quot; similar predicted probabilities together, and calculate what percentage of the time</span><br><span class="hljs-comment"># it actually rains in that bin</span><br>binned_true_p, binned_predict_p = calibration_curve(y_train, predictions, n_bins=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>调用<ahref="https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html">calibration_curve</a>将相似的预测概率分箱在一起，并计算在该绑定中实际下雨的时间百分比。</p><figure><img src="https://image.yinan.fun/202308201042945.png" alt="校准曲线" /><figcaption aria-hidden="true">校准曲线</figcaption></figure><p>如果我们的分类器输出良好的概率，那么校准曲线上这些点将接近这条红线。校准曲线上的点告诉我们，当我们的分类器声称下雨的概率约为25%时，实际上有33%的时间下雨了。</p><p>在这个例子中，朴素贝叶斯分类器给出了看起来像概率的分数，并且是由<code>predict_proba</code>生成的，但当我们查看校准曲线时，我们发现它们实际上只是分数。</p><h2 id="重新校准和普拉特缩放">重新校准和普拉特缩放</h2><p>当模型的<code>predict_proba</code>方法给出分数而不是概率时，我们可以重新校准分数，使其更接近概率。这个过程被称为普拉特缩放（Plattscaling），在Scikit-learn中作为<code>CalibratedClassifierCV</code>实现。</p><p>Platt scaling是一个两参数(two-parameter)优化问题，而优化的目标为：<span class="math display">\[min-\sum_i{t_i{\rm log}(p_i)+(1-t_i){\rm log}(1-p_i)}\]</span> 其中 <span class="math display">\[t_i=\frac{y_i+1}{2}\]</span></p><p><span class="math display">\[p_i = \frac{1}{1+e^{Af(x_i)+B}}\]</span></p><p><spanclass="math inline">\(f\)</span>表示一个二分类模型，y的值为-1或+1，A和B就是普拉特缩放要学习的参数。</p><p>在天气示例中，我们可以将普拉特缩放应用于我们的朴素贝叶斯分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">calibrated_nb_rain = CalibratedClassifierCV(nb_rain, cv=<span class="hljs-number">5</span>, method=<span class="hljs-string">&#x27;isotonic&#x27;</span>)<br><span class="hljs-comment"># Note we need to refit!</span><br>calibrated_nb_rain.fit(X_train, y_train);<br><br><span class="hljs-comment"># access the probabilities after recalibration</span><br>calibrated_probs = calibrated_nb_rain.predict_proba(X_test)[:, <span class="hljs-number">1</span>]<br>binned_true_p, binned_predict_p = calibration_curve(y_test, calibrated_probs, n_bins=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p><img src="https://image.yinan.fun/202308201323942.png"alt="image-20230820115155793" />修正后的概率更接近红线。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分类概率转分数</title>
    <link href="/%E5%88%86%E7%B1%BB%E6%A6%82%E7%8E%87%E8%BD%AC%E5%88%86%E6%95%B0.html"/>
    <url>/%E5%88%86%E7%B1%BB%E6%A6%82%E7%8E%87%E8%BD%AC%E5%88%86%E6%95%B0.html</url>
    
    <content type="html"><![CDATA[<h1 id="概率转分数">概率转分数</h1><p>分类可以分为硬分类和软分类。</p><p><strong>硬分类（hardclassification）</strong>是当我们进行观察并预测它属于哪个类别时。例如，银行可以使用某人的财务历史（观察）来预测他们是否会偿还贷款。医生可以测量肿瘤（观察结果），以预测肿瘤是否是恶性肿瘤，是否需要切除。</p><p>相比之下，<strong>软分类（softclassification）</strong>表示模型对其预测的信心。在发放贷款时，模型可能会做出硬分类，即两个客户都会偿还贷款，但软分类是客户1有80%的机会偿还贷款，而客户2有95%的机会偿还贷款。</p><p>如果我们有一个软分类器，我们可以通过选择一个阈值来制作一个硬分类器：只需将所有软预测高于阈值的观测归类为正类，将软预测低于阈值的观测归类为负类。对于计算概率的scikit-learn分类器，这就是它们在默认阈值为50%的情况下的工作方式。使用概率作为门槛也有助于使模型更容易解释：对于贷款分类器，您只想接受您真正有信心的人（例如，那些偿还贷款的几率高出85%的人）。对于肿瘤病例，您可能会决定让医生检查任何有5%以上恶性几率的肿瘤。</p><p>虽然概率是进行软分类的方便方法，但我们真正需要的只是分数。对于二分类器来说，这是最容易解释的。将其中一个类别称为正类，我们从分数中唯一需要的就是<strong>更高的分数意味着模型更有信心输入（observation）属于正类。</strong></p><p><ahref="https://yinan.fun/分类问题中概率与分数">分类问题中概率与分数</a></p><h2 id="二分类">二分类</h2><p>分数评价好坏，样本是好样本（标签为1）的概率越高，分数越高。</p><ol type="1"><li><p>将逾期率概率值直接乘以100就得到分数了，但是LR得到的概率值往往是不均匀的，比如讲概率值排序后，头部和尾部的概率值较少，大部分概率都集中在中间某个区域，这时这种方法就失效了。<span class="math display">\[Score=p\times 100\]</span></p></li><li><p>还有一种方法是将概率值进行排序，找出头部的20%概率值映射到80-100分，尾部的20%概率值映射到0-60分，中间的60%概率值映射到60-80分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> pandas <span class="hljs-keyword">import</span> DataFrame<br> <br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">score_model</span>(<span class="hljs-params">result_train, bins_num=<span class="hljs-number">10</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        根据输入的样本数据，进行打分模型的训练：</span><br><span class="hljs-string">            输入数据格式：dict 组成list， dict = &#123;score : ml_model_proba&#125;</span><br><span class="hljs-string">            输出数据格式：bin_dict = &#123;i:&#123;&#x27;left&#x27;: , &#x27;right&#x27;: , &#x27;line_train_list&#x27;&#125;&#125;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sorted_result_train = <span class="hljs-built_in">sorted</span>(result_train, key=<span class="hljs-keyword">lambda</span> result_train:result_train[<span class="hljs-string">&#x27;score&#x27;</span>], reverse=<span class="hljs-literal">False</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(sorted_result_train))<br>    score_bins = &#123;&#125;<br>    every_len = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(sorted_result_train) / bins_num)<br>    <span class="hljs-built_in">print</span>(every_len)<br>    <span class="hljs-comment"># 30分粒度，做分桶，桶内直线拟合</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, bins_num):<br>        left = sorted_result_train[i*every_len][<span class="hljs-string">&#x27;score&#x27;</span>]<br>        right = sorted_result_train[(i+<span class="hljs-number">1</span>)*every_len-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;score&#x27;</span>]<br>        step = <span class="hljs-number">300</span>/bins_num<br>        score_map = [<span class="hljs-number">800</span>-i*step, <span class="hljs-number">800</span>-(i+<span class="hljs-number">1</span>)*step]<br>        <span class="hljs-keyword">if</span> i == bins_num-<span class="hljs-number">1</span>:<br>            right = sorted_result_train[-<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;score&#x27;</span>]<br>        line_train = np.polyfit([left, right], score_map, <span class="hljs-number">1</span>)<br>        line_train_list = line_train.tolist()<br>        bins = <span class="hljs-built_in">dict</span>()<br>        bins[<span class="hljs-string">&#x27;left&#x27;</span>] = left<br>        bins[<span class="hljs-string">&#x27;right&#x27;</span>] = right<br>        bins[<span class="hljs-string">&#x27;line_train_list&#x27;</span>] = line_train_list<br>        score_bins[i] = bins<br>    <span class="hljs-keyword">return</span> score_bins<br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_score</span>(<span class="hljs-params">score_bins, result_proab</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">       打分模型：根据输入的result_proab, 按照score_bins进行打分；score_bins就是打分model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    proab = result_proab<br>    score = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> score_bins.items():<br>        <span class="hljs-keyword">if</span> proab &gt;= v[<span class="hljs-string">&#x27;left&#x27;</span>] <span class="hljs-keyword">and</span> proab &lt; v[<span class="hljs-string">&#x27;right&#x27;</span>]:<br>            score = v[<span class="hljs-string">&#x27;line_train_list&#x27;</span>][<span class="hljs-number">0</span>] * proab + v[<span class="hljs-string">&#x27;line_train_list&#x27;</span>][<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> score<br> <br> <br>scores = np.random.random(<span class="hljs-number">20</span>)<br>ids = <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>)<br>df = DataFrame([ids,scores]).T<br>df.columns = [<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>]<br>dic = df.to_dict(orient=<span class="hljs-string">&quot;records&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;dic:&quot;</span>,dic)<br><span class="hljs-comment">#训练打分模型（将逾期的概率映射为500-800之间的分数，分数越高，用户越好）</span><br>score_bins = score_model(dic)<br><span class="hljs-built_in">print</span>(score_bins)<br><span class="hljs-comment">#输入pred概率值，输出分数</span><br>result_proab = <span class="hljs-number">0.083882018</span><br>score = predict_score(score_bins, result_proab)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;score:&quot;</span>, score)<br></code></pre></td></tr></table></figure></li><li><p>评分表</p><p>评分卡预测分数时不需要计算概率，而是直接用线性部分转换成分数，每一个特征x的分数和就是总分。<span class="math display">\[\begin{align}Score&amp;=A+B\times\ln(odds) \\&amp;=A+B\times(\omega_0+\omega_1x_1+\cdots+\omega_nx_x)\\&amp;=(A+B\omega_0)+B\omega_1x_1+\cdots+B\omega_nx_x\end{align}\]</span> <ahref="https://yinan.fun/lr%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%E5%80%BC%E8%BD%AC%E5%88%86%E6%95%B0">评分表原理</a></p><p><ahref="https://yinan.fun/%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B">评分表建模</a></p></li></ol><h2 id="多分类">多分类</h2><p>多分类无法直接根据样本标签确定样本的好坏，需要定义每个标签的好坏程度。</p><ol type="1"><li><p>确定每个类别i分数为<spanclass="math inline">\(s_i\)</span>，分数为 <span class="math display">\[s=\sum_{i=1}^n p_i\times s_i\]</span></p><blockquote><p>二分类，<span class="math inline">\(s_0=0, s_1=100\)</span>。</p></blockquote></li><li><p>计算样本为好样本的概率，再转化为分数。</p></li></ol><h2 id="概率评分方法">概率评分方法</h2><p>分数评价预测概率的准确率</p><p><ahref="https://yinan.fun/%E6%A6%82%E7%8E%87%E8%AF%84%E5%88%86%E6%96%B9%E6%B3%95">概率评分方法</a></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>评分卡模型</title>
    <link href="/%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B.html"/>
    <url>/%E8%AF%84%E5%88%86%E5%8D%A1%E6%A8%A1%E5%9E%8B.html</url>
    
    <content type="html"><![CDATA[<h1 id="评分卡模型">评分卡模型</h1><h2 id="数据导入">数据导入</h2><p>使用kaggle上的<ahref="https://www.kaggle.com/c/GiveMeSomeCredit/data">Give Me SomeCredit</a>数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df = pd.read_csv(<span class="hljs-string">&#x27;cs-training.csv&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="数据预处理">数据预处理</h2><h3 id="缺失值处理">缺失值处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(df.info())<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">&lt;class <span class="hljs-string">&#x27;pandas.core.frame.DataFrame&#x27;</span>&gt;<br>RangeIndex: 150000 entries, 0 to 149999<br>Data columns (total 12 columns):<br> <span class="hljs-comment">#   Column      Non-Null Count   Dtype  </span><br>---  ------      --------------   -----  <br> 0   用户ID        150000 non-null  int64  <br> 1   好坏客户        150000 non-null  int64  <br> 2   可用额度比值      150000 non-null  float64<br> 3   年龄          150000 non-null  int64  <br> 4   逾期30-59天笔数  150000 non-null  int64  <br> 5   负债率         150000 non-null  float64<br> 6   月收入         120269 non-null  float64<br> 7   信贷数量        150000 non-null  int64  <br> 8   逾期90天笔数     150000 non-null  int64  <br> 9   固定资产贷款数量    150000 non-null  int64  <br> 10  逾期60-89天笔数  150000 non-null  int64  <br> 11  家属数量        146076 non-null  float64<br>dtypes: float64(4), int64(8)<br>memory usage: 13.7 MB<br></code></pre></td></tr></table></figure><p>月收入缺失比:19.82%，家属数量缺失比:2.62%。</p><p>家属数量对应的缺失比例低于5%，可直接删除；月收入缺失比比较高，不能直接删除，利用填充平均值的方法进行补充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 缺失值填充</span><br>df = df.fillna(&#123;<span class="hljs-string">&#x27;月收入&#x27;</span>:df[<span class="hljs-string">&#x27;月收入&#x27;</span>].mean()&#125;)<br><span class="hljs-comment"># 缺失值删除</span><br>df.dropna(inplace = <span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h3 id="异常值处理">异常值处理</h3><p>用箱形图判断异常值，再过滤异常值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 异常值过滤</span><br>df = df[df[<span class="hljs-string">&quot;固定资产贷款数量&quot;</span>]&lt;<span class="hljs-number">50</span>]<br>df = df[df[<span class="hljs-string">&quot;逾期30-59天笔数&quot;</span>]&lt;<span class="hljs-number">80</span>]<br>df = df[df[<span class="hljs-string">&quot;年龄&quot;</span>]&gt;<span class="hljs-number">0</span>]<br>df = df[df[<span class="hljs-string">&#x27;可用额度比值&#x27;</span>]&lt;=<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h2 id="特征选择">特征选择</h2><p>过滤掉一些对于目标变量影响权重较小的特征变量，这里使用IV值进行特征筛选。</p><h3 id="woe分箱">WOE分箱</h3><p>WOE（Weight ofEvidence）即证据权重，可以将logistic回归模型转化为标准评分卡格式，WOE是对原始自变量的一种编码形式，要对一个变量进行WOE编码，需要首先把这个变量进行分组处理（也叫离散化、分箱）。WOE=ln(坏样本占比/好样本占比)<span class="math display">\[WOE_i=\ln(\frac{Bad_{X=X_i}/Bad_{tatol}}{Good_{X=X_i}/Good_{tatol}})\]</span> WOE分箱就是将连续变量离散化，即切分成不同的区间段，离散化后的变量具有很好的稳定性，比如年龄这个连续变量，如果是连续值的时候21和29就是两个不同的值，对模型的效果可能就不一样，如果离散化成20-30的时候，这两个年龄对模型的效果就是一样，更加稳定。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 分箱</span><br>cut1, bins1 = pd.qcut(df[<span class="hljs-string">&quot;可用额度比值&quot;</span>],<span class="hljs-number">4</span>,labels=<span class="hljs-literal">False</span>, retbins=<span class="hljs-literal">True</span>)<br>cut2, bins2 = pd.qcut(df[<span class="hljs-string">&quot;年龄&quot;</span>],<span class="hljs-number">8</span>,labels=<span class="hljs-literal">False</span>, retbins=<span class="hljs-literal">True</span>)<br>bins3=[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">13</span>]<br>cut3=pd.cut(df[<span class="hljs-string">&quot;逾期30-59天笔数&quot;</span>],bins3,labels=<span class="hljs-literal">False</span>)<br>cut4=pd.qcut(df[<span class="hljs-string">&quot;负债率&quot;</span>],<span class="hljs-number">3</span>,labels=<span class="hljs-literal">False</span>)<br>cut5=pd.qcut(df[<span class="hljs-string">&quot;月收入&quot;</span>],<span class="hljs-number">4</span>,labels=<span class="hljs-literal">False</span>)<br>cut6=pd.qcut(df[<span class="hljs-string">&quot;信贷数量&quot;</span>],<span class="hljs-number">4</span>,labels=<span class="hljs-literal">False</span>)<br>bins7=[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>,<span class="hljs-number">5</span>, <span class="hljs-number">20</span>]<br>cut7=pd.cut(df[<span class="hljs-string">&quot;逾期90天笔数&quot;</span>],bins7,labels=<span class="hljs-literal">False</span>)<br>bins8=[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">33</span>]<br>cut8=pd.cut(df[<span class="hljs-string">&quot;固定资产贷款数量&quot;</span>],bins8,labels=<span class="hljs-literal">False</span>)<br>bins9=[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>]<br>cut9=pd.cut(df[<span class="hljs-string">&quot;逾期60-89天笔数&quot;</span>],bins9,labels=<span class="hljs-literal">False</span>)<br>bins10=[-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">21</span>]<br>cut10=pd.cut(df[<span class="hljs-string">&quot;家属数量&quot;</span>],bins10,labels=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算WOE</span><br><span class="hljs-comment"># 整体好坏比（定义标签为1为好客户）</span><br>total_rate = df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].<span class="hljs-built_in">sum</span>()/(df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].count()-df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].<span class="hljs-built_in">sum</span>())<br><br><span class="hljs-comment"># 定义WOE函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_woe</span>(<span class="hljs-params">cut</span>):<br>    grouped = df.groupby(cut)[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].value_counts()<br>    <span class="hljs-comment"># 组内好坏比</span><br>    woe = np.log(pd.DataFrame(grouped).unstack().iloc[:,<span class="hljs-number">1</span>]/pd.DataFrame(grouped).unstack().iloc[:,<span class="hljs-number">0</span>]/total_rate)<br>    <span class="hljs-keyword">return</span> woe<br><span class="hljs-comment"># 计算WOE值，如果计算出来的WOE不是单调的就需要重新分箱</span><br>cut1_woe = get_woe(cut1)<br>cut2_woe = get_woe(cut2)<br>cut3_woe = get_woe(cut3)<br>cut4_woe = get_woe(cut4)<br>cut5_woe = get_woe(cut5)<br>cut6_woe = get_woe(cut6)<br>cut7_woe = get_woe(cut7)<br>cut8_woe = get_woe(cut8)<br>cut9_woe = get_woe(cut9)<br>cut10_woe = get_woe(cut10)<br></code></pre></td></tr></table></figure><h3 id="计算iv值">计算IV值</h3><p>IV（Informationvalue，IV）信息值，IV值衡量一个变量的信息量，计算公式为SUM（（好样本占比-坏样本占比）*迹象权数） <span class="math display">\[IV=\sum_{i=1}^{N}(Good_{X=X_i}/Good_{tatol}-Bad_{X=X_i}/Bad_{tatol})\timesWOE_i\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算每个变量的IV值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_iv_data</span>(<span class="hljs-params">cut,cut_woe</span>):<br>    grouped = df.groupby(cut)[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].value_counts()<br>    cut_iv = ((grouped.unstack().iloc[:,<span class="hljs-number">1</span>]/df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].<span class="hljs-built_in">sum</span>()- grouped.unstack().iloc[:,<span class="hljs-number">0</span>]/(df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].count()-df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>].<span class="hljs-built_in">sum</span>()))*cut_woe).<span class="hljs-built_in">sum</span>()<br>    <span class="hljs-keyword">return</span> cut_iv<br><br><br>cut1_iv = get_iv_data(cut1,cut1_woe)<br>cut2_iv = get_iv_data(cut2,cut2_woe)<br>cut3_iv = get_iv_data(cut3,cut3_woe)<br>cut4_iv = get_iv_data(cut4,cut4_woe)<br>cut5_iv = get_iv_data(cut5,cut5_woe)<br>cut6_iv = get_iv_data(cut6,cut6_woe)<br>cut7_iv = get_iv_data(cut7,cut7_woe)<br>cut8_iv = get_iv_data(cut8,cut8_woe)<br>cut9_iv = get_iv_data(cut9,cut9_woe)<br>cut10_iv = get_iv_data(cut10,cut10_woe)<br></code></pre></td></tr></table></figure><figure><img src="https://image.yinan.fun/202308131313073.png"alt="image-20230812214550651" /><figcaption aria-hidden="true">image-20230812214550651</figcaption></figure><p>可以看到["负债率","月收入","信贷数量","固定资产贷款量","家属数量"]这几个特征的IV值过低，对目标变量的影响较小，将其过滤掉。</p><h3 id="用woe值代替原始数据">用WOE值代替原始数据</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># 原始分组后的数据集，根据各组标签替代为对应的WOE值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">replace_data</span>(<span class="hljs-params">cut,cut_woe</span>):<br>   <span class="hljs-comment"># 保存各箱标签</span><br>    a = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> cut.unique():<br>        a.append(i)<br>        a.sort()  <span class="hljs-comment"># 每箱从小到大排序，并计算对应的WOE值</span><br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(a)):<br>        cut.replace(a[m],cut_woe.values[m],inplace = <span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> cut<br><span class="hljs-comment"># 进行替换</span><br>df_new = pd.DataFrame()<br>df_new[<span class="hljs-string">&#x27;可用额度比值&#x27;</span>] = replace_data(cut1,cut1_woe)<br>df_new[<span class="hljs-string">&#x27;年龄&#x27;</span>]=replace_data(cut2,cut2_woe)<br>df_new[<span class="hljs-string">&#x27;逾期30-59天笔数&#x27;</span>] = replace_data(cut3,cut3_woe)<br>df_new[<span class="hljs-string">&#x27;逾期90天笔数&#x27;</span>] = replace_data(cut7,cut7_woe)<br>df_new[<span class="hljs-string">&#x27;逾期60-89天笔数&#x27;</span>] = replace_data(cut9,cut9_woe)<br>df_new[<span class="hljs-string">&#x27;好坏客户&#x27;</span>] = df[<span class="hljs-string">&#x27;好坏客户&#x27;</span>]<br></code></pre></td></tr></table></figure><h2 id="模型训练">模型训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型训练及测试</span><br>x = df_new.iloc[:,<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]<br>y = df_new.iloc[:,-<span class="hljs-number">1</span>]<br>x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = <span class="hljs-number">0.4</span>,random_state = <span class="hljs-number">0</span>)<br>model = LogisticRegression()<br>clf = model.fit(x_train,y_train)<br><span class="hljs-comment"># 预测标签</span><br>y_pred  = clf.predict(x_test)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试集的平均正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(clf.score(x_test,y_test)))<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">测试集的平均正确率：0.9423646788588662<br></code></pre></td></tr></table></figure><h2 id="计算得分">计算得分</h2><p><span class="math display">\[\begin{align}Score&amp;=A+B\times\ln(odds) \\&amp;=A+B\times(\omega_0+\omega_1x_1+\cdots+\omega_nx_x)\\&amp;=(A+B\omega_0)+B\omega_1x_1+\cdots+B\omega_nx_x\end{align}\]</span></p><p>约定：当odds增加一倍，分数增加20分；当odds = 1,分数为600分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 系数值</span><br>coe = clf.coef_<br>bias = clf.intercept_<br><span class="hljs-comment"># score = offset+factor*log(odds)</span><br>factor = <span class="hljs-number">20</span> / np.log(<span class="hljs-number">2</span>)<br>offset = <span class="hljs-number">600</span><br><br><span class="hljs-comment"># 定义变量分数计算函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_score</span>(<span class="hljs-params">coe,woe,factor</span>):<br>    scores = []<br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> woe:<br>        score=<span class="hljs-built_in">round</span>(coe*w*factor,<span class="hljs-number">0</span>)<br>        scores.append(score)<br>    <span class="hljs-keyword">return</span> scores<br><br><span class="hljs-comment"># 计算每个变量得分</span><br>x1 = get_score(coe[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>],cut1_woe,factor)<br>x2 = get_score(coe[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>],cut2_woe,factor)<br>x3 = get_score(coe[<span class="hljs-number">0</span>][<span class="hljs-number">2</span>],cut3_woe,factor)<br>x7 = get_score(coe[<span class="hljs-number">0</span>][<span class="hljs-number">3</span>],cut7_woe,factor)<br>x9 = get_score(coe[<span class="hljs-number">0</span>][<span class="hljs-number">4</span>],cut9_woe,factor)<br>base_score = offset + factor * bias;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">基础分数:[521.32931339]<br>可用额度比值对应分箱:[0., 0.02953715 0.1480833, 0.52137127, 1.]<br>可用额度比值对应的分数:[-22.0, -21.0, -5.0, 19.0]<br>年龄对应分箱:[ 21.  35.  41.  47.  52.  57.  63.  70. 107.]<br>年龄对应的分数:[8.0, 5.0, 4.0, 3.0, -0.0, -6.0, -13.0, -16.0]<br>逾期30-59天笔数对应的分数:[-8.0, 14.0, 27.0, 37.0, 42.0]<br>逾期90天笔数对应的分数:[-6.0, 34.0, 48.0, 57.0, 57.0]<br>逾期60-89天笔数对应的分数:[-3.0, 24.0, 35.0, 39.0]<br></code></pre></td></tr></table></figure><h2 id="获得评分卡">获得评分卡</h2><table><thead><tr class="header"><th>变量</th><th>分箱类别</th><th>分数</th></tr></thead><tbody><tr class="odd"><td>基础分数</td><td>-</td><td>521.32931339</td></tr><tr class="even"><td>可用额度</td><td>0 - 0.02953715</td><td>-22.0</td></tr><tr class="odd"><td></td><td>0.02953715 - 0.1480833</td><td>-21.0</td></tr><tr class="even"><td></td><td>0.1480833 - 0.52137127</td><td>-5.0</td></tr><tr class="odd"><td></td><td>0.52137127 - 1</td><td>19.0</td></tr><tr class="even"><td>年龄</td><td>21 - 35</td><td>8.0</td></tr><tr class="odd"><td></td><td>35 - 41</td><td>5.0</td></tr><tr class="even"><td></td><td>41 - 47</td><td>4.0</td></tr><tr class="odd"><td></td><td>47 - 52</td><td>3.0</td></tr><tr class="even"><td></td><td>52 - 57</td><td>-0.0</td></tr><tr class="odd"><td></td><td>57 - 63</td><td>-6.0</td></tr><tr class="even"><td></td><td>63 - 70</td><td>-13.0</td></tr><tr class="odd"><td></td><td>70 - 107</td><td>-16.0</td></tr><tr class="even"><td>逾期30-59天笔数</td><td>-1 - 0</td><td>-8.0</td></tr><tr class="odd"><td></td><td>0 - 1</td><td>14.0</td></tr><tr class="even"><td></td><td>1 - 3</td><td>27.0</td></tr><tr class="odd"><td></td><td>3 - 5</td><td>37.0</td></tr><tr class="even"><td></td><td>5 - 13</td><td>42.0</td></tr><tr class="odd"><td>逾期60-89天笔数</td><td>-1 - 0</td><td>-3.0</td></tr><tr class="even"><td></td><td>0 - 1</td><td>24.0</td></tr><tr class="odd"><td></td><td>1 - 3</td><td>35.0</td></tr><tr class="even"><td></td><td>3 - 12</td><td>39.0</td></tr><tr class="odd"><td>逾期90天笔数</td><td>-1 - 0</td><td>-6.0</td></tr><tr class="even"><td></td><td>0 - 1</td><td>34.0</td></tr><tr class="odd"><td></td><td>1 - 3</td><td>48.0</td></tr><tr class="even"><td></td><td>3 - 5</td><td>57.0</td></tr><tr class="odd"><td></td><td>5 - 20</td><td>57.0</td></tr></tbody></table><p>上面得到的是不同特征值对应的分数，评分越高表明该用户越有可能相应目标变量，成为坏用户；特征划分区间是依次递增的，特征区间值与得分是相对应的，年龄越大，坏账的可能性越低；逾期笔数越多，坏账可能性越高，得分越高；最后将所有的变量对应的得分相加，就是每个用户的得分。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>电饭煲叉烧</title>
    <link href="/%E7%94%B5%E9%A5%AD%E7%85%B2%E5%8F%89%E7%83%A7.html"/>
    <url>/%E7%94%B5%E9%A5%AD%E7%85%B2%E5%8F%89%E7%83%A7.html</url>
    
    <content type="html"><![CDATA[<h1 id="电饭煲叉烧">电饭煲叉烧</h1><p><ahref="https://www.bilibili.com/video/BV1yV4y187H9">https://www.bilibili.com/video/BV1yV4y187H9</a></p><h3 id="准备梅花肉">准备梅花肉</h3><blockquote><p>可以用五花肉替代</p></blockquote><figure><img src="https://image.yinan.fun/202308061129526.png"alt="一斤二两梅花肉" /><figcaption aria-hidden="true">一斤二两梅花肉</figcaption></figure><h3 id="有点厚用牙签扎扎孔">有点厚，用牙签扎扎孔</h3><blockquote><p>腌的时候切几大块，然后用叉子正反插猪肉，这样更能入味</p></blockquote><figure><img src="https://image.yinan.fun/202308061107984.png"alt="image-20230806110717962" /><figcaption aria-hidden="true">image-20230806110717962</figcaption></figure><h3 id="腌制肉">腌制肉</h3><blockquote><p>蜂蜜可以用冰糖替代</p></blockquote><figure><img src="https://image.yinan.fun/202308061110146.png"alt="image-20230806111004086" /><figcaption aria-hidden="true">image-20230806111004086</figcaption></figure><figure><img src="https://image.yinan.fun/202308061129621.png"alt="image-20230806110857406" /><figcaption aria-hidden="true">image-20230806110857406</figcaption></figure><h3id="套上袋子放冰箱冷藏腌制三个小时以上">套上袋子，放冰箱冷藏腌制三个小时以上</h3><blockquote><p>最好过夜腌制</p></blockquote><figure><img src="https://image.yinan.fun/202308061129764.png"alt="image-20230806111136504" /><figcaption aria-hidden="true">image-20230806111136504</figcaption></figure><h3id="第二天全部倒入电饭煲按煮饭键">第二天全部倒入电饭煲，按煮饭键</h3><blockquote><p>电饭锅就不用加水了，不粘锅也不加水，铁锅要加一些水的。</p><p>普通电饭锅直接煮饭键模式就行了！如果切的厚，煮饭键结束，你看着没熟，可以再煮一会。一般电饭锅煮饭键都是40分钟吧</p></blockquote><figure><img src="https://image.yinan.fun/202308061129019.png"alt="image-20230806111235934" /><figcaption aria-hidden="true">image-20230806111235934</figcaption></figure><h3 id="时间到放入锅里收汁">时间到放入锅里收汁</h3><figure><img src="https://image.yinan.fun/202308061129554.png"alt="image-20230806111421930" /><figcaption aria-hidden="true">image-20230806111421930</figcaption></figure><h3 id="不断用勺子往叉烧上淋汁">不断用勺子往叉烧上淋汁</h3><figure><img src="https://image.yinan.fun/202308061129739.png"alt="image-20230806111536851" /><figcaption aria-hidden="true">image-20230806111536851</figcaption></figure><h3 id="收到浓稠挂汁的状态">收到浓稠挂汁的状态</h3><figure><img src="https://image.yinan.fun/202308061129948.png"alt="image-20230806111701219" /><figcaption aria-hidden="true">image-20230806111701219</figcaption></figure><h3 id="斜刀切">斜刀切</h3><blockquote><p>尽量薄一点</p></blockquote><figure><img src="https://image.yinan.fun/202308061129154.png"alt="image-20230806111848910" /><figcaption aria-hidden="true">image-20230806111848910</figcaption></figure><h3 id="装盘">装盘</h3><figure><img src="https://image.yinan.fun/202308061129908.png"alt="image-20230806112715143" /><figcaption aria-hidden="true">image-20230806112715143</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>吃的</category>
      
    </categories>
    
    
    <tags>
      
      <tag>电饭煲</tag>
      
      <tag>肉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率评分方法</title>
    <link href="/%E6%A6%82%E7%8E%87%E8%AF%84%E5%88%86%E6%96%B9%E6%B3%95.html"/>
    <url>/%E6%A6%82%E7%8E%87%E8%AF%84%E5%88%86%E6%96%B9%E6%B3%95.html</url>
    
    <content type="html"><![CDATA[<h1 id="概率评分方法">概率评分方法</h1><p>一般来说，<strong>评估预测概率准确性</strong>的方法被称为评分规则或评分函数。下面将介绍三种评分方法，可用于评估<strong>分类预测建模</strong>问题的预测概率。</p><h2 id="log-loss-score">Log Loss Score</h2><p>Log loss, also called “logistic loss,” “logarithmic loss,” or “crossentropy” can be used as a measure for evaluating predictedprobabilities.</p><p>将每个预测概率与实际类输出值（0或1）进行比较，并计算出一个分数，该分数根据与预期值的距离来惩罚概率。惩罚是对数的，小差异的分数小和大差异的分数大。</p><p>对于带有真实标签的单个样品 <span class="math inline">\(y \in\{0,1\}\)</span> 和概率估计 <span class="math inline">\(p =\operatorname{Pr}(y = 1)\)</span>, log loss 是： <spanclass="math display">\[L_{\log}(y, p) = -(y \log (p) + (1 - y) \log (1 - p))\]</span></p><figure><img src="https://image.yinan.fun/202308051604385.png"alt="Log loss图像" /><figcaption aria-hidden="true">Log loss图像</figcaption></figure><h3 id="缺点">缺点</h3><p>Logloss的结果对数据集取平均，在平衡数据集的情况下，分数将是合适的，在不平衡数据集的情况下，分数将具有误导性。</p><p>平衡数据集Log Loss预测的折线图</p><figure><img src="https://image.yinan.fun/202308051604968.png"alt="平衡数据集Log Loss预测的折线图" /><figcaption aria-hidden="true">平衡数据集LogLoss预测的折线图</figcaption></figure><p>不平衡数据集（0多）Log Loss预测的折线图</p><figure><img src="https://image.yinan.fun/202308051604640.png"alt="不平衡数据集Log Loss预测的折线图" /><figcaption aria-hidden="true">不平衡数据集LogLoss预测的折线图</figcaption></figure><h3 id="实现">实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sklearn.metrics.log_loss(y_true, y_pred, *, eps=<span class="hljs-string">&#x27;auto&#x27;</span>, normalize=<span class="hljs-literal">True</span>, sample_weight=<span class="hljs-literal">None</span>, labels=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p><ahref="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html</a></p><h2 id="brier-score">Brier Score</h2><p>BrierScore衡量的是预测概率和实际结果之间的<strong>均方差</strong>。分数汇总了概率预测中误差的大小。错误分数始终在0.0和1.0之间，其中完美的模型的分数为0。</p><figure><img src="https://image.yinan.fun/202308051604434.png"alt="Brier Score图像" /><figcaption aria-hidden="true">Brier Score图像</figcaption></figure><h3 id="实现-1">实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sklearn.metrics.brier_score_loss(y_true, y_prob, *, sample_weight=<span class="hljs-literal">None</span>, pos_label=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p><ahref="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html</a></p><p>与Log loss一样，在不平衡数据集的情况下，分数将具有误导性。</p><h2 id="roc-auc-score">ROC AUC Score</h2><p>ROC（Receiver OperatingCharacteristic）曲线是一种用于表示分类模型性能的图形工具。它通过将<strong>真阳性率（TruePositive Rate，TPR）</strong>和<strong>假阳性率（False PositiveRate，FPR）</strong>作为横纵坐标来描绘分类器在<strong>不同阈值</strong>下的性能。</p><blockquote><p>真阳性率（True PositiveRate，TPR）是指分类器正确识别正例的能力，可以理解为所有阳性群体中被检测出来的比率，TPR越接近1越好。</p><p>假阳性率（False PositiveRate，FPR）是指在所有实际为负例的样本中，模型错误地预测为正例的样本比例，FPR越接近0越好。</p></blockquote><p><span class="math display">\[TPR=\frac{TP}{P}=\frac{TP}{TP+FN}\]</span></p><p><span class="math display">\[FPR=\frac{FP}{N}=\frac{FP}{FP+TN}\]</span></p><figure><img src="https://image.yinan.fun/202308051604179.png"alt="混淆矩阵图" /><figcaption aria-hidden="true">混淆矩阵图</figcaption></figure><figure><img src="https://image.yinan.fun/202308051624977.png" alt="ROC曲线" /><figcaption aria-hidden="true">ROC曲线</figcaption></figure><p><code>sklearn</code> <code>LogisticRegression</code> 的ROC曲线：</p><figure><img src="https://image.yinan.fun/202308051604070.png"alt="LogisticRegression的ROC曲线" /><figcaption aria-hidden="true">LogisticRegression的ROC曲线</figcaption></figure><p>AUC（Area Under rocCurve）是<strong>ROC曲线下的面积</strong>，用于衡量分类器性能。AUC值越接近1，表示分类器性能越好；反之，AUC值越接近0，表示分类器性能越差。在实际应用中，我们常常通过计算AUC值来评估分类器的性能。</p><p>ROC曲线围成的面积(即AUC)可以解读为：从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。</p><blockquote><p><strong>AUC的一般判断标准：</strong></p><p>0.5 - 0.7：效果较低，但用于预测股票已经很不错了</p><p>0.7 - 0.85：效果一般</p><p>0.85 - 0.95：效果很好</p><p>0.95 - 1：效果非常好，但一般不太可能</p></blockquote><h3 id="优点">优点</h3><p>TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，所以即使P或N的整体数量发生了改变，也不会影响到另一列。也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。</p><h3 id="缺点-1">缺点</h3><p>在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，ROC曲线的横轴采用FPR，根据<spanclass="math inline">\(FPR=\frac{FP}{N}=\frac{FP}{FP+TN}\)</span>，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。</p><h3 id="实现-2">实现</h3><ol type="1"><li><p><code>roc_curve</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_true, y_score)<br>pyplot.plot(fpr, tpr)<br></code></pre></td></tr></table></figure><p><ahref="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html</a></p></li><li><p><code>roc_auc_score</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">auc_score = sklearn.metrics.roc_auc_score(y_true, y_score)<br></code></pre></td></tr></table></figure><p><ahref="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html</a></p></li></ol><h2 id="参考资料">参考资料</h2><ol type="1"><li>https://machinelearningmastery.com/how-to-score-probability-predictions-in-python/</li><li>https://zhuanlan.zhihu.com/p/616190701</li><li>https://zhuanlan.zhihu.com/p/352271971</li><li>https://zhuanlan.zhihu.com/p/34655990</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LR的预测概率值转分数</title>
    <link href="/LR%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%E5%80%BC%E8%BD%AC%E5%88%86%E6%95%B0.html"/>
    <url>/LR%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%E5%80%BC%E8%BD%AC%E5%88%86%E6%95%B0.html</url>
    
    <content type="html"><![CDATA[<h1 id="lr的预测概率值转分数评分卡">LR的预测概率值转分数（评分卡）</h1><p>背景：在互联网金融中，一般用LR模型来预测一个借贷用户的好坏，但是LR模型得到的结果是逾期的概率值，如何仅仅给出预测概率值，人一般很难判断这个概率值代表的用户质量好坏，但是如果能够给出分数就比较理解了。而我们一般要把逾期概率转为分数来供公司进行决策，那么如何将逾期概率转化为用户质量的得分，例如0-100呢？</p><h2 id="评分卡的分数转化">评分卡的分数转化</h2><p>逻辑回归（LogisticRegression）是一种因变量与事件发生的概率之间的一种映射关系。其中的<strong>p/(1-p)</strong>，称为<strong>odds</strong>，即<strong>事件成功的概率</strong>除以<strong>事件失败的概率</strong>。<span class="math display">\[odds=\frac{p}{1-p}\]</span> 对几率取对数得到<strong>对数几率</strong>(logodds，亦称logit)： <span class="math display">\[log\frac{p}{1-p}=\omega^Tx+b\]</span></p><blockquote><p><span class="math display">\[y=\frac{1}{1+e^{-(w^{\rm{T}}x+b)}}\]</span></p><p><span class="math display">\[\ln \frac{y}{1-y}=w^{\rm{T}}x+b\]</span></p></blockquote><p><span class="math display">\[Score=A+B\times\ln(odds)\]</span></p><p>设定当每增加1倍时，增加的分数PDO（point of doubleodds），即比率翻番的倍数 <span class="math display">\[Score+PDO=A+B\times\ln(2odds)\]</span></p><h2 id="求解a和b">求解A和B</h2><p>要算出系数A、B的话，需要从业务角度先预设两个前提条件：</p><ol type="1"><li>在某个特定的比率<spanclass="math inline">\(\theta_0\)</span>设定特定的预期分值<spanclass="math inline">\(S_0\)</span></li><li>指定比率翻番时分数的变动值（PDO）</li></ol><blockquote><p>解释：</p><ol type="1"><li>比如根据业务经验，消费金融信贷的客户违约率4.8%算正常（<spanclass="math inline">\(\theta_0=Odds=5\%\)</span>）。预设评分卡的分值为0-100分，那取预期分值<spanclass="math inline">\(S_0\)</span>为50分，并指定当Odds按双倍上下浮动时（比如2.5%或10%），分值则对应上下变动10分（比如60分或40分）。</li><li>这里<spanclass="math inline">\(\theta_0=5\%\)</span>是根据业务经验来的，没有数学依据；</li><li>0-100分是根据做评分卡的需要来的，没有数学依据。要是想做成600-1000分的评分卡也可以，修改对应的<spanclass="math inline">\(S_0\)</span>和PDO就行；</li><li><spanclass="math inline">\(S_0=50\)</span>分是根据0-100分来的，也可以取45分或73分，不重要。重要的是随着Odds翻番变动时，分数也随之变动的联动变化体系（你翻番我就变PDO=10分）</li></ol></blockquote><p><span class="math display">\[B = PDO/ln(2)\]</span></p><p><span class="math display">\[A = S_0-PDO*ln(\theta_0)/ln(2)\]</span></p><h2 id="评分卡的好处">评分卡的好处</h2><p>评分卡不使用概率转换分数，而是用线性部分转换成分数，这样做的好处是，每一个特征<spanclass="math inline">\(x\)</span>的分数和就是总分，而如果用概率转分，则单个特征增加s分，总分并不增加s分。<span class="math display">\[\begin{align}Score&amp;=A+B\times\ln(odds) \\&amp;=A+B\times(\omega_0+\omega_1x_1+\cdots+\omega_nx_x)\\&amp;=(A+B\omega_0)+B\omega_1x_1+\cdots+B\omega_nx_x\end{align}\]</span> <spanclass="math inline">\((A+B\omega_0)\)</span>是基础分数，<spanclass="math inline">\(B\omega_1x_1,\cdots,B\omega_nx_x\)</span>是每个变量对应分配的分数，求和得到总分数。</p><h2 id="分箱">分箱</h2><p>如果之前步骤中每个变量都有进行分箱操作，就把每个变量对应的分数，分别乘以变量中每个分箱的WOE，得到每个分箱的评分结果。</p><blockquote><p>WOE（Weight ofEvidence）即证据权重，可以将logistic回归模型转化为标准评分卡格式，WOE是对原始自变量的一种编码形式，要对一个变量进行WOE编码，需要首先把这个变量进行分组处理（也叫离散化、分箱）。WOE=ln(坏样本占比/好样本占比)<span class="math display">\[WOE_i=\ln(\frac{Bad_{X=X_i}/Bad_{tatol}}{Good_{X=X_i}/Good_{tatol}})\]</span></p></blockquote><figure><img src="https://image.yinan.fun/202308271006692.png" alt="评分卡" /><figcaption aria-hidden="true">评分卡</figcaption></figure><p>以上步骤都完成后，假如新产生一个新样本，我们只需将此用户每个变量对应到各分箱中得到其对应的WOE值，再根据上面的公式计算出这个样本在每个变量下的分数。最后将所有变量对应的分数相加，即为最终评分结果。</p><h2 id="参考资料">参考资料</h2><ol type="1"><li>https://blog.csdn.net/sscc_learning/article/details/78591210</li><li>https://www.ngui.cc/el/3600135.html?action=onClick</li></ol>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像质量评价指标</title>
    <link href="/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.html"/>
    <url>/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.html</url>
    
    <content type="html"><![CDATA[<h1 id="图像质量评价指标">图像质量评价指标</h1><p>图像质量评价指标的最常见分类是通过参考图像的可用性，即全参考（full-reference,FR）、减少参考（reduced-reference, RR）和无参考（no-reference,NR）质量度量。FR度量计算失真图像和参考图像之间的相似性，当参考图像的部分信息可用时应用RR度量，而NR度量利用图像统计来评估图像质量，因为参考图像的信息完全不可用。</p><figure><img src="https://image.yinan.fun/image-20230724182132014.png"alt="Representative FR and RR IQA methods" /><figcaption aria-hidden="true">Representative FR and RR IQAmethods</figcaption></figure><h2 id="psnr-peak-signal-to-noise-ratio-峰值信噪比">1. PSNR (PeakSignal-to-Noise Ratio) 峰值信噪比</h2><p>给定一个大小为<span class="math inline">\(m\timesn\)</span>的干净图像和噪声图像，<strong>均方误差(MSE)</strong>定义为：</p><p><span class="math display">\[MSE=\frac{1}{mn}\sum_{i=0}^{m-1}\sum_{i=0}^{n-1}[I(i,j)-K(i,j)]^2\]</span> 然后<strong>PSNR(dB) </strong> 就定义为：</p><p><span class="math display">\[PSNR=10\times log_{10}(\frac{MAX_I^2}{MSE})\]</span></p><p>其中<spanclass="math inline">\(MAX_I^2\)</span>为图片可能的最大像素值。如果每个像素都由8 位二进制来表示，那么就为255。通常，如果像素值由B位二进制来表示，那么<spanclass="math inline">\(MAX_I^2=2^B-1\)</span>。</p><p>一般地，针对 uint8 数据，最大像素值为255,；针对浮点型数据，最大像素值为 1。</p><p>上面是针对灰度图像的计算方法，如果是彩色图像，通常有三种方法来计算。</p><ul><li>分别计算 RGB 三个通道的 PSNR，然后取平均值。</li><li>计算 RGB 三通道的 MSE ，然后再除以 3 。</li><li>将图片转化为 YCbCr 格式，然后只计算 Y 分量也就是亮度分量的PSNR。</li></ul><p>其中，第二和第三种方法比较常见。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># method 1</span><br>diff = im1 - im2<br>mse = np.mean(np.square(diff))<br>psnr = <span class="hljs-number">10</span> * np.log10(<span class="hljs-number">255</span> * <span class="hljs-number">255</span> / mse)<br><br><span class="hljs-comment"># method 2</span><br><span class="hljs-keyword">from</span> skimage.metrics <span class="hljs-keyword">import</span> structural_similarity <span class="hljs-keyword">as</span> ssim<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment">#转为灰度图像</span><br>img1 = Image.<span class="hljs-built_in">open</span>(img1_path).convert(<span class="hljs-string">&#x27;L&#x27;</span>)<br>img2 = Image.<span class="hljs-built_in">open</span>(img2_path).convert(<span class="hljs-string">&#x27;L&#x27;</span>)<br>img2 = img2.resize(img1.size)<br>image_true, image_test = np.array(img1), np.array(img2)<br>psnr = skimage.metrics.peak_signal_noise_ratio(image_true, image_test, *, data_range=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><p><ahref="https://scikit-image.org/docs/dev/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio">https://scikit-image.org/docs/dev/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio</a></p><ul><li><p>参数：</p><p><code>image_true</code>:ndarray。Ground-truth图像，与im_test形状相同</p><p><code>image_test</code>: ndarray。测试图像</p><p><code>data_range</code>: int,optional。输入图像的数据范围(可能值的最小值和最大值之间的距离)。默认情况下，这是根据图像数据类型估计的</p></li><li><p>返回值</p><p><code>psnr</code>: float。PSNR值度量</p></li></ul><p>针对超光谱图像，我们需要针对不同波段分别计算PSNR，然后取平均值，这个指标称为 MPSNR。</p><p>PSNR等图像保真度测量方法虽然被广泛采用，但却无法很好地反映视觉质量。为了进一步接近人类视觉系统（humanvisual system, HVS）的质量评估，人们引入了结构相似性指数测量（Structuralsimilarity index measure, SSIM），利用其对结构信息变化的敏感性。</p><h2 id="ssim-structural-similarity-结构相似性">2. SSIM (StructuralSIMilarity) 结构相似性</h2><p>SSIM公式基于样本x和y之间的三个比较衡量：亮度 (luminance)、对比度(contrast) 和结构 (structure)。</p><p><span class="math display">\[l(x,y)=\frac{2\mu_x\mu_y+c_1}{\mu_x^2+\mu_y^2+c_1}\\c(x,y)=\frac{2\sigma_x\sigma_y+c_2}{\sigma_x^2+\sigma_y^2+c_2}\\s(x,y)=\frac{2\sigma_{xy}+c_3}{\sigma_x\sigma_y+c_3}\\\]</span></p><p>一般取<span class="math inline">\(c_3=c_2/2\)</span>。</p><ul><li><span class="math inline">\(\mu_x\)</span>为x均值，<spanclass="math inline">\(\sigma_x\)</span>为x方差，<spanclass="math inline">\(\sigma_{xy}\)</span>为x和y的协方差</li><li><span class="math inline">\(c_1=(k_1L)^2\)</span>,<spanclass="math inline">\(c_2=(k_2L)^2\)</span>为两个常数，避免除零</li><li>L为像素值的范围，<span class="math inline">\(L=2^B-1\)</span></li><li><span class="math inline">\(k_1=0.01,k_2=0.03\)</span>为默认值</li></ul><p>那么</p><p><span class="math display">\[SSIM(x,y)=[l(x,y)^\alpha\times c(x,y)^\beta\times s(x,y)^\gamma]\]</span> <spanclass="math inline">\(\alpha,\beta,\gamma\)</span>分别代表了不同特征在SSIM衡量中的占比，当都为1时，有：<span class="math display">\[SSIM(x,y)=\frac{(2\mu_x\mu_y+c_1)(2\sigma_x\sigma_y+c_2)}{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2+\sigma_y^2+c_2)}\]</span></p><p>每次计算的时候都从图片上取一个<span class="math inline">\(N\timesN\)</span>的窗口，然后不断滑动窗口进行计算，最后取平均值作为全局的SSIM。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">skimage.metrics.structural_similarity(im1, im2, *, win_size=<span class="hljs-literal">None</span>, gradient=<span class="hljs-literal">False</span>, data_range=<span class="hljs-literal">None</span>, <br>                                      channel_axis=<span class="hljs-literal">None</span>, gaussian_weights=<span class="hljs-literal">False</span>, full=<span class="hljs-literal">False</span>, **kwargs)<br></code></pre></td></tr></table></figure><p><ahref="https://scikit-image.org/docs/dev/api/skimage.metrics.html#skimage.metrics.structural_similarity">https://scikit-image.org/docs/dev/api/skimage.metrics.html#skimage.metrics.structural_similarity</a></p><ul><li><p>参数：</p><p><code>im1</code>, <code>im2</code>:ndarray。两个图片，任何具有相同形状的维度。</p><p><code>win_size</code>: int or None,optional。用来比对的滑动窗口的变长。必须是奇数。如果</p><p><code>gradient</code>: bool,optional。如果为True，也会返回相对于im2的梯度</p><p><code>data_range</code>: float,optional。输入图像的数据范围（最大和最小值之间可能的距离）使用默认，这会从图像数据类型中估计</p><p><code>channel_axis</code>: int or None,optional。如果为None，图像被假定为灰度图（单通道），否则，使用该参数索引轴的数组对应的通道</p><p><code>gaussian_weights</code>: bool,optional。如果为True，则每个patch的均值和方差由宽度σ=1.5的归一化高斯核进行空间加权。</p><p><code>full</code>: bool,optional。如果为True，也返回完整的结构相似度图像</p></li><li><p>返回值</p><p><code>mssim</code>: float。图像上的平均结构相似度指数</p><p><code>grad</code>:ndarray。im1和im2之间的结构相似度梯度。这只在梯度设置为True时返回</p><p><code>S</code>:ndarray。完整的SSIM映像。这只在full设置为True时返回</p></li></ul><p>针对超光谱图像，我们需要针对不同波段分别计算SSIM，然后取平均值，这个指标称为 MSSIM。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>评价指标</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>了解Llama</title>
    <link href="/%E4%BA%86%E8%A7%A3Llama.html"/>
    <url>/%E4%BA%86%E8%A7%A3Llama.html</url>
    
    <content type="html"><![CDATA[<h1 id="了解llama">了解Llama</h1><p>官网：<ahref="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a></p><p>文章：<ahref="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/</a></p><h2 id="介绍">介绍</h2><p>This release includes model weights and starting code for pretrainedand fine-tuned Llama language models — ranging from 7B to 70Bparameters(7B, 13B, 70B).</p><p>Llama 2 pretrained models are trained on 2 trillion tokens, and havedouble the context length than Llama 1. Its fine-tuned models have beentrained on over 1 million human annotations.</p><p><strong>Auto-regressive transformers</strong> are<strong>pretrained</strong> on an extensive corpus of self-superviseddata, followed by <strong>alignment</strong> with human preferences viatechniques such as Reinforcement Learning with Human Feedback(RLHF).</p><blockquote><p>Auto-regression is a time series model that <strong>uses observationsfrom previous time steps as input</strong> to a regression equation topredict the value at the next time step</p></blockquote><figure><img src="https://image.yinan.fun/image-20230723111842222.png"alt="AT VS. NAT" /><figcaption aria-hidden="true">AT VS. NAT</figcaption></figure><p>Llama2模型训练包括：预训练，有监督微调，RLHF。</p><figure><img src="https://image.yinan.fun/image-20230721150225425.png"alt="Training of Llama 2-Chat" /><figcaption aria-hidden="true">Training of Llama 2-Chat</figcaption></figure><p>The training methodology is simple, but high computationalrequirements.</p><h2 id="训练">训练</h2><h3 id="预训练数据">预训练数据</h3><ul><li><p>训练语料库包括来自公开来源的新数据组合，其中不包括来自 Meta产品或服务的数据。</p></li><li><p>剔除某些已知包含大量个人隐私信息的网站的数据。</p></li><li><p>在 2万亿个tokem的数据上进行了训练，很好地权衡性能与成本。</p></li><li><p>对最真实的数据源进行上采样，以增加知识和减少错误。</p></li></ul><h3 id="训练细节">训练细节</h3><p>We adopt most of the <strong>pretraining setting and modelarchitecture from Llama 1</strong>. We use the <strong>standardtransformer architecture</strong> (Vaswani et al., 2017), applypre-normalization using <strong>RMSNorm</strong> (Zhang and Sennrich,2019), use the <strong>SwiGLU</strong> activation function (Shazeer,2020), and rotary positional embeddings (RoPE, Su et al. 2022). Theprimary architectural differences from Llama 1 include increased<strong>context length</strong> and <strong>grouped-queryattention</strong> (GQA).</p><blockquote><p>LayerNorm是对特征张量按照某一维度或某几个维度进行0均值，1方差的归一化。RMSNorm是对LayerNorm的一个改进，没有做re-center操作（移除了其中的均值项）。RMSNorm也是一种标准化方法，但与 LayerNorm不同，它不是使用整个样本的均值和方差，而是使用平方根的均值来归一化，这样做可以降低噪声的影响。</p></blockquote><blockquote><p>旋转式位置编码（RoPE）最早是一种能够将相对位置信息依赖集成到self-attention中并提升transformer架构性能的位置编码方式。</p></blockquote><blockquote><p>GQA（Grouped-QueryAttention）是分组查询注意力，GQA将查询头分成G组，每个组共享一个Key 和Value 矩阵。GQA-G是指具有G组的grouped-queryattention。其中多个查询头关注相同的键和值头，以减少推理过程中 KV缓存的大小，并可以显著提高推理吞吐量。GQA-1具有单个组，因此具有单个Key和 Value，等效于MQA。而GQA-H具有与头数相等的组，等效于MHA。</p></blockquote><figure><img src="https://image.yinan.fun/image-20230723112453383.png"alt="Llama 2 family of models" /><figcaption aria-hidden="true">Llama 2 family of models</figcaption></figure><ul><li><p>Hyperparameters</p><p>We trained using the <strong>AdamW optimizer</strong> (Loshchilov andHutter, 2017), with β1 = 0.9, β2 = 0.95, eps = 10-5. We use a<strong>cosine learning rate schedule</strong>, with warmup of 2000steps, and decay final learning rate down to 10% of the peak learningrate. We use a weight decay of 0.1 and gradient clipping of1.0.</p></li></ul><figure><img src="https://image.yinan.fun/image-20230723115454030.png"alt="Training Loss for Llama 2 models" /><figcaption aria-hidden="true">Training Loss for Llama 2models</figcaption></figure><ul><li><p>Tokenizer</p><p>We use the same tokenizer as Llama 1; it employs a <strong>bytepairencoding</strong> (BPE) algorithm (Sennrich et al., 2016) using theimplementation from SentencePiece (Kudo and Richardson, 2018). As withLlama 1, we split all numbers into individual digits and use bytes todecompose unknown UTF-8 characters. The total vocabulary size is 32ktokens.</p></li><li><p>Llama 2预训练模型评估</p><ul><li>和开源模型对比，Llama 2性能最好。</li></ul><figure><img src="https://image.yinan.fun/image-20230723120200445.png"alt="Overall performance on grouped academic benchmarks compared to open-source base models" /><figcaption aria-hidden="true">Overall performance on grouped academicbenchmarks compared to open-source base models</figcaption></figure><ul><li>和闭源模型对比，Llama 2有很大的性能差距。</li></ul><figure><img src="https://image.yinan.fun/image-20230723120239606.png"alt="Comparison to closed-source models" /><figcaption aria-hidden="true">Comparison to closed-sourcemodels</figcaption></figure></li></ul><h2 id="微调">微调</h2><p>Llama 2-Chat是数月研究和反复应用对齐（alignment）技术的成果，包括指令调整（instructiontuning）和 RLHF，需要大量的计算和注释资源。</p><h3 id="supervised-fine-tuning-sft">Supervised Fine-Tuning (SFT)</h3><ul><li><p>Getting Started</p><p>To bootstrap, we started the SFT stage with <strong>publiclyavailable instruction tuning data</strong> (Chung et al., 2022), asutilized previously in Touvron et al. (2023).</p></li><li><p>Quality Is All You Need</p><p>By setting aside millions of examples from third-party datasets andusing <strong>fewer but higher-quality examples</strong> from our ownvendor-based annotation efforts, our results notably improved.</p></li></ul><p>For the fine-tuning process, each sample consists of a prompt and ananswer. To ensure the model sequence length is properly filled, we<strong>concatenate all the prompts and answers</strong> from thetraining set. A special token is utilized to separate the prompt andanswer segments. We utilize an autoregressive objective and zero-out theloss on tokens from the user prompt, so as a result, we backpropagateonly on answer tokens. Finally, we fine-tune the model for 2 epochs.</p><h3 id="reinforcement-learning-with-human-feedback-rlhf">ReinforcementLearning with Human Feedback (RLHF)</h3><p>RLHF is a model training procedure that is applied to a fine-tunedlanguage model to <strong>further align model behavior with humanpreferences and instruction following</strong>. We collect data thatrepresents empirically sampled human preferences, whereby humanannotators select which of two model outputs they prefer. This humanfeedback is subsequently used to <strong>train a reward model</strong>,which learns patterns in the preferences of the human annotators and canthen automate preference decisions.</p><ul><li><h4 id="human-preference-data-collection">Human Preference DataCollection</h4><p>Our annotation procedure proceeds as follows. We ask annotators tofirst <strong>write a prompt</strong>, then choose between two sampledmodel responses, based on provided criteria. In order to<strong>maximize the diversity</strong>, the two responses to a givenprompt are sampled from two different model variants, and varying thetemperature hyper-parameter. In addition to giving participants a forcedchoice, we also ask annotators to <strong>label the degree</strong> towhich they prefer their chosen response over the alternative: eithertheir choice is significantly better, better, slightly better, ornegligibly better/ unsure.</p><p>For our collection of preference annotations, we focus on<strong>helpfulness and safety</strong>.</p><p>Llama 2-Chat improvement also <strong>shifted the model’s datadistribution</strong>. Since reward model accuracy can quickly degradeif not exposed to this new sample distribution, i.e., fromhyper-specialization (Scialom et al., 2020b), it is important before anew Llama 2-Chat tuning iteration to <strong>gather new preferencedata</strong> using the latest Llama 2-Chat iterations. This step helpskeep the reward model on-distribution and maintain an accurate rewardfor the latest model.</p></li><li><h4 id="reward-modeling">Reward Modeling</h4><p>The reward model takes a model response and its corresponding prompt(including contexts from previous turns) as inputs and outputs a scalarscore to indicate the quality (e.g., helpfulness and safety) of themodel generation.</p><p>Helpfulness and safety sometimes trade off (Bai et al., 2022a), whichcan make it challenging for a single reward model to perform well onboth. To address this, we train <strong>two separate rewardmodels</strong>, one optimized for helpfulness (referred to asHelpfulness RM) and another for safety (Safety RM).</p><p><strong>Training Objectives</strong> <span class="math display">\[\mathcal{L}_{ranking}=-log(\sigma(r_{\theta}(x,y_c)-r_{\theta}(x,y_r)-m(r)))\]</span> where <span class="math inline">\(r_{\theta}(x,y)\)</span> isthe scalar score output for prompt <spanclass="math inline">\(x\)</span> and completion <spanclass="math inline">\(y\)</span> with model weights <spanclass="math inline">\(\theta\)</span>. <spanclass="math inline">\(y_c\)</span> is the preferred response thatannotators choose and <span class="math inline">\(y_r\)</span> is therejected counterpart. Margin <span class="math inline">\(m(r)\)</span>is a discrete function of the preference rating. Naturally, we use alarge margin for pairs with distinct responses, and a smaller one forthose with similar responses.</p></li><li><h4 id="iterative-fine-tuning">Iterative Fine-Tuning</h4><p>Two main algorithms</p><ul><li><p>Proximal Policy Optimization (PPO) (Schulman et al., 2017), thestandard in RLHF literature.</p><blockquote><p>近端策略优化（proximal policyoptimization，PPO），通过重要性采样把同策略换成异策略。</p><p><ahref="https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5">https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5</a></p></blockquote></li><li><p>Rejection Sampling fine-tuning. We <strong>sample Koutputs</strong> from the model and <strong>select the bestcandidate</strong> with our reward. Here, we go one step further, anduse the selected outputs for a <strong>gradient update</strong>. Foreach prompt, the sample obtaining the highest reward score is consideredthe new <strong>gold standard</strong>. Similar to Scialom et al.(2020a), we then fine-tune our model on the new set of ranked samples,reinforcing the reward.</p><blockquote><p>挑出分数最高的response作为training target，做supervised fine-tuning(SFT)。</p></blockquote><p>We perform rejection <strong>sampling only</strong> with our<strong>largest 70B Llama 2-Chat</strong>. All smaller models are<strong>fine-tuned on rejection sampled data from the largermodel</strong>, thus distilling the large-model capabilities into thesmaller ones.</p></li></ul></li></ul><h2 id="使用llama">使用Llama</h2><p><ahref="https://huggingface.co/docs/transformers/main/en/model_doc/llama">https://huggingface.co/docs/transformers/main/en/model_doc/llama</a></p><p><ahref="https://huggingface.co/docs/transformers/main/en/model_doc/llama2">https://huggingface.co/docs/transformers/main/en/model_doc/llama2</a></p><ul><li>下载模型参数。Weights for the LLaMA models can be obtained from byfilling out <ahref="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form">thisform</a></li><li>转换为Hugging Face格式。After downloading the weights, they willneed to be converted to the Hugging Face Transformers format using the<ahref="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">conversionscript</a>. The script can be called with the following (example)command:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">python src/transformers/models/llama/convert_llama_weights_to_hf.py \<br>    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path<br></code></pre></td></tr></table></figure><ul><li>加载模型和tokenizer。After conversion, the model and tokenizer canbe loaded via:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaForCausalLM, LlamaTokenizer<br><br>tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)<br>model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>调试运行华驼模型</title>
    <link href="/%E8%B0%83%E8%AF%95%E8%BF%90%E8%A1%8C%E5%8D%8E%E9%A9%BC%E6%A8%A1%E5%9E%8B.html"/>
    <url>/%E8%B0%83%E8%AF%95%E8%BF%90%E8%A1%8C%E5%8D%8E%E9%A9%BC%E6%A8%A1%E5%9E%8B.html</url>
    
    <content type="html"><![CDATA[<h1 id="调试运行华驼模型">调试运行华驼模型</h1><p>论文地址：<ahref="https://arxiv.org/abs/2304.06975">https://arxiv.org/abs/2304.06975</a></p><p>项目地址：<ahref="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese">https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese</a></p><ul><li><p>运行<code>infer.py</code>报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">UnicodeDecodeError: <span class="hljs-string">&#x27;gbk&#x27;</span> codec can<span class="hljs-string">&#x27;t decode byte 0xae in position 84: illegal </span><br><span class="hljs-string">multibyte sequence</span><br></code></pre></td></tr></table></figure><p>修改<code>prompter.py</code>21行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> fp:<br></code></pre></td></tr></table></figure><p>修改<code>infer.py</code>18行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(instruct_dir, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br></code></pre></td></tr></table></figure></li><li><p>运行<code>infer.py</code>报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">HFValidationError: Repo <span class="hljs-built_in">id</span> must use alphanumeric chars or <span class="hljs-string">&#x27;-&#x27;</span>, <span class="hljs-string">&#x27;_&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;--&#x27;</span> <br>and <span class="hljs-string">&#x27;..&#x27;</span> are forbidden, <span class="hljs-string">&#x27;-&#x27;</span> and <span class="hljs-string">&#x27;.&#x27;</span> cannot start or end the name, max length is<br>96: <span class="hljs-string">&#x27;&#x27;</span>.<br></code></pre></td></tr></table></figure><p>尝试下载模型参数，调用本地模型参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">base_model: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;./llama-7b-hf&quot;</span>,<br></code></pre></td></tr></table></figure></li><li><p>运行<code>infer.py</code>报错（使用CPU）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">The current device_map had weights offloaded to the disk. Please provide an offload_folder <span class="hljs-keyword">for</span> them. Alternatively, make sure you have safetensors installed <span class="hljs-keyword">if</span> the model you are using offers the weights <span class="hljs-keyword">in</span> this format<br></code></pre></td></tr></table></figure><p><ahref="https://github.com/huggingface/transformers/issues/18698">https://github.com/huggingface/transformers/issues/18698</a>这里说</p><p>There is no support for using the CPU as a main device in Accelerateyet. If you want to use the model on CPU, just don't specific<code>device_map="auto"</code>.</p><p>参数去掉<code>device_map="auto"</code></p></li><li><p>修改<code>infer.py</code>13行（使用CPU）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p>运行<code>infer.py</code>报错（使用CPU）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">RuntimeError: <span class="hljs-string">&quot;addmm_impl_cpu_&quot;</span> not implemented <span class="hljs-keyword">for</span> <span class="hljs-string">&#x27;Half&#x27;</span><br></code></pre></td></tr></table></figure><p>CPU无法使用 <code>fp16</code>（半精度），把<code>model.half()</code>，改成<code>model.float()</code></p></li><li><p>未微调模型结果（CPU跑了两个小时）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">###infering###</span><br><span class="hljs-comment">###instruction###</span><br>小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。<br><span class="hljs-comment">###golden output###</span><br>小张可能患有心肌炎，建议进行心电图和心脏超声等检查来确定诊断。治疗方案包括使用泼尼松、生脉饮和三磷酸腺苷等药物，同时建议适当控制体温，保持良好的营养状况。<br><span class="hljs-comment">###model output###</span><br>小张感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。 问题：小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。 问题：小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。 问题：小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减��<br></code></pre></td></tr></table></figure></li><li><p>微调模型结果（基于医学知识库对<code>LLaMA</code>进行指令微调的<code>LoRA</code>权重文件）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash">using lora ./lora-llama-med<br><span class="hljs-comment">###infering###</span><br><span class="hljs-comment">###instruction###</span><br>小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。<br><span class="hljs-comment">###golden output###</span><br>小张可能患有心肌炎，建议进行心电图和心脏超声等检查来确定诊断。治疗方案包括使用泼尼松、生脉饮和三磷酸腺苷等药物，同时建议适当控制体温，保持良好的营养状况。<br><span class="hljs-comment">###model output###</span><br>小张可能患有心肌炎，需要进行心脏超声、心电图检查等辅助检查以确定诊断。治疗方案包括使用抗生素、抗坏血酸等药物进行治疗，同时建议加强营养，避免过度劳累。&lt;/s&gt;<br><span class="hljs-comment">###infering###</span><br><span class="hljs-comment">###instruction###</span><br>小刘最近咳嗽流鼻涕，应该采取哪些治疗方案？<br><span class="hljs-comment">###golden output###</span><br>中药对咳嗽的治疗具有多靶点优势，如连花清咳片等，其融汇了东汉张仲景《伤寒论》中宣肺泄热、止咳平喘的麻杏石甘汤；明代叶文龄《医学统旨》中的清金化痰汤，其中清半夏燥湿化痰，桑白皮止咳平喘，苦杏仁镇咳平喘，桔梗宣肺利咽、镇咳排痰，前胡降气化痰；宫廷要药大黄通腑泄肺，让引起咳嗽的肺部热毒通过排便排出体外；再配合具有清热解毒作用的山银花、黄芩、连翘等，可以说是集中了清宣肺热、止咳化痰、促进排痰、利咽平喘的一批药物，发挥多靶点治疗作用。<br><span class="hljs-comment">###model output###</span><br>对于小刘的咳嗽流鼻涕，治疗方案可以考虑使用链霉素、氨苄西林、硫唑嘌呤等药物。&lt;/s&gt;<br></code></pre></td></tr></table></figure></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Idea快捷键</title>
    <link href="/Idea%E5%BF%AB%E6%8D%B7%E9%94%AE.html"/>
    <url>/Idea%E5%BF%AB%E6%8D%B7%E9%94%AE.html</url>
    
    <content type="html"><![CDATA[<h1 id="idea开发常用快捷键">IDEA开发常用快捷键</h1><table><thead><tr class="header"><th>快捷键组合</th><th>实现效果</th></tr></thead><tbody><tr class="odd"><td>psvm + Tab键 / main + Tab键</td><td>public static void main(String[] args)</td></tr><tr class="even"><td>sout + Tab键</td><td>System.out.println()</td></tr><tr class="odd"><td>Ctrl + X</td><td>删除当前行</td></tr><tr class="even"><td>Ctrl +D</td><td>复制当前行</td></tr><tr class="odd"><td>Alt+Insert(或右键Generate)</td><td>生成代码(如get,set方法,构造函数等)</td></tr><tr class="even"><td>Ctrl+Alt+T</td><td>生成try catch （或者 Alt+enter选择）</td></tr><tr class="odd"><td>CTRL+ALT+T</td><td>把选中的代码放在 TRY{} IF{} ELSE{} 里</td></tr><tr class="even"><td>Ctr+shift+U</td><td>实现大小写之间的转化</td></tr><tr class="odd"><td>ALT+回车</td><td>导入包,自动修正</td></tr><tr class="even"><td>CTRL+ALT+L</td><td>格式化代码</td></tr><tr class="odd"><td>CTRL+ALT+I</td><td>自动缩进</td></tr><tr class="even"><td>CTRL+E</td><td>最近更改的代码</td></tr><tr class="odd"><td>fori</td><td>生成for (int i = 0; i &lt; ; i++) {}</td></tr><tr class="even"><td>Alt + &lt;–左右–&gt;键</td><td>实现窗口左右更换（多窗口）</td></tr><tr class="odd"><td>Ctrl + 鼠标点击</td><td>快速找到成员变量的出处</td></tr><tr class="even"><td>Shift+F6</td><td>重构/重命名 (包、类、方法、变量、甚至注释等)</td></tr><tr class="odd"><td>CTRL+Q</td><td>查看当前方法的声明</td></tr><tr class="even"><td>Ctrl+Alt+V</td><td>自动创建变量（new 对象();之后选择按快捷键）</td></tr><tr class="odd"><td>Ctrl+O</td><td>重写方法</td></tr><tr class="even"><td>Ctrl+I</td><td>实现方法</td></tr><tr class="odd"><td>ALT+/</td><td>代码提示</td></tr><tr class="even"><td>Ctrl+Shift+R</td><td>在当前项目中替换指定内容</td></tr><tr class="odd"><td>Ctrl+E</td><td>最近编辑的文件列表</td></tr><tr class="even"><td>Ctrl+P</td><td>显示方法参数信息</td></tr><tr class="odd"><td>Ctrl+Shift+Insert</td><td>查看历史复制记录，idea可以保留历史复制的 100 条记录</td></tr></tbody></table><h2 id="控制台语句-system.out-相关">控制台语句 System.out 相关：</h2><table><thead><tr class="header"><th>生成控制台的相关快捷键</th><th>描述</th></tr></thead><tbody><tr class="odd"><td>sout + Tab键</td><td>生成System.out.println();，输出到控制台语句并换行。</td></tr><tr class="even"><td>souf + Tab键</td><td>生成System.out.printf("");,输出一个格式化字符串到控制台。</td></tr><tr class="odd"><td>soutm + Tab键</td><td>生成System.out.println("类名.方法名");，输出当前 类和方法名到控制台。</td></tr><tr class="even"><td>soutp + Tab键</td><td>生成System.out.println(所有方法参数名+值);，输出当前方法的参数名和值 到控制台。</td></tr></tbody></table><h2 id="查找">查找</h2><table><thead><tr class="header"><th>快捷键</th><th>介绍</th></tr></thead><tbody><tr class="odd"><td>Ctrl + F</td><td>在当前文件进行文本查找</td></tr><tr class="even"><td>Ctrl + R</td><td>在当前文件进行文本替换</td></tr><tr class="odd"><td>Shift + Ctrl + F</td><td>在项目进行文本查找</td></tr><tr class="even"><td>Shift + Ctrl + R</td><td>在项目进行文本替换</td></tr><tr class="odd"><td>Shift + Shift</td><td>快速搜索</td></tr><tr class="even"><td>Ctrl + N</td><td>查找class</td></tr><tr class="odd"><td>Ctrl + Shift + N</td><td>查找文件</td></tr><tr class="even"><td>Ctrl + Shift + Alt + N</td><td>查找symbol（查找某个方法名）</td></tr></tbody></table><h2 id="跳转切换">跳转切换</h2><table><thead><tr class="header"><th>快捷键</th><th>介绍</th></tr></thead><tbody><tr class="odd"><td>Ctrl + E</td><td>最近文件</td></tr><tr class="even"><td>Ctrl + Tab</td><td>切换文件</td></tr><tr class="odd"><td>Ctrl + Alt + ←/→</td><td>跳转历史光标所在处</td></tr><tr class="even"><td>Alt + ←/→ 方向键</td><td>切换子tab</td></tr><tr class="odd"><td>Ctrl + G</td><td>go to（跳转指定行号）</td></tr></tbody></table><h2 id="编码相关">编码相关</h2><table><thead><tr class="header"><th>快捷键</th><th>介绍</th></tr></thead><tbody><tr class="odd"><td>Ctrl + W</td><td>快速选中</td></tr><tr class="even"><td>(Shift + Ctrl) + Alt + J</td><td>快速选中同文本</td></tr><tr class="odd"><td>Ctrl + C/Ctrl + X/Ctrl + D</td><td>快速复制或剪切</td></tr><tr class="even"><td>多行选中 Tab / Shift + Tab</td><td>tab</td></tr><tr class="odd"><td>Ctrl + Y</td><td>删除整行</td></tr><tr class="even"><td>滚轮点击变量/方法/类</td><td>快速进入变量/方法/类的定义处</td></tr><tr class="odd"><td>Shift + 点击Tab</td><td>快速关闭tab</td></tr><tr class="even"><td>Ctrl + Z 、Ctrl + Shift + Z</td><td>后悔药，撤销/取消撤销</td></tr><tr class="odd"><td>Ctrl + Shift + enter</td><td>自动收尾，代码自动补全</td></tr><tr class="even"><td>Alt + enter</td><td>IntelliJ IDEA根据光标所在问题，提供快速修复选择，光标放在的位置不同提示的结果也不同</td></tr><tr class="odd"><td>Alt + ↑/↓</td><td>方法快速跳转</td></tr><tr class="even"><td>F2</td><td>跳转到下一个高亮错误 或 警告位置</td></tr><tr class="odd"><td>Alt + Insert</td><td>代码自动生成，如生成对象的 set / get 方法，构造函数，toString()等</td></tr><tr class="even"><td>Ctrl + Shift + L</td><td>格式化代码</td></tr><tr class="odd"><td>Shift + F6</td><td>快速修改方法名、变量名、文件名、类名等</td></tr><tr class="even"><td>Ctrl + F6</td><td>快速修改方法签名</td></tr></tbody></table><h2 id="代码阅读相关">代码阅读相关</h2><table><thead><tr class="header"><th>快捷键</th><th>介绍</th></tr></thead><tbody><tr class="odd"><td>Ctrl + P</td><td>方法参数提示显示</td></tr><tr class="even"><td>Ctrl + Shift + i</td><td>就可以在当前类里再弹出一个窗口出来</td></tr><tr class="odd"><td>Alt + F7</td><td>可以列出变量在哪些地方被使用了</td></tr><tr class="even"><td>光标在子类接口名，Ctrl + u</td><td>跳到父类接口</td></tr><tr class="odd"><td>Alt + F1 + 1， esc</td><td></td></tr><tr class="even"><td>(Shift) + Ctrl + +/-</td><td>代码块折叠</td></tr><tr class="odd"><td>Ctrl + Shift + ←/→</td><td>移动窗口分割线</td></tr><tr class="even"><td>Ctrl + (Alt) + B</td><td>跳转方法定义/实现</td></tr><tr class="odd"><td>Ctrl + H</td><td>类的层级关系</td></tr><tr class="even"><td>Ctrl + F12</td><td>Show Members 类成员快速显示</td></tr></tbody></table><h2 id="版本管理相关">版本管理相关</h2><table><thead><tr class="header"><th>快捷键</th><th>介绍</th></tr></thead><tbody><tr class="odd"><td>Ctrl + D</td><td>Show Diff</td></tr><tr class="even"><td>(Shift) + F7</td><td>（上）下一处修改</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>备忘大全</category>
      
    </categories>
    
    
    <tags>
      
      <tag>快捷键</tag>
      
      <tag>Idea</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Domain-Adversarial Training of Neural Networks（DaNN）实现</title>
    <link href="/Domain-Adversarial-Training-of-Neural-Networks%EF%BC%88DaNN%EF%BC%89%E5%AE%9E%E7%8E%B0.html"/>
    <url>/Domain-Adversarial-Training-of-Neural-Networks%EF%BC%88DaNN%EF%BC%89%E5%AE%9E%E7%8E%B0.html</url>
    
    <content type="html"><![CDATA[<h1id="domain-adversarial-training-of-neural-networksdann实现">Domain-AdversarialTraining of Neural Networks（DaNN）实现</h1><h2 id="总体介绍">总体介绍</h2><p>在传统的机器学习中，我们经常需要<strong>大量带标签的数据</strong>进行训练，并且需要保证<strong>训练集和测试集中的数据分布相似</strong>。在一些问题中，如果训练集和测试集的数据具有不同的分布，训练后的分类器在测试集上就没有好的表现。</p><p><strong>域适应</strong>（DomainAdaption）是<strong>迁移学习</strong>中一个重要的分支，目的是把具有不同分布的<strong>源域</strong>(SourceDomain) 和<strong>目标域</strong> (Target Domain)中的数据，映射到同一个特征空间，寻找某一种度量准则，使其在这个空间上的“距离”尽可能近。然后，我们在源域(带标签) 上训练好的分类器，就可以直接用于目标域数据的分类。</p><p><strong>DaNN</strong>是一种域适应学习方法，它采用了GAN的思想。为了使模型在目标集上也能有好的表现，它的目的是使模型特征提取器在源域和目标域提取的特征具有相同的分布。</p><p>DANN结构主要包含3个部分：</p><figure><img src="https://image.yinan.fun/image-20230713155252247.png"alt="image-20230713155252247" /><figcaption aria-hidden="true">image-20230713155252247</figcaption></figure><ul><li><strong>特征提取器</strong> （feature extractor） -图示绿色部分，用来<strong>将数据映射到特定的特征空间</strong>，使标签预测器能够分辨出来自源域数据的类别的同时，域判别器无法区分数据来自哪个域。</li><li>标签预测器 （label predictor） -图示蓝色部分，对来自源域的数据进行分类，尽可能分出<strong>正确的标签</strong>。</li><li><strong>域判别器</strong>（domain classifier）-图示红色部分，对特征空间的数据进行分类，尽可能分出<strong>数据来自哪个域</strong>。</li></ul><p>对抗迁移网络的总损失由两部分构成：网络的训练损失（标签预测器损失）和域判别损失。</p><p>我们通过最小化目标函数来更新标签预测器的参数，最大化目标函数来更新域判别器的参数。</p><p>相关论文：https://arxiv.org/abs/1505.07818</p><h2 id="代码实现">代码实现</h2><p>特征提取器实现，VGG网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeatureExtractor</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(FeatureExtractor, self).__init__()<br><br>        self.conv = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">128</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">256</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br><br>            nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.BatchNorm2d(<span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv(x).squeeze()<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>标签预测器实现，由线性层组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LabelPredictor</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(LabelPredictor, self).__init__()<br><br>        self.layer = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br><br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br><br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, h</span>):<br>        c = self.layer(h)<br>        <span class="hljs-keyword">return</span> c<br></code></pre></td></tr></table></figure><p>域判别器实现，由线性层组成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DomainClassifier</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(DomainClassifier, self).__init__()<br><br>        self.layer = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">512</span>),<br>            nn.ReLU(),<br><br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">512</span>),<br>            nn.ReLU(),<br><br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">512</span>),<br>            nn.ReLU(),<br><br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">512</span>),<br>            nn.ReLU(),<br><br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">1</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, h</span>):<br>        y = self.layer(h)<br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><p>损失函数选择</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">class_criterion = nn.CrossEntropyLoss()<br>domain_criterion = nn.BCEWithLogitsLoss()<br></code></pre></td></tr></table></figure><p>训练过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i, ((source_data, source_label), (target_data, _)) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(source_dataloader, target_dataloader)):<br>    source_data = source_data.to(device)<br>    source_label = source_label.to(device)<br>    target_data = target_data.to(device)<br>    mixed_data = torch.cat([source_data, target_data], dim=<span class="hljs-number">0</span>)<br>    domain_label = torch.zeros([source_data.shape[<span class="hljs-number">0</span>] + target_data.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>]).to(device)<br>    domain_label[:source_data.shape[<span class="hljs-number">0</span>]] = <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 1. 先训练domain classifier</span><br>    feature = feature_extractor(mixed_data)<br>    domain_logits = domain_classifier(feature.detach())<br>    loss = domain_criterion(domain_logits, domain_label)<br>    domain_classifier_losses.append(loss.item())<br>    running_D_loss += loss.item()<br>    loss.backward()<br>    optimizer_D.step()<br><br>    <span class="hljs-comment"># 2. 训练 feature extractor 和 label classifier</span><br>    class_logits = label_predictor(feature[:source_data.shape[<span class="hljs-number">0</span>]])<br>    domain_logits = domain_classifier(feature)<br>    loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)<br>    total_losses.append(loss.item())<br>    running_F_loss += loss.item()<br>    loss.backward()<br>    optimizer_F.step()<br>    optimizer_C.step()<br><br>    optimizer_D.zero_grad()<br>    optimizer_F.zero_grad()<br>    optimizer_C.zero_grad()<br></code></pre></td></tr></table></figure><h2 id="结果">结果</h2><p>训练损失上下波动，是特征提取器和域判别器在不断对抗。</p><figure><img src="https://image.yinan.fun/image-20230713171352103.png"alt="image-20230713171352103" /><figcaption aria-hidden="true">image-20230713171352103</figcaption></figure><p>训练10、100、200个epoch后，特征提取器提取的特征分布按类别和source/target展示。可以看到，随着训练的过程，特征越来越能区分不同的类，source和targetdomain的分布越来越一致。</p><figure><img src="https://image.yinan.fun/image-20230713171446520.png"alt="image-20230713171446520" /><figcaption aria-hidden="true">image-20230713171446520</figcaption></figure><figure><img src="https://image.yinan.fun/image-20230713171459818.png"alt="image-20230713171459818" /><figcaption aria-hidden="true">image-20230713171459818</figcaption></figure><figure><img src="https://image.yinan.fun/image-20230713171519185.png"alt="image-20230713171519185" /><figcaption aria-hidden="true">image-20230713171519185</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>项目</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>了解ChatGPT</title>
    <link href="/%E4%BA%86%E8%A7%A3ChatGPT.html"/>
    <url>/%E4%BA%86%E8%A7%A3ChatGPT.html</url>
    
    <content type="html"><![CDATA[<h1 id="了解chatgpt">了解ChatGPT</h1><h2 id="chatgpt的功能">1. ChatGPT的功能</h2><p>如果我们向ChatGPT询问ChatGPT的<strong>功能</strong>：</p><p>ChatGPT具有以下功能：</p><ol type="1"><li>回答问题：ChatGPT可以<strong>回答各种问题</strong>，包括常见知识、事实、定义、历史事件等。</li><li>提供解释：ChatGPT可以<strong>解释复杂的概念</strong>、原理、术语等，并帮助你更好地理解。</li><li>提供建议：ChatGPT可以<strong>提供关于各种主题的建议</strong>，例如旅行目的地、电影推荐、食谱等。</li><li>进行对话：ChatGPT可以<strong>进行对话</strong>，你可以与它交流，分享观点、提问问题或寻求帮助。</li><li>写作辅助：如果你需要<strong>写作文章</strong>、创作故事或撰写邮件，ChatGPT可以<strong>提供灵感</strong>、提供写作建议和进行文本生成。</li></ol><p>请注意，尽管ChatGPT具备广泛的知识和语言理解能力，但它仍然是一个模型，<strong>可能会出现错误或提供不准确的答案</strong>。在使用时，请谨慎评估和验证信息的准确性。（ChatGPT生成）</p><p>另外，从OpenAI的官网我们可以了解到，ChatGPT<strong>以对话的方式进行交互</strong>，对话形式使ChatGPT能够回答后续问题、承认错误、质疑不正确的前提并拒绝不适当的请求。</p><p>OpenAI的官网也列出了ChatGPT的<strong>缺点</strong>，例如有时会回答看似有道理但是是错误或没意义的回答，对提问的方式敏感，模型过于冗长，在用户询问模糊的问题时猜测用户的想法。</p><h2 id="chatgpt的原理">2. ChatGPT的原理</h2><p>如果我们向ChatGPT询问ChatGPT的<strong>原理</strong>：</p><p>ChatGPT的实现是基于<strong>深度学习</strong>和<strong>自监督学习</strong>的方法。下面将详细说明ChatGPT的实现过程：</p><ol type="1"><li>数据收集：为了<strong>预训练</strong>ChatGPT，大量的互联网<strong>文本数据</strong>被收集和准备。这些数据可以包括网页内容、书籍、维基百科、新闻文章等。这些文本数据被用来构建一个庞大的语料库，作为模型的训练数据。</li><li>构建Transformer模型：ChatGPT是基于<strong>Transformer</strong>模型构建的。Transformer模型是一种使用自注意力机制（self-attention）的序列到序列模型，它能够处理长距离依赖关系并捕捉上下文信息。Transformer模型由多个编码器和解码器层组成，每个层都包含多头注意力机制和前馈神经网络。</li><li>预训练：在<strong>预训练</strong>阶段，ChatGPT使用大规模的语料库进行训练。模型通过<strong>自监督学习</strong>的方式进行预训练，即在没有人工标注的情况下进行训练。预训练的目标是<strong>预测下一个单词或标记</strong>。通过这种方式，模型能够学习到语言的统计规律、语义关系和上下文信息。预训练使用了大量的计算资源和并行计算，通常需要花费数天甚至数周的时间。</li><li>微调：在预训练之后，ChatGPT需要进行<strong>微调</strong>以<strong>适应特定的任务</strong>。对于对话式交互，微调阶段通常使用<strong>对话数据</strong>进行训练。对话数据包括用户的问题和模型的回答。通过在这些对话数据上进行微调，模型能够更好地理解和生成对话，并提供准确的回答。</li><li>模型优化和部署：在微调之后，ChatGPT需要进行<strong>模型优化和部署</strong>，以提高性能和效率。这包括模型的压缩、加速和优化，以便在实际应用中能够快速响应用户的请求。</li></ol><p>需要注意的是，ChatGPT的实现过程非常复杂，并需要大量的计算资源和数据。OpenAI团队在实现ChatGPT时投入了大量的研究和工程工作，以使其具备强大的语言理解和生成能力。（ChatGPT生成）</p><p>OpenAI的官网简要说明了ChatGPT的方法。模型训练使用了<strong>ReinforcementLearning from Human Feedback</strong>(RLHF)，即以强化学习方式依据人类反馈优化语言模型。开始使用有监督的微调训练初始模型。然后收集comparisondata为强化学习训练一个奖励模型（reward model）。最后使用Proximal PolicyOptimization强化学习算法微调模型。</p><figure><img src="https://image.yinan.fun/ChatGPT_Diagram.svg"alt="Chatgpt Diagram" /><figcaption aria-hidden="true">Chatgpt Diagram</figcaption></figure><p>OpenAI的官网说：We trained this model using Reinforcement Learningfrom Human Feedback (RLHF), using the same methods as <ahref="https://openai.com/blog/instruction-following/"><strong>InstructGPT</strong></a>,but with slight differences in the data collection setup.因此，要了解更多关于ChatGPT也可以阅读instructGPT的论文：https://arxiv.org/abs/2203.02155。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ChatGPT</tag>
      
      <tag>语言模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>爱楼表情包网站</title>
    <link href="/%E7%88%B1%E6%A5%BC%E8%A1%A8%E6%83%85%E5%8C%85%E7%BD%91%E7%AB%99.html"/>
    <url>/%E7%88%B1%E6%A5%BC%E8%A1%A8%E6%83%85%E5%8C%85%E7%BD%91%E7%AB%99.html</url>
    
    <content type="html"><![CDATA[<h1 id="爱楼-表情包网站">爱楼-表情包网站</h1><h2 id="页面展示">页面展示</h2><ul><li>主页</li></ul><figure><img src="https://image.yinan.fun/202308242323122.png"alt="image-20230717161539797" /><figcaption aria-hidden="true">image-20230717161539797</figcaption></figure><ul><li>管理员页面</li></ul><figure><img src="https://image.yinan.fun/image-20230717161629434.png"alt="image-20230717161629434" /><figcaption aria-hidden="true">image-20230717161629434</figcaption></figure><ul><li>后端文档</li></ul><figure><img src="https://image.yinan.fun/image-20230717161823777.png"alt="image-20230717161823777" /><figcaption aria-hidden="true">image-20230717161823777</figcaption></figure><h2 id="需求分析">需求分析</h2><ol type="1"><li>用户根据名字和标签搜索表情包</li><li>用户上传表情包</li><li>管理员管理表情包</li></ol><h2 id="技术栈">技术栈</h2><h3 id="前端">前端</h3><ul><li>框架：Vue 3</li><li>组件库：Ant Design Vue</li><li>请求：Axios</li><li>路由：Vue Router</li><li>文件下载：FileSaver</li></ul><h3 id="后端">后端</h3><ul><li>Java 8</li><li>开发框架：SpringBoot 2.x</li><li>数据访问：MyBatis + MyBatis Plus</li><li>缓存：Redis</li><li>项目管理：Maven</li><li>接口文档：Swagger + Knife4j</li></ul><h3 id="存储">存储</h3><ul><li>数据库：MySQL</li><li>对象存储：七牛云 COS</li></ul><h3 id="部署">部署</h3><ul><li>前端：Nginx + Docker</li><li>后端：Docker容器</li></ul><h2 id="特性">特性</h2><ol type="1"><li>使用 Redis缓存首页高频访问的表情包列表，将接口响应时长从120毫秒缩短至20毫秒。<br /></li><li>选用 MyBatis + MyBatis-Plus进行数据访问层开发，复用大多数通用方法，并且通过继承定制了自己的通用操作模板，大幅提升了项目开发效率。<br /></li><li>为了明确接口的返回，自定义统一的错误码，并封装了全局异常处理器，从而规范了异常返回、屏蔽了项目冗余的报错细节。<br /></li><li>自主编写 Dockerfile，并通过容器部署，提高部署上线效率。<br /></li><li>使用 Knife4j + Swagger自动生成后端接口文档，避免了人工编写维护文档的麻烦。<br /></li><li>前端使用 Vant UI 组件库，并封装了全局通用的 Layout组件，使主页、搜索页、管理页布局一致、并减少重复代码。</li></ol><h2 id="软件设计">软件设计</h2><h3 id="数据库设计">数据库设计</h3><p><strong>数据库名：</strong> ilou</p><table><thead><tr class="header"><th style="text-align: left;">表名</th><th style="text-align: left;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><a href="#emoji">emoji</a></td><td style="text-align: left;">表情</td></tr><tr class="even"><td style="text-align: left;"><a href="#tag">tag</a></td><td style="text-align: left;">标签</td></tr></tbody></table><p><strong>表名：</strong> <a id="tag">emoji</a></p><p><strong>说明：</strong> 表情</p><p><strong>数据列：</strong></p><table><thead><tr class="header"><th style="text-align: center;">序号</th><th style="text-align: left;">名称</th><th style="text-align: center;">数据类型</th><th style="text-align: center;">长度</th><th style="text-align: center;">小数位</th><th style="text-align: center;">允许空值</th><th style="text-align: center;">主键</th><th style="text-align: center;">默认值</th><th style="text-align: center;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: left;">id</td><td style="text-align: center;">int</td><td style="text-align: center;">10</td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">Y</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">2</td><td style="text-align: left;">url</td><td style="text-align: center;">varchar</td><td style="text-align: center;">4096</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">3</td><td style="text-align: left;">name</td><td style="text-align: center;">varchar</td><td style="text-align: center;">512</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">4</td><td style="text-align: left;">userId</td><td style="text-align: center;">int</td><td style="text-align: center;">10</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;">0</td><td style="text-align: center;">上传用户id</td></tr><tr class="odd"><td style="text-align: center;">5</td><td style="text-align: left;">tags</td><td style="text-align: center;">varchar</td><td style="text-align: center;">1024</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;"></td><td style="text-align: center;">标签数组json</td></tr><tr class="even"><td style="text-align: center;">6</td><td style="text-align: left;">reviewStatus</td><td style="text-align: center;">int</td><td style="text-align: center;">10</td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">N</td><td style="text-align: center;">0</td><td style="text-align: center;">0-待审核,1-通过,2-拒绝</td></tr><tr class="odd"><td style="text-align: center;">7</td><td style="text-align: left;">createTime</td><td style="text-align: center;">datetime</td><td style="text-align: center;">19</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;">CURRENT_TIMESTAMP</td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">8</td><td style="text-align: left;">isDelete</td><td style="text-align: center;">tinyint</td><td style="text-align: center;">3</td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">N</td><td style="text-align: center;">0</td><td style="text-align: center;">是否删除0-未删除1-已删除</td></tr><tr class="odd"><td style="text-align: center;">9</td><td style="text-align: left;">updateTime</td><td style="text-align: center;">datetime</td><td style="text-align: center;">19</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;">CURRENT_TIMESTAMP</td><td style="text-align: center;"></td></tr></tbody></table><p><strong>表名：</strong> <a id="tag">tag</a></p><p><strong>说明：</strong> 标签</p><p><strong>数据列：</strong></p><table style="width:100%;"><thead><tr class="header"><th style="text-align: center;">序号</th><th style="text-align: left;">名称</th><th style="text-align: center;">数据类型</th><th style="text-align: center;">长度</th><th style="text-align: center;">小数位</th><th style="text-align: center;">允许空值</th><th style="text-align: center;">主键</th><th style="text-align: center;">默认值</th><th style="text-align: center;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">1</td><td style="text-align: left;">id</td><td style="text-align: center;">int</td><td style="text-align: center;">10</td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">Y</td><td style="text-align: center;"></td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">2</td><td style="text-align: left;">name</td><td style="text-align: center;">varchar</td><td style="text-align: center;">512</td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">N</td><td style="text-align: center;"></td><td style="text-align: center;">标签名</td></tr><tr class="odd"><td style="text-align: center;">3</td><td style="text-align: left;">createTime</td><td style="text-align: center;">datetime</td><td style="text-align: center;">19</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;">CURRENT_TIMESTAMP</td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">4</td><td style="text-align: left;">isDelete</td><td style="text-align: center;">tinyint</td><td style="text-align: center;">3</td><td style="text-align: center;">0</td><td style="text-align: center;">N</td><td style="text-align: center;">N</td><td style="text-align: center;">0</td><td style="text-align: center;">是否删除0-未删除1-已删除</td></tr><tr class="odd"><td style="text-align: center;">5</td><td style="text-align: left;">updateTime</td><td style="text-align: center;">datetime</td><td style="text-align: center;">19</td><td style="text-align: center;">0</td><td style="text-align: center;">Y</td><td style="text-align: center;">N</td><td style="text-align: center;">CURRENT_TIMESTAMP</td><td style="text-align: center;"></td></tr></tbody></table><h3 id="后端接口设计">后端接口设计</h3><p><ahref="ilou-api.yinan.fun/api/doc.html">ilou-api.yinan.fun/api/doc.html</a></p><h3 id="通用返回对象">通用返回对象</h3><p>给对象补充一些信息，告诉前端这个请求在业务层面上是成功还是失败</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs java">SUCCESS(<span class="hljs-number">0</span>, <span class="hljs-string">&quot;ok&quot;</span>),<br>PARAMS_ERROR(<span class="hljs-number">40000</span>, <span class="hljs-string">&quot;请求参数错误&quot;</span>),<br>NOT_LOGIN_ERROR(<span class="hljs-number">40100</span>, <span class="hljs-string">&quot;未登录&quot;</span>),<br>NO_AUTH_ERROR(<span class="hljs-number">40101</span>, <span class="hljs-string">&quot;无权限&quot;</span>),<br>NOT_FOUND_ERROR(<span class="hljs-number">40400</span>, <span class="hljs-string">&quot;请求数据不存在&quot;</span>),<br>FORBIDDEN_ERROR(<span class="hljs-number">40300</span>, <span class="hljs-string">&quot;禁止访问&quot;</span>),<br>SYSTEM_ERROR(<span class="hljs-number">50000</span>, <span class="hljs-string">&quot;系统内部异常&quot;</span>),<br>OPERATION_ERROR(<span class="hljs-number">50001</span>, <span class="hljs-string">&quot;操作失败&quot;</span>),<br>FILE_UPLOAD_ERROR(<span class="hljs-number">50010</span>, <span class="hljs-string">&quot;文件上传错误&quot;</span>);<br></code></pre></td></tr></table></figure><h3 id="全局异常处理器">全局异常处理器</h3><ol type="1"><li><p>定义业务异常类</p><ol type="1"><li>相对于 java 的异常类，支持更多字段</li><li>自定义构造函数，更灵活 / 快捷的设置字段</li></ol></li><li><p>编写全局异常处理器（利用 SpringAOP，在调用方法前后进行额外的处理）</p></li></ol><h3 id="源码">源码</h3><p><ahref="https://github.com/wyn04/ilou">https://github.com/wyn04/ilou</a></p>]]></content>
    
    
    <categories>
      
      <category>项目</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JAVA</tag>
      
      <tag>SpringBoot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyInstaller python 打包工具使用</title>
    <link href="/PyInstaller-python-%E6%89%93%E5%8C%85%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8.html"/>
    <url>/PyInstaller-python-%E6%89%93%E5%8C%85%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8.html</url>
    
    <content type="html"><![CDATA[<h1 id="pyinstaller-python-打包工具使用">PyInstaller python打包工具使用</h1><h2 id="参数">参数</h2><table><thead><tr class="header"><th>参数名</th><th>描述</th><th>说明</th></tr></thead><tbody><tr class="odd"><td>-D</td><td>生成one-folder的程序（默认）</td><td>生成结果是一个目录，各种第三方依赖、资源和exe同时存储在该目录</td></tr><tr class="even"><td>-F</td><td>生成one-file的程序</td><td>生成结果是一个exe文件，所有的第三方依赖、资源和代码均被打包进该exe内</td></tr><tr class="odd"><td>–add-data</td><td>打包额外资源</td><td>用法：pyinstaller main.py--add-data=src;dest。windows以;分割，macOS/linux以:分割</td></tr><tr class="even"><td>-c</td><td>显示命令行窗口</td><td>与-w相反，默认含有此参数</td></tr><tr class="odd"><td>-w</td><td>不显示命令行窗口</td><td>编写GUI程序时使用此参数有用。</td></tr><tr class="even"><td>-i</td><td>为main.exe指定图标</td><td>用法：pyinstaller -i beauty.ico main.py</td></tr></tbody></table><p>具体参数看：https://blog.csdn.net/weixin_39000819/article/details/80942423</p><h2 id="打包图片等资源">打包图片等资源</h2><p>即使使用了<code>–add-data</code>参数也大概率还是会出现图片文件没有找到的情况。</p><p>因为当app运行的时候，会先<strong>把资源解压到一个系统的临时目录</strong>，包括打包进去的img文件等。但是，此时的代码并不能直接通过 “img/texture.png”这样的相对路径来得到文件,应该是因为本身程序就不在这里，所以自然引用不到。</p><p>这时需要用到 sys 的一个半私有的属性<code>sys._MEIPASS</code>，根据官方的说明，它是 pyinstaller运行时创建的<strong>临时目录的绝对路径</strong>。需要在程序运行时判断当前是不是有这个属性，如果有的话，使用生成临时目录下图片的绝对路径，如果没有的话，那就直接返回当前程序的绝对路径：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_resource_path</span>(<span class="hljs-params">relative_path</span>):<br>    <span class="hljs-comment"># 资源路径拼接当前运行时临时目录路径</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(sys, <span class="hljs-string">&#x27;_MEIPASS&#x27;</span>):<br>        <span class="hljs-keyword">return</span> os.path.join(sys._MEIPASS, relative_path)<br>    <span class="hljs-keyword">return</span> os.path.join(os.path.abspath(<span class="hljs-string">&quot;.&quot;</span>), relative_path)<br></code></pre></td></tr></table></figure><p>每次用到图片资源地址的时候都调用<code>get_resource_path</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">texture_filename = get_resource_path(<span class="hljs-string">&quot;img/texture.png&quot;</span>)<br>texture = cv2.imread(texture_filename)<br></code></pre></td></tr></table></figure><p>之后就可以使用下面的命令（windows要将<code>:</code>改成<code>;</code>）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">pyinstaller --add-data img:img -wF img2sketch.py</span><br></code></pre></td></tr></table></figure><p>在<code>disk</code>目录下生成可执行文件</p><figure><img src="https://image.yinan.fun/image-20230522162328233.png"alt="image-20230522162328233" /><figcaption aria-hidden="true">image-20230522162328233</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker常用命令大全</title>
    <link href="/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8.html"/>
    <url>/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8.html</url>
    
    <content type="html"><![CDATA[<h1 id="docker常用命令大全">Docker常用命令大全</h1><h2 id="帮助类启动命令">1、帮助类启动命令</h2><ul><li>启动docker： systemctl start docker</li><li>停止docker： systemctl stop docker</li><li>重启docker： systemctl restart docker</li><li>查看docker状态： systemctl status docker</li><li>开机启动： systemctl enable docker</li><li>查看docker概要信息： docker info</li><li>查看docker总体帮助文档： docker --help</li><li>查看docker命令帮助文档： docker 具体命令 --help</li></ul><h2 id="镜像命令">2、镜像命令</h2><h3 id="列出本地主机上的镜像">1、列出本地主机上的镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker images<br></code></pre></td></tr></table></figure><figure><img src="https://image.yinan.fun/7000-20230417102853412.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在这里插入图片描述</p><p>各个选项说明:</p><ul><li>REPOSITORY：表示镜像的仓库源</li><li>TAG：镜像的标签版本号</li><li>IMAGE ID：镜像ID</li><li>CREATED：镜像创建时间</li><li>SIZE：镜像大小</li></ul><blockquote><p>同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用REPOSITORY:TAG 来定义不同的镜像。</p><p>如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker将默认使用 ubuntu:latest 镜像</p></blockquote><p>OPTIONS说明：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">-a :列出本地所有的镜像（含历史映像层）<br><br>docker images -a<br><br><br><br>-q :只显示镜像ID。<br><br>docker images -q<br></code></pre></td></tr></table></figure><h3 id="搜索镜像">2、搜索镜像</h3><p>官方搜索网址：https://hub.docker.com/</p><p>命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker search [OPTIONS] 镜像名字<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker ~]# docker search mysql<br><br>NAME                             DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED<br><br>mysql                            MySQL is a widely used, open-source relation…   12378     [OK]       <br><br>mariadb                          MariaDB Server is a high performing open sou…   4762      [OK]       <br><br>mysql/mysql-server               Optimized MySQL Server Docker images. Create…   917                  [OK]<br><br>percona                          Percona Server is a fork of the MySQL relati…   572       [OK]       <br><br>phpmyadmin                       phpMyAdmin - A web interface for MySQL and M…   494       [OK]       <br><br>mysql/mysql-cluster              Experimental MySQL Cluster Docker images. Cr…   93                   <br><br>centos/mysql-57-centos7          MySQL 5.7 SQL database server                   92                   <br><br>bitnami/mysql                    Bitnami MySQL Docker Image                      68                   [OK]<br><br>ubuntu/mysql                     MySQL open source fast, stable, multi-thread…   29                   <br><br>circleci/mysql                   MySQL is a widely used, open-source relation…   25                   <br><br>mysql/mysql-router               MySQL Router provides transparent routing be…   23                   <br><br>centos/mysql-56-centos7          MySQL 5.6 SQL database server                   22                   <br><br>google/mysql                     MySQL server for Google Compute Engine          21                   [OK]<br><br>vmware/harbor-db                 Mysql container for Harbor                      10                   <br><br>mysqlboy/docker-mydumper         docker-mydumper containerizes MySQL logical …   3                    <br><br>mysqlboy/mydumper                mydumper for mysql logcial backups              3                    <br><br>bitnami/mysqld-exporter                                                          2                    <br><br>ibmcom/mysql-s390x               Docker image for mysql-s390x                    2                    <br><br>mysql/mysql-operator             MySQL Operator for Kubernetes                   0                    <br><br>ibmcom/tidb-ppc64le              TiDB is a distributed NewSQL database compat…   0                    <br><br>mysqlboy/elasticsearch                                                           0                    <br><br>mysqleatmydata/mysql-eatmydata                                                   0                    <br><br>cimg/mysql                                                                       0                    <br><br>mysql/ndb-operator               MySQL NDB Operator for Kubernetes               0                    <br><br>mirantis/mysql  <br></code></pre></td></tr></table></figure><figure><img src="https://image.yinan.fun/7000.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在这里插入图片描述</p><h4 id="options说明">OPTIONS说明：</h4><ul><li>--limit : 只列出N个镜像，默认25个</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker search --limit 5 redis<br></code></pre></td></tr></table></figure><h3 id="拉取下载镜像">3、拉取（下载）镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull 某个XXX镜像名字<br></code></pre></td></tr></table></figure><ul><li>docker pull 某个XXX镜像名字:TAG</li><li>docker pull 镜像名字</li></ul><blockquote><p>没有TAG就是最新版</p><p>等价于：docker pull 镜像名字:latest</p></blockquote><h3 id="查看镜像">4、查看镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker system df 查看镜像/容器/数据卷所占的空间<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker ~]# docker system df<br><br>TYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE<br><br>Images          1         1         13.26kB   0B (0%)<br><br>Containers      1         0         0B        0B<br><br>Local Volumes   0         0         0B        0B<br><br>Build Cache     0         0         0B        0B<br><br>[root@docker ~]# <br></code></pre></td></tr></table></figure><h3 id="删除镜像">5、删除镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker rmi 某个XXX镜像名字ID<br></code></pre></td></tr></table></figure><h4 id="删除单个">1、删除单个</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker rmi  -f 镜像ID<br></code></pre></td></tr></table></figure><h4 id="删除多个">2、删除多个</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker rmi -f 镜像名1:TAG 镜像名2:TAG <br></code></pre></td></tr></table></figure><h3 id="删除全部">3、删除全部</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker rmi -f $(docker images -qa)<br></code></pre></td></tr></table></figure><h3id="面试题谈谈docker虚悬镜像是什么">6、面试题：谈谈docker虚悬镜像是什么？</h3><blockquote><p>仓库名、标签都是<none>的镜像，俗称虚悬镜像dangling image</p></blockquote><figure><img src="https://image.yinan.fun/7000-20230417102853439.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在这里插入图片描述</p><h2 id="容器命令">3、容器命令</h2><blockquote><p>有镜像才能创建容器</p></blockquote><figure><img src="https://image.yinan.fun/7000-20230417102853482.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>本次用centos进行演示</p><h3 id="新建启动命令">1、新建+启动命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run [OPTIONS] IMAGE [COMMAND] [ARG...]<br></code></pre></td></tr></table></figure><p>OPTIONS说明（常用）：有些是一个减号，有些是两个减号</p><p>--name="容器新名字" 为容器指定一个名称；</p><p>-d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)；</p><p>-i：以交互模式运行容器，通常与 -t 同时使用；</p><p>-t：为容器重新分配一个伪输入终端，通常与 -i 同时使用；</p><p>也即启动交互式容器(前台有伪终端，等待交互)；</p><p>-P: 随机端口映射，大写P</p><p>-p: 指定端口映射，小写p</p><figure><img src="https://image.yinan.fun/7000-20230417102853467.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在这里插入图片描述</p><figure><img src="https://image.yinan.fun/7000-20230417102853397.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在这里插入图片描述</p><blockquote><p>#使用镜像centos:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -it centos /bin/bash <br></code></pre></td></tr></table></figure><p>参数说明：</p><p>-i: 交互式操作。</p><p>-t: 终端。</p><p>centos : centos 镜像。</p><p>/bin/bash：放在镜像名后的是命令，这里我们希望有个交互式Shell，因此用的是 /bin/bash。</p><p>要退出终端，直接输入 exit:</p><h3 id="列出当前所有正在运行的容器">2、列出当前所有正在运行的容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker ps [OPTIONS]<br></code></pre></td></tr></table></figure><blockquote><p>OPTIONS说明（常用）：</p><p>-a :列出当前所有正在运行的容器+历史上运行过的</p><p>-l :显示最近创建的容器。</p><p>-n：显示最近n个创建的容器。</p><p>-q :静默模式，只显示容器编号。</p></blockquote><h3 id="退出容器">3、退出容器</h3><h4 id="exec退出">1、<code>exec</code>退出</h4><blockquote><p>run进去容器，exit退出，容器停止</p></blockquote><h4 id="ctrlpq">2、<code>ctrl+p+q</code></h4><blockquote><p>run进去容器，ctrl+p+q退出，容器不停止</p></blockquote><h3 id="启动已停止运行的容器">4、启动已停止运行的容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker start 容器ID或者容器名<br></code></pre></td></tr></table></figure><h3 id="重启容器">5、重启容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker restart 容器ID或者容器名<br></code></pre></td></tr></table></figure><h3 id="停止容器">6、停止容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker stop 容器ID或者容器名<br></code></pre></td></tr></table></figure><h3 id="强制停止容器">7、强制停止容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker kill 容器ID或容器名<br></code></pre></td></tr></table></figure><h3 id="删除已停止的容器">8、删除已停止的容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker rm 容器ID<br></code></pre></td></tr></table></figure><h4 id="一次性删除多个容器实例">一次性删除多个容器实例</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker rm -f $(docker ps -a -q)<br><br><br><br>docker ps -a -q | xargs docker rm<br></code></pre></td></tr></table></figure><p><code>docker ps -a -q</code>：查询进行的容器ID</p><h2 id="实操">4、实操</h2><h3 id="启动守护式容器后台服务器">1、启动守护式容器(后台服务器)</h3><blockquote><p>在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 -d指定容器的后台运行模式。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -d 容器名<br></code></pre></td></tr></table></figure><p>#使用镜像centos:latest以后台模式启动一个容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -d centos<br></code></pre></td></tr></table></figure><p>问题：然后docker ps -a 进行查看,会发现容器已经退出很重要的要说明的一点:Docker容器后台运行,就必须有一个前台进程.容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的。</p><p>这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如servicenginxstart但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用,这样的容器后台启动后,会立即自杀因为他觉得他没事可做了.所以，最佳的解决方案是,将你要运行的程序以前台进程的形式运行，常见就是命令行模式，表示我还有交互操作，别中断，O(∩_∩)O哈哈~</p><h4 id="案例演示">案例演示</h4><blockquote><p><ahref="https://cloud.tencent.com/product/crs?from=20065&amp;from_column=20065">redis</a>前后台启动演示case（下载一个Redis6.0.8镜像演示）</p></blockquote><ul><li>前台交互式启动</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -it redis<br></code></pre></td></tr></table></figure><ul><li>后台守护式启动</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -d redis<br></code></pre></td></tr></table></figure><p>PS：启动容器后，先查看容器是否启动正常，往往伴随着<code>docker ps</code></p><h3 id="查看容器日志">2、查看容器日志</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker logs 容器ID<br></code></pre></td></tr></table></figure><p>PS：容器ID可省略为前三位</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker ~]# docker ps <br><br>CONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS      NAMES<br><br>94b526c73db2   redis     &quot;docker-entrypoint.s…&quot;   4 seconds ago   Up 3 seconds   6379/tcp   admiring\_poincare<br><br>[root@docker ~]# docker logs 94b<br><br>1:C 10 Apr 2022 09:50:03.396 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo<br><br>1:C 10 Apr 2022 09:50:03.396 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=1, just started<br><br>1:C 10 Apr 2022 09:50:03.396 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf<br><br>1:M 10 Apr 2022 09:50:03.397 \* monotonic clock: POSIX clock\_gettime<br><br>1:M 10 Apr 2022 09:50:03.398 \* Running mode=standalone, port=6379.<br><br>1:M 10 Apr 2022 09:50:03.399 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.<br><br>1:M 10 Apr 2022 09:50:03.399 # Server initialized<br><br>1:M 10 Apr 2022 09:50:03.399 # WARNING overcommit\_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &#x27;vm.overcommit\_memory = 1&#x27; to /etc/sysctl.conf and then reboot or run the command &#x27;sysctl vm.overcommit\_memory=1&#x27; for this to take effect.<br><br>1:M 10 Apr 2022 09:50:03.399 \* Ready to accept connections<br><br>[root@docker ~]# <br></code></pre></td></tr></table></figure><h3 id="查看容器内运行的进程">3、查看容器内运行的进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker top 容器ID<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker ~]# docker top 94b<br><br>UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD<br><br>polkitd             35360               35341               0                   17:50               ?                   00:00:00            redis-server \*:6379<br></code></pre></td></tr></table></figure><h3 id="查看容器内部细节">4、查看容器内部细节</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker inspect 容器ID<br></code></pre></td></tr></table></figure><h3 id="进入容器内部">5、进入容器内部</h3><h4 id="使用exec进入容器">1、使用<code>exec</code>进入容器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec -it 容器ID bashShell<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker ~]# docker exec -it 94b /bin/bash<br><br>root@94b526c73db2:/data# redis-c<br><br>redis-check-aof  redis-check-rdb  redis-cli        <br><br>root@94b526c73db2:/data# redis-cli <br><br>127.0.0.1:6379&gt; ping<br><br>PONG<br><br>127.0.0.1:6379&gt; set k1 v1<br><br>OK<br><br>127.0.0.1:6379&gt; get k1<br><br>&quot;v1&quot;<br><br>127.0.0.1:6379&gt; exit<br></code></pre></td></tr></table></figure><p>exec帮助命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker ~]# docker exec --help<br><br><br><br>Usage:  docker exec [OPTIONS] CONTAINER COMMAND [ARG...]<br><br><br><br>Run a command in a running containe<br><br><br><br>Options:<br><br>  -d, --detach               Detached mode: run command in the background<br><br>      --detach-keys string   Override the key sequence for detaching a containe<br><br>  -e, --env list             Set environment variables<br><br>      --env-file list        Read in a file of environment variables<br><br>  -i, --interactive          Keep STDIN open even if not attached<br><br>      --privileged           Give extended privileges to the command<br><br>  -t, --tty                  Allocate a pseudo-TTY<br><br>  -u, --user string          Username or UID (format: &lt;name|uid&gt;[:&lt;group|gid&gt;])<br><br>  -w, --workdir string       Working directory inside the containe<br><br>[root@docker ~]# <br></code></pre></td></tr></table></figure><h4 id="使用attach进入容器">2、使用<code>attach</code>进入容器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker attach 容器ID<br></code></pre></td></tr></table></figure><h4 id="两者的区别">3、两者的区别</h4><ul><li>attach直接进入容器启动命令的终端，不会启动新的进程，用exit退出，会导致容器的停止。</li><li>exec是在容器中打开新的终端，并且可以启动新的进程，用exit退出，不会导致容器的停止。</li></ul><p>PS：推荐大家使用<code>docker exec</code>命令，因为退出容器终端，不会导致容器的停止。</p><h2 id="从容器内拷贝文件到主机上">5、从容器内拷贝文件到主机上</h2><blockquote><p>把文件从容器内部复制到主机上</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker cp  容器ID:容器内路径 目的主机路径<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker cp 3065f084c80d:a.txt a.txt<br></code></pre></td></tr></table></figure><h2 id="导入和导出容器">6、导入和导出容器</h2><h3 id="导出容器">1、导出容器</h3><blockquote><p>export 导出容器的内容留作为一个tar归档文件对应import命令</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker export 容器ID &gt; 文件名.ta<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker /]# docker export 3065f084c80d &gt; centos.tar.gz<br><br>[root@docker /]# ll<br><br>总用量 233004<br><br>-rw-r--r--.   1 root root         0 4月  10 18:16 a.txt<br><br>lrwxrwxrwx.   1 root root         7 4月   9 15:59 bin -&gt; usr/bin<br><br>dr-xr-xr-x.   5 root root      4096 4月   9 16:06 boot<br><br>-rw-r--r--.   1 root root 238572032 4月  10 19:04 centos.tar.gz<br><br>drwxr-xr-x.  20 root root      3220 4月   9 16:29 dev<br><br>drwxr-xr-x.  86 root root      8192 4月  10 11:03 etc<br><br>drwxr-xr-x.   3 root root        22 4月   9 16:05 home<br><br>lrwxrwxrwx.   1 root root         7 4月   9 15:59 lib -&gt; usr/lib<br><br>lrwxrwxrwx.   1 root root         9 4月   9 15:59 lib64 -&gt; usr/lib64<br><br>drwxr-xr-x.   2 root root         6 4月  11 2018 media<br><br>drwxr-xr-x.   2 root root         6 4月  11 2018 mnt<br><br>drwxr-xr-x.   4 root root        34 4月   9 20:38 opt<br><br>dr-xr-xr-x. 124 root root         0 4月   9 16:29 proc<br><br>dr-xr-x---.   5 root root       176 4月   9 20:36 root<br><br>drwxr-xr-x.  31 root root       940 4月   9 21:01 run<br><br>lrwxrwxrwx.   1 root root         8 4月   9 15:59 sbin -&gt; usr/sbin<br><br>drwxr-xr-x.   2 root root         6 4月  11 2018 srv<br><br>dr-xr-xr-x.  13 root root         0 4月   9 16:29 sys<br><br>drwxrwxrwt.  13 root root      4096 4月  10 18:16 tmp<br><br>drwxr-xr-x.  13 root root       155 4月   9 15:59 us<br></code></pre></td></tr></table></figure><h3 id="导入容器">2、导入容器</h3><blockquote><p>import 从tar包中的内容创建一个新的文件系统再导入为镜像对应export</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat 文件名.tar | docker import - 镜像用户/镜像名:镜像版本号<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@docker /]# cat centos.tar.gz | docker import - xiaobear/centos:8.8<br><br>sha256:12e7a58fc36a755fa63a82f78364291de7271c3ade90f2f5581cc50bc2416cb2<br><br>[root@docker /]# docker images<br><br>REPOSITORY        TAG       IMAGE ID       CREATED              SIZE<br><br>xiaobear/centos   8.8       12e7a58fc36a   About a minute ago   231MB<br><br>redis             latest    7614ae9453d1   3 months ago         113MB<br><br>hello-world       latest    feb5d9fea6a5   6 months ago         13.3kB<br><br>centos            latest    5d0da3dc9764   6 months ago         231MB<br><br>[root@docker /]# <br></code></pre></td></tr></table></figure><h2 id="常用命令汇总">7、常用命令汇总</h2><figure><img src="https://image.yinan.fun/7000-20230417102853593.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><table><thead><tr class="header"><th>命令</th><th>英文</th><th>中文</th></tr></thead><tbody><tr class="odd"><td>attach</td><td>Attach to a running container</td><td>当前 shell 下 attach 连接指定运行镜像</td></tr><tr class="even"><td>build</td><td>Build an image from a Dockerfile</td><td>通过 Dockerfile 定制镜像</td></tr><tr class="odd"><td>commit</td><td>Create a new image from a container changes</td><td>提交当前容器为新的镜像</td></tr><tr class="even"><td>cp</td><td>Copy files/folders from the containers filesystem to the hostpath</td><td>容器中拷贝指定文件或者目录到宿主机中</td></tr><tr class="odd"><td>create</td><td>Create a new container</td><td>创建一个新的容器，同 run，但不启动容器</td></tr><tr class="even"><td>diff</td><td>Inspect changes on a container's filesystem</td><td>查看 docker 容器变化</td></tr><tr class="odd"><td>events</td><td>Get real time events from the server</td><td>从 docker 服务获取容器实时事件</td></tr><tr class="even"><td>exec</td><td>Run a command in an existing container</td><td>在已存在的容器上运行命令</td></tr><tr class="odd"><td>export</td><td>Stream the contents of a container as a tar archive</td><td>导出容器的内容流作为一个 tar 归档文件对应 import</td></tr><tr class="even"><td>images</td><td>List images</td><td>列出系统当前镜像</td></tr><tr class="odd"><td>import</td><td>Create a new filesystem image from the contents of a tarball</td><td>从tar包中的内容创建一个新的文件系统映像对应export</td></tr><tr class="even"><td>info</td><td>Display system-wide information</td><td>显示系统相关信息</td></tr><tr class="odd"><td>inspect</td><td>Return low-level information on a container</td><td>查看容器详细信息</td></tr><tr class="even"><td>kill</td><td>Kill a running container</td><td>kill 指定 docker 容器</td></tr><tr class="odd"><td>load</td><td>Load an image from a tar archive</td><td>从一个 tar 包中加载一个镜像对应 save</td></tr><tr class="even"><td>login</td><td>Register or Login to the docker registry server</td><td>注册或者登陆一个 docker 源服务器</td></tr><tr class="odd"><td>logout</td><td>Log out from a Docker registry server</td><td>从当前 Docker registry 退出</td></tr><tr class="even"><td>logs</td><td>Fetch the logs of a container</td><td>输出当前容器日志信息</td></tr><tr class="odd"><td>port</td><td>Lookup the public-facing port which is NAT-ed to PRIVATE_PORT</td><td>查看映射端口对应的容器内部源端口</td></tr><tr class="even"><td>pause</td><td>Pause all processes within a container</td><td>暂停容器</td></tr><tr class="odd"><td>ps</td><td>List containers</td><td>列出容器列表</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2. 日志系统：一条SQL更新语句是如何执行的？</title>
    <link href="/2.%20%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%EF%BC%9A%E4%B8%80%E6%9D%A1SQL%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F.html"/>
    <url>/2.%20%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%EF%BC%9A%E4%B8%80%E6%9D%A1SQL%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F.html</url>
    
    <content type="html"><![CDATA[<h2 id="日志系统一条sql更新语句是如何执行的">2.日志系统：一条SQL更新语句是如何执行的？</h2><p>与查询流程不一样的是，更新流程还涉及两个重要的日志模块：<strong>redolog（重做日志）和 binlog（归档日志）</strong>。</p><h3 id="redo-log">2.1 redo log</h3><p>如果<strong>每一次的更新操作都需要写进磁盘</strong>，然后磁盘也要找到对应的那条记录，然后再更新，整个过程<strong>IO成本、查找成本都很高</strong>。</p><p><strong>WAL</strong>的全称是Write-AheadLogging，它的关键点就是<strong>先写日志，再写磁盘</strong>。</p><p>当有一条记录需要<strong>更新</strong>的时候，InnoDB引擎就会<strong>先把记录写到redolog</strong>里面，并<strong>更新内存</strong>，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录<strong>更新到磁盘里面</strong>，而这个更新往往是在系统比较<strong>空闲</strong>的时候做。</p><p>InnoDB的redolog是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示</p><figure><img src="https://image.yinan.fun/image-20230508202933697.png"alt="image-20230508202933697" /><figcaption aria-hidden="true">image-20230508202933697</figcaption></figure><p><strong>writepos</strong>是<strong>当前记录的位置</strong>，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。<strong>checkpoint</strong>是<strong>当前要擦除的位置</strong>，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。</p><p>write pos和checkpoint之间的是redolog上还空着的部分，可以用来记录新的操作。如果writepos追上checkpoint，表示redolog这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。</p><p>有了redolog，InnoDB就可以保证<strong>即使数据库发生异常重启，之前提交的记录都不会丢失</strong>，这个能力称为<strong>crash-safe</strong>。</p><h3 id="binlog">2.2 binlog</h3><p>redolog是InnoDB引擎特有的日志，而<strong>Server层</strong>也有自己的日志，称为binlog（归档日志）。</p><p>这两种日志有以下三点不同。</p><ol type="1"><li><p>redolog是<strong>InnoDB引擎特有</strong>的；binlog是<strong>MySQL的Server层实现</strong>的，所有引擎都可以使用。</p></li><li><p>redolog是<strong>物理日志</strong>，记录的是“在某个数据页上<strong>做了什么修改</strong>”；binlog是<strong>逻辑日志</strong>，记录的是这个<strong>语句的原始逻辑</strong>，比如“给ID=2这一行的c字段加1”。</p></li><li><p>redolog是<strong>循环写</strong>的，空间固定会用完；binlog是可以<strong>追加写入</strong>的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并<strong>不会覆盖</strong>以前的日志。</p></li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; create table T(ID int primary key, c int);<br>mysql&gt; update T set c=c+1 where ID=2;<br></code></pre></td></tr></table></figure><p>执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。</p><ol type="1"><li><p>执行器先<strong>找引擎取ID=2这一行</strong>。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。</p></li><li><p>执行器拿到引擎给的行数据，<strong>把这个值加上1</strong>，比如原来是N，现在就是N+1，得到新的一行数据，再<strong>调用引擎接口写入这行新数据</strong>。</p></li><li><p><strong>引擎</strong>将这行新数据<strong>更新到内存</strong>中，同时将这个更新操作<strong>记录到redolog</strong>里面，此时redolog处于<strong>prepare状态</strong>。然后告知执行器执行完成了，随时可以提交事务。</p></li><li><p><strong>执行器</strong>生成这个操作的binlog，并把<strong>binlog写入磁盘</strong>。</p></li><li><p>执行器调用引擎的提交事务接口，引擎把刚刚写入的redolog改成<strong>提交（commit）状态</strong>，更新完成。</p></li></ol><p>这里我给出这个update语句的执行流程图，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的</p><figure><img src="https://image.yinan.fun/image-20230508202945483.png"alt="image-20230508202945483" /><figcaption aria-hidden="true">image-20230508202945483</figcaption></figure><p>将redolog的写入拆成了两个步骤：prepare和commit，这就是"两阶段提交"。</p><h3 id="两阶段提交">2.3 两阶段提交</h3><p>为什么必须有“两阶段提交”呢？这是为了<strong>让两份日志之间的逻辑一致</strong>。</p><p>当需要数据库<strong>恢复到指定的某一秒</strong>时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：</p><ul><li><p>首先，找到<strong>最近的一次全量备份</strong>，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库；</p></li><li><p>然后，从备份的时间点开始，<strong>将备份的binlog依次取出来</strong>，重放到中午误删表之前的那个时刻。</p></li></ul><p>由于redolog和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redolog再写binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。</p><p>仍然用前面的update语句来做例子。假设当前ID=2的行，字段c的值是0，再假设执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash，会出现什么情况呢？</p><ol type="1"><li><strong>先写先 redo log后写后 binlog</strong>。假设在redolog写完，binlog还没有写完的时候，MySQL进程异常重启。由于我们前面说过的，redolog写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同。</li><li><strong>先写先 binlog后写后 redolog</strong>。如果在binlog写完之后crash，由于redolog还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是1，与原库的值不同。</li></ol><p>可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。</p><p>简单说，redolog和binlog都可以用于表示事务的提交状态，而两阶段提交就是<strong>让这两个状态保持逻辑上的一致</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程</tag>
      
      <tag>数据库</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1. 基础架构：一条SQL查询语句是如何执行的？</title>
    <link href="/1.%20%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%EF%BC%9A%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F.html"/>
    <url>/1.%20%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%EF%BC%9A%E4%B8%80%E6%9D%A1SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%EF%BC%9F.html</url>
    
    <content type="html"><![CDATA[<h2 id="基础架构一条sql查询语句是如何执行的">1.基础架构：一条SQL查询语句是如何执行的？</h2><p>下面是<strong>MySQL的基本架构</strong>示意图，从中你可以清楚地看到SQL语句在MySQL的各个功能模块中的执行过程。</p><figure><imgsrc="https://image.yinan.fun/f6b1fbd6cbe8b2592a8d535221e7b51d-20230504093007977.png"alt="image-20230308152659467" /><figcaption aria-hidden="true">image-20230308152659467</figcaption></figure><p>MySQL可以分为<strong>Server层</strong>和<strong>存储引擎层</strong>两部分。</p><ol type="1"><li><p>Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数<strong>核心服务功能</strong>，以及所有的<strong>内置函数</strong>（如日期、时间、数学和加密函数等），所有<strong>跨存储引擎的功能</strong>都在这一层实现，比如存储过程、触发器、视图等。</p></li><li><p>存储引擎层负责<strong>数据的存储和提取</strong>。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL5.5.5版本开始成为了默认存储引擎。也就是说，你执行createtable建表的时候，如果不指定引擎类型，<strong>默认使用的就是InnoDB</strong>。不过，你也可以通过指定存储引擎的类型来选择别的引擎，比如在<code>create table</code>语句中使用<code>engine=memory</code>,来指定使用内存引擎创建表。不同存储引擎的表数据存取方式不同，支持的功能也不同。</p></li></ol><h3 id="连接器">1.1 连接器</h3><p>连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mysql -h$ip -P$port -u$user -p<br></code></pre></td></tr></table></figure><p>连接命令中的mysql是<strong>客户端工具</strong>，用来跟服务端建立连接。在完成经典的TCP握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。</p><ul><li><p>如果用户名或密码不对，你就会收到一个<strong>"Access denied foruser"的错误</strong>，然后客户端程序结束执行。</p></li><li><p>如果用户名密码认证通过，连接器会到<strong>权限表里面查出你拥有的权限</strong>。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有<strong>再新建的连接才会使用新的权限设置</strong>。</p></li></ul><p>客户端如果太<strong>长时间没动静</strong>，连接器就会<strong>自动将它断开</strong>。这个时间是由参数<code>wait_timeout</code>控制的，默认值是8小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：<code>Lost connection to MySQL server during query</code>。这时候如果你要继续，就需要重连，然后再执行请求了。</p><ul><li><strong>长连接</strong>是指连接成功后，如果客户端<strong>持续有请求</strong>，则一直使用<strong>同一个连接</strong>。</li><li><strong>短连接</strong>则是指每次执行完很少的<strong>几次查询就断开连接</strong>，下次查询再重新建立一个。</li></ul><p><strong>建立连接的过程通常是比较复杂</strong>的，所以建议在使用中要尽量减少建立连接的动作，也就是<strong>尽量使用长连接</strong>。</p><p>但是全部使用长连接后，MySQL占用内存涨得特别快，这是因为MySQL在执行过程中<strong>临时使用的内存是管理在连接对象里面的</strong>。这些资源会在<strong>连接断开的时候才释放</strong>。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。</p><ol type="1"><li><p><strong>定期断开长连接</strong>。使用一段时间，或者程序里面判断<strong>执行过一个占用内存的大查询后</strong>，断开连接，之后要查询再重连。</p></li><li><p>如果你用的是MySQL5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行<code>mysql_reset_connection</code>来<strong>重新初始化连接资源</strong>。这个过程<strong>不需要重连</strong>和重新做权限验证，但是会<strong>将连接恢复到刚刚创建完时的状态</strong>。</p></li></ol><h3 id="查询缓存">1.2 查询缓存</h3><p>MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。<strong>之前执行过的语句及其结果</strong>可能会以<strong>key-value对</strong>的形式，被直接缓存在内存中。<strong>key是查询的语句，value是查询的结果。</strong>如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。</p><p>如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个<strong>效率会很高</strong>。</p><p>但是大多数情况下<strong>建议不要使用查询缓存</strong>：查询缓存的<strong>失效非常频繁</strong>，只要有对一个<strong>表的更新</strong>，这个表上所有的<strong>查询缓存都会被清空</strong>。对于<strong>更新压力大的数据库</strong>来说，查询缓存的命中率会非常低。除非你的业务就是有一张<strong>静态表</strong>，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。</p><p>在MySQL也提供了这种“按需使用”的方式。你可以将参数<code>query_cache_type</code>设置成<code>DEMAND</code>，这样对于<strong>默认的SQL语句都不使用查询缓存</strong>。而对于你<strong>确定要使用查询缓存的语句</strong>，可以用<code>SQL_CACHE</code>显式指定，像下面这个语句一样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; select SQL_CACHE * from T where ID=10；<br></code></pre></td></tr></table></figure><p>MySQL8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。</p><h3 id="分析器">1.3 分析器</h3><p>首先，MySQL需要<strong>知道你要做什么</strong>，因此需要对SQL语句做解析。</p><ul><li><p>分析器先会做<strong>“词法分析”</strong>。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的<strong>字符串分别是什么</strong>，代表什么。MySQL从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。</p></li><li><p>做完了这些识别以后，就要做<strong>“语法分析”</strong>。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句<strong>是否满足MySQL语法</strong>。如果你的语句不对，就会收到“You have an error in your SQLsyntax”的错误提醒，比如下面这个 语句select少打了开头的字母“s”。</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; elect * from t where ID=1; <br>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#x27;elect * from t where ID=1&#x27; at line 1 <br></code></pre></td></tr></table></figure><p>一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“usenear”的内容。</p><h3 id="优化器">1.4 优化器</h3><p>优化器是在表里面有<strong>多个索引</strong>的时候，决定使用哪个索引；或者在一个语句有<strong>多表关联</strong>（join）的时候，决定各个表的连接顺序。</p><p>比如你执行下面这样的语句，这个语句是执行两个表的join：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;<br></code></pre></td></tr></table></figure><ul><li><p>既可以先从表t1里面取出c=10的记录的ID值，再根据ID值关联到表t2，再判断t2里面d的值是否等于20。</p></li><li><p>也可以先从表t2里面取出d=20的记录的ID值，再根据ID值关联到t1，再判断t1里面c的值是否等于10。</p></li></ul><p>这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。</p><h3 id="执行器">1.5 执行器</h3><p>MySQL通过<strong>分析器</strong>知道了你要<strong>做什么</strong>，通过<strong>优化器</strong>知道了该<strong>怎么做</strong>，于是就进入了执行器阶段，开始执行语句。</p><p>要先判断一下你对这个表T有没有<strong>执行查询的权限</strong>，如果没有，就会返回没有权限的错误，如下所示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; select * from T where ID=10; <br>ERROR 1142 (42000): SELECT command denied to user &#x27;b&#x27;@&#x27;localhost&#x27; for table &#x27;T&#x27;<br></code></pre></td></tr></table></figure><p>如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去<strong>使用这个引擎提供的接口</strong>。</p><p>比如我们这个例子中的表T中，ID字段<strong>没有索引</strong>，那么执行器的执行流程是这样的：</p><ol type="1"><li><p>调用InnoDB引擎接口取这个表的<strong>第一行</strong>，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；</p></li><li><p>调用引擎接口取<strong>“下一行”</strong>，重复相同的判断逻辑，直到取到这个表的最后一行。</p></li><li><p>执行器将上述<strong>遍历</strong>过程中所有满足条件的行组成的记录集作为结果集返回给客户端。</p></li></ol><p>至此，这个语句就执行完成了。</p><p>对于<strong>有索引</strong>的表，执行的逻辑也差不多。第一次调用的是<strong>“取满足条件的第一行”</strong>这个接口，之后循环取<strong>“满足条件的下一行”</strong>这个接口，这些接口都是引擎中已经定义好的。</p><p>你会在数据库的慢查询日志中看到一个<code>rows_examined</code>的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编程</tag>
      
      <tag>数据库</tag>
      
      <tag>MySQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
