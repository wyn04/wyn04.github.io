

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta name="baidu-site-verification" content="codeva-pajW5JmcAJ" />
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://image.yinan.fun/icon.jpg">
  <link rel="icon" href="https://image.yinan.fun/icon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="yinan">
  <meta name="keywords" content="">
  
    <meta name="description" content="了解Llama 官网：https:&#x2F;&#x2F;ai.meta.com&#x2F;llama&#x2F; 文章：https:&#x2F;&#x2F;ai.meta.com&#x2F;research&#x2F;publications&#x2F;llama-2-open-foundation-and-fine-tuned-chat-models&#x2F; 介绍 This release includes model weights and starting code for pret">
<meta property="og:type" content="article">
<meta property="og:title" content="了解Llama">
<meta property="og:url" content="https://yinan.fun/%E4%BA%86%E8%A7%A3Llama">
<meta property="og:site_name" content="yinan的博客">
<meta property="og:description" content="了解Llama 官网：https:&#x2F;&#x2F;ai.meta.com&#x2F;llama&#x2F; 文章：https:&#x2F;&#x2F;ai.meta.com&#x2F;research&#x2F;publications&#x2F;llama-2-open-foundation-and-fine-tuned-chat-models&#x2F; 介绍 This release includes model weights and starting code for pret">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://image.yinan.fun/image-20230723111842222.png">
<meta property="og:image" content="https://image.yinan.fun/image-20230721150225425.png">
<meta property="og:image" content="https://image.yinan.fun/image-20230723112453383.png">
<meta property="og:image" content="https://image.yinan.fun/image-20230723115454030.png">
<meta property="og:image" content="https://image.yinan.fun/image-20230723120200445.png">
<meta property="og:image" content="https://image.yinan.fun/image-20230723120239606.png">
<meta property="article:published_time" content="2023-07-23T04:32:17.000Z">
<meta property="article:modified_time" content="2023-09-14T01:18:36.262Z">
<meta property="article:author" content="yinan">
<meta property="article:tag" content="语言模型">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://image.yinan.fun/image-20230723111842222.png">
  
  
  
  <title>了解Llama - yinan的博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/iconfont_run/iconfont.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"yinan.fun","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"uYqB58EtGUKNooZzFSr2jweW-MdYXbMMI","app_key":"XGFOdYaumLCAdUreSIE8jhWS","server_url":null,"path":"window.location.pathname","ignore_local":true}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="yinan的博客" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Yinan&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" target="_blank" rel="noopener" href="https://run.yinan.fun/">
                <i class="iconfont icon-paobu"></i>
                <span>跑步</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner_img.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="了解Llama"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-07-23 12:32" pubdate>
          2023年7月23日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          62 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">了解Llama</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="了解llama">了解Llama</h1>
<p>官网：<a
target="_blank" rel="noopener" href="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a></p>
<p>文章：<a
target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/</a></p>
<h2 id="介绍">介绍</h2>
<p>This release includes model weights and starting code for pretrained
and fine-tuned Llama language models — ranging from 7B to 70B
parameters(7B, 13B, 70B).</p>
<p>Llama 2 pretrained models are trained on 2 trillion tokens, and have
double the context length than Llama 1. Its fine-tuned models have been
trained on over 1 million human annotations.</p>
<p><strong>Auto-regressive transformers</strong> are
<strong>pretrained</strong> on an extensive corpus of self-supervised
data, followed by <strong>alignment</strong> with human preferences via
techniques such as Reinforcement Learning with Human Feedback
(RLHF).</p>
<blockquote>
<p>Auto-regression is a time series model that <strong>uses observations
from previous time steps as input</strong> to a regression equation to
predict the value at the next time step</p>
</blockquote>
<figure>
<img src="https://image.yinan.fun/image-20230723111842222.png" srcset="/img/loading.gif" lazyload
alt="AT VS. NAT" />
<figcaption aria-hidden="true">AT VS. NAT</figcaption>
</figure>
<p>Llama2模型训练包括：预训练，有监督微调，RLHF。</p>
<figure>
<img src="https://image.yinan.fun/image-20230721150225425.png" srcset="/img/loading.gif" lazyload
alt="Training of Llama 2-Chat" />
<figcaption aria-hidden="true">Training of Llama 2-Chat</figcaption>
</figure>
<p>The training methodology is simple, but high computational
requirements.</p>
<h2 id="训练">训练</h2>
<h3 id="预训练数据">预训练数据</h3>
<ul>
<li><p>训练语料库包括来自公开来源的新数据组合，其中不包括来自 Meta
产品或服务的数据。</p></li>
<li><p>剔除某些已知包含大量个人隐私信息的网站的数据。</p></li>
<li><p>在 2
万亿个tokem的数据上进行了训练，很好地权衡性能与成本。</p></li>
<li><p>对最真实的数据源进行上采样，以增加知识和减少错误。</p></li>
</ul>
<h3 id="训练细节">训练细节</h3>
<p>We adopt most of the <strong>pretraining setting and model
architecture from Llama 1</strong>. We use the <strong>standard
transformer architecture</strong> (Vaswani et al., 2017), apply
pre-normalization using <strong>RMSNorm</strong> (Zhang and Sennrich,
2019), use the <strong>SwiGLU</strong> activation function (Shazeer,
2020), and rotary positional embeddings (RoPE, Su et al. 2022). The
primary architectural differences from Llama 1 include increased
<strong>context length</strong> and <strong>grouped-query
attention</strong> (GQA).</p>
<blockquote>
<p>LayerNorm是对特征张量按照某一维度或某几个维度进行0均值，1方差的归一化。RMSNorm是对LayerNorm的一个改进，没有做re-center操作（移除了其中的均值项）。RMSNorm
也是一种标准化方法，但与 LayerNorm
不同，它不是使用整个样本的均值和方差，而是使用平方根的均值来归一化，这样做可以降低噪声的影响。</p>
</blockquote>
<blockquote>
<p>旋转式位置编码（RoPE）最早是一种能够将相对位置信息依赖集成到self-attention中并提升transformer架构性能的位置编码方式。</p>
</blockquote>
<blockquote>
<p>GQA（Grouped-Query
Attention）是分组查询注意力，GQA将查询头分成G组，每个组共享一个Key 和
Value 矩阵。GQA-G是指具有G组的grouped-query
attention。其中多个查询头关注相同的键和值头，以减少推理过程中 KV
缓存的大小，并可以显著提高推理吞吐量。GQA-1具有单个组，因此具有单个Key
和 Value，等效于MQA。而GQA-H具有与头数相等的组，等效于MHA。</p>
</blockquote>
<figure>
<img src="https://image.yinan.fun/image-20230723112453383.png" srcset="/img/loading.gif" lazyload
alt="Llama 2 family of models" />
<figcaption aria-hidden="true">Llama 2 family of models</figcaption>
</figure>
<ul>
<li><p>Hyperparameters</p>
<p>We trained using the <strong>AdamW optimizer</strong> (Loshchilov and
Hutter, 2017), with β1 = 0.9, β2 = 0.95, eps = 10-5. We use a
<strong>cosine learning rate schedule</strong>, with warmup of 2000
steps, and decay final learning rate down to 10% of the peak learning
rate. We use a weight decay of 0.1 and gradient clipping of
1.0.</p></li>
</ul>
<figure>
<img src="https://image.yinan.fun/image-20230723115454030.png" srcset="/img/loading.gif" lazyload
alt="Training Loss for Llama 2 models" />
<figcaption aria-hidden="true">Training Loss for Llama 2
models</figcaption>
</figure>
<ul>
<li><p>Tokenizer</p>
<p>We use the same tokenizer as Llama 1; it employs a <strong>bytepair
encoding</strong> (BPE) algorithm (Sennrich et al., 2016) using the
implementation from SentencePiece (Kudo and Richardson, 2018). As with
Llama 1, we split all numbers into individual digits and use bytes to
decompose unknown UTF-8 characters. The total vocabulary size is 32k
tokens.</p></li>
<li><p>Llama 2预训练模型评估</p>
<ul>
<li>和开源模型对比，Llama 2性能最好。</li>
</ul>
<figure>
<img src="https://image.yinan.fun/image-20230723120200445.png" srcset="/img/loading.gif" lazyload
alt="Overall performance on grouped academic benchmarks compared to open-source base models" />
<figcaption aria-hidden="true">Overall performance on grouped academic
benchmarks compared to open-source base models</figcaption>
</figure>
<ul>
<li>和闭源模型对比，Llama 2有很大的性能差距。</li>
</ul>
<figure>
<img src="https://image.yinan.fun/image-20230723120239606.png" srcset="/img/loading.gif" lazyload
alt="Comparison to closed-source models" />
<figcaption aria-hidden="true">Comparison to closed-source
models</figcaption>
</figure></li>
</ul>
<h2 id="微调">微调</h2>
<p>Llama 2-Chat
是数月研究和反复应用对齐（alignment）技术的成果，包括指令调整（instruction
tuning）和 RLHF，需要大量的计算和注释资源。</p>
<h3 id="supervised-fine-tuning-sft">Supervised Fine-Tuning (SFT)</h3>
<ul>
<li><p>Getting Started</p>
<p>To bootstrap, we started the SFT stage with <strong>publicly
available instruction tuning data</strong> (Chung et al., 2022), as
utilized previously in Touvron et al. (2023).</p></li>
<li><p>Quality Is All You Need</p>
<p>By setting aside millions of examples from third-party datasets and
using <strong>fewer but higher-quality examples</strong> from our own
vendor-based annotation efforts, our results notably improved.</p></li>
</ul>
<p>For the fine-tuning process, each sample consists of a prompt and an
answer. To ensure the model sequence length is properly filled, we
<strong>concatenate all the prompts and answers</strong> from the
training set. A special token is utilized to separate the prompt and
answer segments. We utilize an autoregressive objective and zero-out the
loss on tokens from the user prompt, so as a result, we backpropagate
only on answer tokens. Finally, we fine-tune the model for 2 epochs.</p>
<h3 id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement
Learning with Human Feedback (RLHF)</h3>
<p>RLHF is a model training procedure that is applied to a fine-tuned
language model to <strong>further align model behavior with human
preferences and instruction following</strong>. We collect data that
represents empirically sampled human preferences, whereby human
annotators select which of two model outputs they prefer. This human
feedback is subsequently used to <strong>train a reward model</strong>,
which learns patterns in the preferences of the human annotators and can
then automate preference decisions.</p>
<ul>
<li><h4 id="human-preference-data-collection">Human Preference Data
Collection</h4>
<p>Our annotation procedure proceeds as follows. We ask annotators to
first <strong>write a prompt</strong>, then choose between two sampled
model responses, based on provided criteria. In order to
<strong>maximize the diversity</strong>, the two responses to a given
prompt are sampled from two different model variants, and varying the
temperature hyper-parameter. In addition to giving participants a forced
choice, we also ask annotators to <strong>label the degree</strong> to
which they prefer their chosen response over the alternative: either
their choice is significantly better, better, slightly better, or
negligibly better/ unsure.</p>
<p>For our collection of preference annotations, we focus on
<strong>helpfulness and safety</strong>.</p>
<p>Llama 2-Chat improvement also <strong>shifted the model’s data
distribution</strong>. Since reward model accuracy can quickly degrade
if not exposed to this new sample distribution, i.e., from
hyper-specialization (Scialom et al., 2020b), it is important before a
new Llama 2-Chat tuning iteration to <strong>gather new preference
data</strong> using the latest Llama 2-Chat iterations. This step helps
keep the reward model on-distribution and maintain an accurate reward
for the latest model.</p></li>
<li><h4 id="reward-modeling">Reward Modeling</h4>
<p>The reward model takes a model response and its corresponding prompt
(including contexts from previous turns) as inputs and outputs a scalar
score to indicate the quality (e.g., helpfulness and safety) of the
model generation.</p>
<p>Helpfulness and safety sometimes trade off (Bai et al., 2022a), which
can make it challenging for a single reward model to perform well on
both. To address this, we train <strong>two separate reward
models</strong>, one optimized for helpfulness (referred to as
Helpfulness RM) and another for safety (Safety RM).</p>
<p><strong>Training Objectives</strong> <span class="math display">\[
\mathcal{L}_{ranking}=-log(\sigma(r_{\theta}(x,y_c)-r_{\theta}(x,y_r)-m(r)))
\]</span> where <span class="math inline">\(r_{\theta}(x,y)\)</span> is
the scalar score output for prompt <span
class="math inline">\(x\)</span> and completion <span
class="math inline">\(y\)</span> with model weights <span
class="math inline">\(\theta\)</span>. <span
class="math inline">\(y_c\)</span> is the preferred response that
annotators choose and <span class="math inline">\(y_r\)</span> is the
rejected counterpart. Margin <span class="math inline">\(m(r)\)</span>
is a discrete function of the preference rating. Naturally, we use a
large margin for pairs with distinct responses, and a smaller one for
those with similar responses.</p></li>
<li><h4 id="iterative-fine-tuning">Iterative Fine-Tuning</h4>
<p>Two main algorithms</p>
<ul>
<li><p>Proximal Policy Optimization (PPO) (Schulman et al., 2017), the
standard in RLHF literature.</p>
<blockquote>
<p>近端策略优化（proximal policy
optimization，PPO），通过重要性采样把同策略换成异策略。</p>
<p><a
target="_blank" rel="noopener" href="https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5">https://datawhalechina.github.io/easy-rl/#/chapter5/chapter5</a></p>
</blockquote></li>
<li><p>Rejection Sampling fine-tuning. We <strong>sample K
outputs</strong> from the model and <strong>select the best
candidate</strong> with our reward. Here, we go one step further, and
use the selected outputs for a <strong>gradient update</strong>. For
each prompt, the sample obtaining the highest reward score is considered
the new <strong>gold standard</strong>. Similar to Scialom et al.
(2020a), we then fine-tune our model on the new set of ranked samples,
reinforcing the reward.</p>
<blockquote>
<p>挑出分数最高的response作为training target，做supervised fine-tuning
(SFT)。</p>
</blockquote>
<p>We perform rejection <strong>sampling only</strong> with our
<strong>largest 70B Llama 2-Chat</strong>. All smaller models are
<strong>fine-tuned on rejection sampled data from the larger
model</strong>, thus distilling the large-model capabilities into the
smaller ones.</p></li>
</ul></li>
</ul>
<h2 id="使用llama">使用Llama</h2>
<p><a
target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/llama">https://huggingface.co/docs/transformers/main/en/model_doc/llama</a></p>
<p><a
target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/llama2">https://huggingface.co/docs/transformers/main/en/model_doc/llama2</a></p>
<ul>
<li>下载模型参数。Weights for the LLaMA models can be obtained from by
filling out <a
target="_blank" rel="noopener" href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form">this
form</a></li>
<li>转换为Hugging Face格式。After downloading the weights, they will
need to be converted to the Hugging Face Transformers format using the
<a
target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">conversion
script</a>. The script can be called with the following (example)
command:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">python src/transformers/models/llama/convert_llama_weights_to_hf.py \<br>    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path<br></code></pre></td></tr></table></figure>
<ul>
<li>加载模型和tokenizer。After conversion, the model and tokenizer can
be loaded via:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaForCausalLM, LlamaTokenizer<br><br>tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)<br>model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)<br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="category-chain-item">学习笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">#语言模型</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>了解Llama</div>
      <div>https://yinan.fun/了解Llama</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>yinan</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年7月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87" title="图像质量评价指标">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">图像质量评价指标</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/%E8%B0%83%E8%AF%95%E8%BF%90%E8%A1%8C%E5%8D%8E%E9%A9%BC%E6%A8%A1%E5%9E%8B" title="调试运行华驼模型">
                        <span class="hidden-mobile">调试运行华驼模型</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'wyn04/comment-utterances');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
